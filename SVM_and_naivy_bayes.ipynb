{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a Support Vector Machine (SVM)?\n"
      ],
      "metadata": {
        "id": "WCXfA5oCRuyx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A **Support Vector Machine (SVM)** is a supervised machine learning algorithm used primarily for classification and, to some extent, regression tasks. SVM works by finding the optimal hyperplane that separates data points belonging to different classes. It is particularly effective for high-dimensional spaces and for cases where there is a clear margin of separation between classes.\n",
        "\n",
        "### Key Concepts in SVM:\n",
        "\n",
        "1. **Hyperplane**: A hyperplane is a decision boundary that separates data points of different classes. In a 2D space, this is simply a line, and in 3D, it's a plane. In higher dimensions, it’s a hyperplane.\n",
        "\n",
        "2. **Support Vectors**: These are the data points that are closest to the hyperplane and are critical for defining the position and orientation of the hyperplane. These points \"support\" the hyperplane, and the SVM aims to maximize the margin between these points and the hyperplane.\n",
        "\n",
        "3. **Margin**: The margin is the distance between the hyperplane and the closest data points from either class (the support vectors). SVM tries to maximize this margin, ensuring the best possible separation between the classes.\n",
        "\n",
        "4. **Linear vs. Non-linear SVM**:\n",
        "   - **Linear SVM**: When the data is linearly separable, a straight line or hyperplane can be used to separate the classes.\n",
        "   - **Non-linear SVM**: When the data is not linearly separable, the SVM uses a **kernel trick** to transform the data into a higher-dimensional space where it becomes linearly separable.\n",
        "\n",
        "5. **Kernel Trick**: In cases where the data is not linearly separable, SVM can use a mathematical function (called a kernel) to map the data into a higher-dimensional space where it can be separated by a hyperplane. Common kernels include:\n",
        "   - **Linear Kernel**\n",
        "   - **Polynomial Kernel**\n",
        "   - **Radial Basis Function (RBF) Kernel**\n",
        "   - **Sigmoid Kernel**\n",
        "\n",
        "6. **Soft Margin**: In real-world scenarios, data is often noisy, and some data points may be misclassified. To handle this, SVM allows for a **soft margin**, which permits some points to be on the wrong side of the margin, controlled by a regularization parameter (C). A higher value of **C** makes the margin stricter, while a lower value of **C** allows more misclassifications.\n",
        "\n",
        "### Advantages of SVM:\n",
        "- **Effective in high-dimensional spaces**, making it ideal for text classification (like spam filtering) or other tasks where features are many.\n",
        "- **Robust to overfitting**, especially in high-dimensional space.\n",
        "- **Works well for both linearly and non-linearly separable data** (via the kernel trick).\n",
        "\n",
        "### Disadvantages of SVM:\n",
        "- **Computationally expensive**: Training SVM models can be slow, especially with large datasets.\n",
        "- **Difficult to interpret**: SVMs don't provide probabilities (although they can be modified to do so), making them less interpretable than some other models (like decision trees).\n",
        "- **Not great with large datasets**: SVM can struggle with very large datasets because of the computational cost.\n",
        "\n",
        "### Example Use Cases:\n",
        "- **Text classification** (e.g., spam vs. not spam)\n",
        "- **Image classification**\n",
        "- **Bioinformatics** (e.g., protein classification)\n",
        "\n",
        "In summary, SVM is a powerful classifier that works by finding the optimal decision boundary that maximizes the margin between classes. Its effectiveness, especially with non-linear data, makes it a popular choice in machine learning tasks."
      ],
      "metadata": {
        "id": "PnmXaU98R8Kl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the difference between Hard Margin and Soft Margin SVM?\n"
      ],
      "metadata": {
        "id": "8_WaSPAuR_JY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The key difference between **Hard Margin SVM** and **Soft Margin SVM** lies in how they handle **misclassifications** or **errors** in the data. Both are variations of the Support Vector Machine (SVM) algorithm, but they are used in different scenarios, depending on the nature of the data.\n",
        "\n",
        "### 1. **Hard Margin SVM**:\n",
        "   - **Definition**: Hard Margin SVM refers to the traditional version of the SVM algorithm, where the model tries to find a hyperplane that perfectly separates the classes with **no misclassifications**.\n",
        "   - **Ideal Scenario**: Hard Margin SVM is only applicable when the data is **linearly separable**, meaning that there exists a hyperplane that can perfectly divide the two classes without any overlap.\n",
        "   - **No Tolerance for Misclassifications**: In Hard Margin SVM, the algorithm does not allow any errors or points that fall on the wrong side of the margin. Every data point must be correctly classified.\n",
        "   - **Mathematical Formulation**: The objective is to maximize the margin (distance between the hyperplane and the nearest data points, i.e., support vectors) while ensuring that **all** data points are on the correct side of the margin.\n",
        "   - **Pros**: When data is perfectly separable, Hard Margin SVM performs well with clear decision boundaries.\n",
        "   - **Cons**: It is **sensitive to outliers** and **does not work well with noisy or overlapping data**, as even a single misclassified point would prevent the model from finding an optimal solution.\n",
        "\n",
        "   ### Example:\n",
        "   - Hard Margin SVM would be used when you know your data is clean and perfectly separable, such as in problems where there’s clear, distinct separation (e.g., high-quality data).\n",
        "\n",
        "### 2. **Soft Margin SVM**:\n",
        "   - **Definition**: Soft Margin SVM is a more flexible version of the SVM algorithm. It allows some misclassifications or errors, introducing **slack variables** to permit certain points to lie within the margin or on the wrong side of the hyperplane.\n",
        "   - **Ideal Scenario**: Soft Margin SVM is useful when the data is **not linearly separable** or when there is **noise or overlap** between the classes.\n",
        "   - **Tolerance for Misclassifications**: Soft Margin SVM aims to find the best balance between maximizing the margin and allowing some misclassifications. The algorithm introduces a **regularization parameter (C)** to control the trade-off between achieving a large margin and having a small number of misclassifications.\n",
        "     - A **high value of C** results in a model with a **narrower margin** and **fewer misclassifications**, making the model stricter.\n",
        "     - A **low value of C** allows a **larger margin** but with more **misclassifications**.\n",
        "   - **Mathematical Formulation**: The objective is to maximize the margin while minimizing a combination of misclassification errors and the width of the margin (controlled by the parameter C).\n",
        "   - **Pros**: Soft Margin SVM is **robust to noise** and **works well with non-linearly separable data**. It is the preferred approach in most real-world datasets, which are rarely perfectly separable.\n",
        "   - **Cons**: There’s a trade-off between the margin width and the number of misclassifications, so choosing an appropriate **C** value is important.\n",
        "\n",
        "   ### Example:\n",
        "   - Soft Margin SVM would be used in real-world situations where there is noisy or overlapping data, such as in text classification or when dealing with imbalanced datasets.\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Differences**:\n",
        "| Feature                 | **Hard Margin SVM**                        | **Soft Margin SVM**                            |\n",
        "|-------------------------|--------------------------------------------|------------------------------------------------|\n",
        "| **Data Assumption**      | Data is linearly separable                | Data is not necessarily linearly separable     |\n",
        "| **Misclassifications**   | No misclassifications allowed             | Allows some misclassifications (errors)        |\n",
        "| **Robustness to Noise**  | Not robust to noisy data or outliers      | Robust to noisy data and outliers              |\n",
        "| **Complexity**           | Simpler to implement, but restrictive     | More flexible, but requires tuning of C        |\n",
        "| **Use Cases**            | Ideal for perfectly separable data        | Suitable for noisy, overlapping, or complex data |\n",
        "\n",
        "### **In Summary**:\n",
        "- **Hard Margin SVM**: Used when data is perfectly separable and no errors are allowed.\n",
        "- **Soft Margin SVM**: More commonly used in real-world scenarios where the data is noisy or not perfectly separable, allowing some tolerance for misclassifications.\n",
        "\n",
        "Soft Margin SVM is generally the preferred approach because it handles practical, noisy data better."
      ],
      "metadata": {
        "id": "If0tYOOVR_Gw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is the mathematical intuition behind SVM?\n"
      ],
      "metadata": {
        "id": "nG1Vf-yXR_D2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The mathematical intuition behind **Support Vector Machines (SVM)** lies in finding the **optimal hyperplane** that best separates data points belonging to different classes. Here's a step-by-step breakdown of the mathematical concept:\n",
        "\n",
        "### 1. **Linear Classification Problem**:\n",
        "\n",
        "Imagine you have a set of data points that belong to two classes (say **Class 1** and **Class 2**). Each data point is represented by a vector in a feature space, and the goal of SVM is to find a hyperplane (in 2D, this would be a line) that **separates** these two classes with the **maximum margin**.\n",
        "\n",
        "#### Goal:\n",
        "Find a **hyperplane** \\( \\mathbf{w} \\cdot \\mathbf{x} + b = 0 \\), where:\n",
        "- \\( \\mathbf{w} \\) is the **weight vector** (normal vector to the hyperplane),\n",
        "- \\( \\mathbf{x} \\) is the **data point vector**,\n",
        "- \\( b \\) is the **bias term** (a constant that shifts the hyperplane).\n",
        "\n",
        "### 2. **Margin Maximization**:\n",
        "\n",
        "- **Margin**: The margin is the **distance** between the hyperplane and the nearest data points from either class. The larger the margin, the better the separation between the classes.\n",
        "- The **support vectors** are the data points that are closest to the hyperplane, and they are crucial because they determine the position and orientation of the hyperplane.\n",
        "\n",
        "#### Formula for Margin:\n",
        "\n",
        "The margin \\( M \\) is defined as the distance from the hyperplane to the closest support vector. Mathematically, it is given by:\n",
        "\n",
        "\\[\n",
        "M = \\frac{2}{\\| \\mathbf{w} \\|}\n",
        "\\]\n",
        "\n",
        "Thus, to maximize the margin, we want to **maximize \\( \\frac{1}{\\| \\mathbf{w} \\|} \\)**, which is equivalent to minimizing \\( \\frac{1}{2} \\| \\mathbf{w} \\|^2 \\). This is a **convex optimization** problem.\n",
        "\n",
        "### 3. **Optimization Problem**:\n",
        "\n",
        "The SVM optimization problem can be formulated as follows:\n",
        "\n",
        "#### Objective:\n",
        "Maximize the margin \\( M = \\frac{2}{\\| \\mathbf{w} \\|} \\), or equivalently, **minimize \\( \\frac{1}{2} \\| \\mathbf{w} \\|^2 \\)**.\n",
        "\n",
        "#### Constraints:\n",
        "For a data point \\( \\mathbf{x}_i \\) belonging to class \\( y_i \\), the following condition must hold for the points to be correctly classified:\n",
        "\n",
        "\\[\n",
        "y_i (\\mathbf{w} \\cdot \\mathbf{x}_i + b) \\geq 1 \\quad \\text{for all } i\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- \\( y_i \\) is the class label of the data point \\( \\mathbf{x}_i \\) (either +1 or -1),\n",
        "- \\( \\mathbf{w} \\cdot \\mathbf{x}_i + b \\) is the signed distance of the point \\( \\mathbf{x}_i \\) from the hyperplane.\n",
        "\n",
        "This constraint ensures that:\n",
        "- Data points from **Class 1** are on one side of the hyperplane, and\n",
        "- Data points from **Class 2** are on the other side.\n",
        "\n",
        "In other words, we want all data points to be correctly classified, and the support vectors should lie on the boundary of the margin.\n",
        "\n",
        "#### Mathematical Formulation of the Optimization Problem:\n",
        "\\[\n",
        "\\text{Minimize:} \\quad \\frac{1}{2} \\| \\mathbf{w} \\|^2\n",
        "\\]\n",
        "\\[\n",
        "\\text{Subject to:} \\quad y_i (\\mathbf{w} \\cdot \\mathbf{x}_i + b) \\geq 1, \\quad \\forall i\n",
        "\\]\n",
        "\n",
        "### 4. **Lagrangian and Dual Formulation**:\n",
        "\n",
        "The above optimization problem is a **constrained optimization** problem. To solve it efficiently, we use the **Lagrangian multiplier** technique, which leads to the **dual form** of the problem.\n",
        "\n",
        "The Lagrangian is given by:\n",
        "\n",
        "\\[\n",
        "L(\\mathbf{w}, b, \\alpha) = \\frac{1}{2} \\| \\mathbf{w} \\|^2 - \\sum_{i=1}^N \\alpha_i [y_i (\\mathbf{w} \\cdot \\mathbf{x}_i + b) - 1]\n",
        "\\]\n",
        "\n",
        "Where \\( \\alpha_i \\) are the **Lagrange multipliers** that enforce the constraints. By taking the partial derivatives of \\( L \\) with respect to \\( \\mathbf{w} \\) and \\( b \\), and setting them to zero, we get the **dual problem**, which involves only the **kernel** function (if the data is non-linearly separable).\n",
        "\n",
        "The dual form is easier to solve and allows us to apply the **kernel trick** for non-linear separation.\n",
        "\n",
        "### 5. **Kernel Trick (Non-linearly Separable Data)**:\n",
        "\n",
        "When data is not linearly separable, the idea is to map the data into a higher-dimensional space where it becomes separable using a kernel function \\( K(\\mathbf{x}_i, \\mathbf{x}_j) \\).\n",
        "\n",
        "The **kernel trick** allows us to compute the inner product \\( \\mathbf{x}_i \\cdot \\mathbf{x}_j \\) in this higher-dimensional space without explicitly mapping the data points. Common kernels include:\n",
        "- **Linear Kernel**: \\( K(\\mathbf{x}_i, \\mathbf{x}_j) = \\mathbf{x}_i \\cdot \\mathbf{x}_j \\)\n",
        "- **Polynomial Kernel**: \\( K(\\mathbf{x}_i, \\mathbf{x}_j) = (\\mathbf{x}_i \\cdot \\mathbf{x}_j + c)^d \\)\n",
        "- **Radial Basis Function (RBF) Kernel**: \\( K(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp(-\\gamma \\| \\mathbf{x}_i - \\mathbf{x}_j \\|^2) \\)\n",
        "\n",
        "This allows SVM to perform **non-linear classification** efficiently.\n",
        "\n",
        "### 6. **Final Decision Function**:\n",
        "\n",
        "Once the model is trained, the decision function used to classify a new point \\( \\mathbf{x} \\) is:\n",
        "\n",
        "\\[\n",
        "f(\\mathbf{x}) = \\text{sign} \\left( \\sum_{i=1}^N \\alpha_i y_i K(\\mathbf{x}_i, \\mathbf{x}) + b \\right)\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- \\( \\alpha_i \\) are the learned Lagrange multipliers (support vector weights),\n",
        "- \\( K(\\mathbf{x}_i, \\mathbf{x}) \\) is the kernel function, and\n",
        "- \\( b \\) is the bias term.\n",
        "\n",
        "### **Summary of Mathematical Intuition**:\n",
        "- The goal of SVM is to find a **hyperplane** that **maximizes the margin** between data points of different classes.\n",
        "- The optimization problem is formulated to **minimize the norm of the weight vector** \\( \\mathbf{w} \\) while ensuring that each data point is correctly classified with a margin of at least 1.\n",
        "- The **support vectors** (data points closest to the hyperplane) are key to determining the position of the hyperplane.\n",
        "- In cases of non-linearly separable data, the **kernel trick** is used to transform the data into a higher-dimensional space where a linear separation is possible.\n",
        "\n",
        "SVM is powerful because it not only finds the optimal decision boundary but also generalizes well to new data, especially when using the appropriate kernel."
      ],
      "metadata": {
        "id": "Pz5RvmeDR-9x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is the role of Lagrange Multipliers in SVM?"
      ],
      "metadata": {
        "id": "atZiVqUsR-68"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In **Support Vector Machines (SVM)**, **Lagrange multipliers** play a crucial role in the optimization process. They are used to handle the constraints imposed by the **margin maximization** problem. Specifically, they help in converting the original constrained optimization problem into an **unconstrained optimization problem**, which is easier to solve. The Lagrange multipliers are a key part of the **dual formulation** of SVM, and they are critical for finding the **support vectors** and ensuring the correct classification of data points.\n",
        "\n",
        "Here’s a detailed explanation of the role of Lagrange multipliers in SVM:\n",
        "\n",
        "### 1. **Formulation of the SVM Optimization Problem**:\n",
        "\n",
        "We start by defining the objective function and constraints for the **hard margin SVM** (the case where no misclassification is allowed):\n",
        "\n",
        "- **Objective**: Maximize the margin between the two classes, which is equivalent to minimizing \\( \\frac{1}{2} \\| \\mathbf{w} \\|^2 \\), where \\( \\mathbf{w} \\) is the weight vector of the hyperplane.\n",
        "- **Constraints**: For each data point \\( \\mathbf{x}_i \\), it should satisfy the condition that the point is correctly classified. Mathematically, for a data point \\( \\mathbf{x}_i \\) with class label \\( y_i \\) (either +1 or -1), the condition is:\n",
        "  \\[\n",
        "  y_i (\\mathbf{w} \\cdot \\mathbf{x}_i + b) \\geq 1 \\quad \\text{for all } i\n",
        "  \\]\n",
        "  This ensures that all points lie on the correct side of the margin.\n",
        "\n",
        "Thus, the SVM optimization problem can be written as:\n",
        "\n",
        "- **Minimize**: \\( \\frac{1}{2} \\| \\mathbf{w} \\|^2 \\)\n",
        "- **Subject to**: \\( y_i (\\mathbf{w} \\cdot \\mathbf{x}_i + b) \\geq 1 \\)\n",
        "\n",
        "### 2. **Lagrangian Function**:\n",
        "\n",
        "To solve this constrained optimization problem, we use **Lagrange multipliers**. The idea is to create a Lagrangian function by introducing multipliers \\( \\alpha_i \\) for each of the constraints. The Lagrangian function combines the objective function and the constraints in a single expression:\n",
        "\n",
        "\\[\n",
        "L(\\mathbf{w}, b, \\alpha) = \\frac{1}{2} \\| \\mathbf{w} \\|^2 - \\sum_{i=1}^N \\alpha_i [y_i (\\mathbf{w} \\cdot \\mathbf{x}_i + b) - 1]\n",
        "\\]\n",
        "\n",
        "Here:\n",
        "- \\( \\alpha_i \\) are the **Lagrange multipliers**, which are non-negative values that correspond to each constraint.\n",
        "- The term \\( y_i (\\mathbf{w} \\cdot \\mathbf{x}_i + b) - 1 \\) is the constraint for each data point.\n",
        "\n",
        "### 3. **Intuition Behind Lagrange Multipliers**:\n",
        "\n",
        "The Lagrange multipliers \\( \\alpha_i \\) have a specific interpretation:\n",
        "- If \\( \\alpha_i > 0 \\), the corresponding data point \\( \\mathbf{x}_i \\) lies on the **margin boundary** (i.e., it is one of the **support vectors**). These are the critical points that influence the position of the hyperplane.\n",
        "- If \\( \\alpha_i = 0 \\), the data point \\( \\mathbf{x}_i \\) is either correctly classified and far from the margin, or it does not contribute to the optimization of the margin. These are the points that do **not** influence the hyperplane's position.\n",
        "\n",
        "### 4. **Solving the Optimization Problem**:\n",
        "\n",
        "The next step is to **maximize the Lagrangian** with respect to \\( \\mathbf{w} \\) and \\( b \\), while **minimizing** it with respect to \\( \\alpha_i \\). This yields the **dual formulation** of the SVM problem. By differentiating the Lagrangian with respect to \\( \\mathbf{w} \\) and \\( b \\), and setting the derivatives to zero, we can eliminate \\( \\mathbf{w} \\) and \\( b \\) and arrive at a new optimization problem that only involves the Lagrange multipliers \\( \\alpha_i \\).\n",
        "\n",
        "The dual form of the SVM optimization problem becomes:\n",
        "\n",
        "\\[\n",
        "\\text{Maximize} \\quad W(\\alpha) = \\sum_{i=1}^N \\alpha_i - \\frac{1}{2} \\sum_{i,j=1}^N \\alpha_i \\alpha_j y_i y_j (\\mathbf{x}_i \\cdot \\mathbf{x}_j)\n",
        "\\]\n",
        "\n",
        "**Subject to**:\n",
        "- \\( \\alpha_i \\geq 0 \\) for all \\( i \\),\n",
        "- \\( \\sum_{i=1}^N \\alpha_i y_i = 0 \\) (the sum of the weighted labels must be zero, ensuring that the model is balanced).\n",
        "\n",
        "### 5. **Dual Form and the Kernel Trick**:\n",
        "\n",
        "Once we have the dual formulation, the optimization problem becomes easier to solve because:\n",
        "- The only variables we need to optimize are the **Lagrange multipliers** \\( \\alpha_i \\).\n",
        "- The dual formulation involves the **inner products** \\( \\mathbf{x}_i \\cdot \\mathbf{x}_j \\), which can be replaced by a **kernel function** \\( K(\\mathbf{x}_i, \\mathbf{x}_j) \\), allowing the SVM to handle **non-linear decision boundaries**. This is known as the **kernel trick**, which projects the data into a higher-dimensional space where a linear separator is possible.\n",
        "\n",
        "### 6. **Support Vectors and the Role of \\( \\alpha_i \\)**:\n",
        "\n",
        "The Lagrange multipliers \\( \\alpha_i \\) determine which points are the **support vectors**:\n",
        "- **Support vectors** are the data points that lie on the margin or are misclassified (if using **soft margin SVM**).\n",
        "- The support vectors are the only points that influence the position of the hyperplane. Data points that do not lie on the margin (i.e., for which \\( \\alpha_i = 0 \\)) do not affect the solution.\n",
        "\n",
        "### 7. **Final Decision Function**:\n",
        "\n",
        "Once the Lagrange multipliers \\( \\alpha_i \\) are obtained, the final decision function for classifying a new point \\( \\mathbf{x} \\) is:\n",
        "\n",
        "\\[\n",
        "f(\\mathbf{x}) = \\text{sign} \\left( \\sum_{i=1}^N \\alpha_i y_i K(\\mathbf{x}_i, \\mathbf{x}) + b \\right)\n",
        "\\]\n",
        "\n",
        "Where \\( K(\\mathbf{x}_i, \\mathbf{x}) \\) is the kernel function and \\( b \\) is the bias term (which can be determined after optimization).\n",
        "\n",
        "### **Summary of the Role of Lagrange Multipliers in SVM**:\n",
        "- Lagrange multipliers \\( \\alpha_i \\) are introduced to handle the constraints in the optimization problem.\n",
        "- They allow us to transform the original **constrained optimization** problem into an **unconstrained dual optimization** problem.\n",
        "- The multipliers \\( \\alpha_i \\) identify the **support vectors** (data points closest to the margin) that influence the optimal hyperplane.\n",
        "- The dual form of the problem is easier to solve and enables the use of the **kernel trick**, allowing SVM to work with non-linearly separable data.\n",
        "\n",
        "Thus, **Lagrange multipliers** are fundamental for SVM because they allow the model to focus only on the most important data points (the support vectors) and make the optimization process more efficient."
      ],
      "metadata": {
        "id": "z4UyHtV0R-4V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 5. What are Support Vectors in SVM?\n"
      ],
      "metadata": {
        "id": "3vsu_vlWR-0r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Support Vectors** are the key elements in a **Support Vector Machine (SVM)** model that are critical to determining the optimal hyperplane (decision boundary) that separates different classes. They are the **data points** that are closest to the separating hyperplane and have a direct influence on the position and orientation of this hyperplane.\n",
        "\n",
        "Let’s break down the concept of support vectors:\n",
        "\n",
        "### 1. **Definition of Support Vectors**:\n",
        "Support Vectors are the **data points** from the training set that:\n",
        "- Lie closest to the decision boundary (hyperplane).\n",
        "- Influence the position and orientation of the decision boundary.\n",
        "- Are the most important points for the SVM model, as they define the margin.\n",
        "\n",
        "In other words, **support vectors** are the **critical data points** that are used to define the **maximum margin** between the two classes in SVM. These are the points that, if removed, could potentially change the location of the hyperplane.\n",
        "\n",
        "### 2. **Role in SVM**:\n",
        "\n",
        "The main goal of an SVM is to find the **hyperplane** that maximizes the **margin** between the two classes. The **margin** is the distance between the hyperplane and the nearest data points from either class. The SVM algorithm aims to maximize this margin because a larger margin typically leads to better generalization and less overfitting.\n",
        "\n",
        "The margin is **defined by the support vectors**. The support vectors are the **closest data points** to the hyperplane and determine its position. If the data points that lie closest to the hyperplane are changed, the hyperplane itself would shift. Therefore, only the support vectors are important for defining the hyperplane, while the other points further away from the hyperplane do not have any impact on its position.\n",
        "\n",
        "### 3. **Mathematical Interpretation**:\n",
        "\n",
        "In a **linear SVM**, the objective is to maximize the margin between two classes, which is defined as the distance between the hyperplane and the support vectors. The formula for the margin \\( M \\) is given by:\n",
        "\n",
        "\\[\n",
        "M = \\frac{2}{\\| \\mathbf{w} \\|}\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- \\( \\mathbf{w} \\) is the weight vector perpendicular to the hyperplane.\n",
        "- The support vectors are the points that lie on the margin boundaries, and they satisfy the following conditions:\n",
        "  \\[\n",
        "  y_i (\\mathbf{w} \\cdot \\mathbf{x}_i + b) = 1\n",
        "  \\]\n",
        "  for support vectors, where:\n",
        "  - \\( \\mathbf{x}_i \\) is a support vector,\n",
        "  - \\( y_i \\) is the class label (+1 or -1),\n",
        "  - \\( b \\) is the bias term.\n",
        "\n",
        "For **non-linearly separable data** (in the case of **soft margin SVM**), the support vectors are the data points that either:\n",
        "- Lie closest to the margin,\n",
        "- Or are misclassified and lie within the margin.\n",
        "\n",
        "### 4. **Support Vectors and the Decision Boundary**:\n",
        "\n",
        "The decision boundary (or hyperplane) in an SVM is influenced only by the support vectors. The hyperplane is defined as:\n",
        "\n",
        "\\[\n",
        "\\mathbf{w} \\cdot \\mathbf{x} + b = 0\n",
        "\\]\n",
        "\n",
        "- The support vectors are the closest data points to this hyperplane, and they lie on the **margin boundaries**.\n",
        "- The points that are farther away from the hyperplane do not affect the hyperplane position and do not qualify as support vectors.\n",
        "\n",
        "### 5. **Characteristics of Support Vectors**:\n",
        "- **Critical for Model Construction**: Support vectors are the most important points for constructing the optimal decision boundary (hyperplane). Removing them from the training data would change the decision boundary.\n",
        "- **Influence the Margin**: The margin is maximized based on the positions of the support vectors. The larger the margin, the better the generalization of the model.\n",
        "- **Not Always All Points**: Not all data points are support vectors. Only the points that lie closest to the margin, or that are misclassified, are considered support vectors.\n",
        "- **Few Data Points**: In practice, support vectors are often a small fraction of the overall dataset. SVM only requires these points to make accurate predictions, making it computationally efficient when dealing with high-dimensional spaces.\n",
        "\n",
        "### 6. **Support Vectors in Different Cases**:\n",
        "\n",
        "#### a. **Linearly Separable Data (Hard Margin SVM)**:\n",
        "- In this case, all the data points can be perfectly separated by a hyperplane.\n",
        "- The support vectors are the points that are on the boundary of the margin, and there is no misclassification.\n",
        "- These support vectors are the closest points to the hyperplane, and their positions define the optimal margin.\n",
        "\n",
        "#### b. **Non-Linearly Separable Data (Soft Margin SVM)**:\n",
        "- In this case, the data is not perfectly separable, and some points may fall inside the margin or be misclassified.\n",
        "- The support vectors are the points that either lie closest to the margin or are misclassified but are still important for defining the decision boundary.\n",
        "- In **soft margin SVM**, a penalty is introduced for misclassifications, but support vectors are still the critical points for determining the hyperplane.\n",
        "\n",
        "#### c. **Non-linear Decision Boundaries (Using Kernel Trick)**:\n",
        "- When SVM is applied with a **non-linear kernel** (such as polynomial or RBF), the support vectors help define the decision boundary in a higher-dimensional space, even if the data is not linearly separable in the original feature space.\n",
        "- The kernel allows SVM to find complex decision boundaries by implicitly mapping data into higher-dimensional spaces where it becomes separable.\n",
        "\n",
        "### 7. **Visualizing Support Vectors**:\n",
        "In a 2D space, you can visualize support vectors as the points that are closest to the decision boundary (hyperplane). For example, in a binary classification problem:\n",
        "- The **support vectors** lie on the **margin boundaries** (the lines parallel to the decision boundary and one unit away from it).\n",
        "- The rest of the data points do not directly influence the hyperplane.\n",
        "\n",
        "### **Summary of the Role of Support Vectors**:\n",
        "- **Support vectors** are the data points closest to the decision boundary (hyperplane) that define the margin.\n",
        "- They are the **most important data points** because they determine the optimal hyperplane.\n",
        "- Only the support vectors affect the decision boundary — removing other points does not impact the model's performance.\n",
        "- In **linear SVM**, they lie on the margin boundaries; in **non-linear SVM**, they are points that either lie on the margin or are misclassified.\n",
        "\n",
        "Support vectors are at the heart of the SVM algorithm, and understanding their role is key to understanding how SVM works and why it is effective, especially in high-dimensional spaces."
      ],
      "metadata": {
        "id": "rEEHXU9SR-w1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is a Support Vector Classifier (SVC)?\n"
      ],
      "metadata": {
        "id": "q9O8-W3ER-uD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A **Support Vector Classifier (SVC)** is an algorithm derived from the **Support Vector Machine (SVM)** framework, which is used for **classification tasks**. Specifically, an SVC is a type of SVM that focuses on finding the optimal **decision boundary** (or hyperplane) that separates two classes of data points in a given feature space.\n",
        "\n",
        "In essence, **SVC** is the practical implementation of the SVM concept applied to **classification problems**. It aims to find a decision boundary that maximizes the **margin** between data points of different classes. The points that lie closest to the decision boundary are called **support vectors**, and they play a crucial role in determining the hyperplane's position.\n",
        "\n",
        "### Key Concepts of Support Vector Classifier (SVC):\n",
        "\n",
        "1. **Optimal Hyperplane (Decision Boundary)**:\n",
        "   - The core goal of SVC is to find a **hyperplane** that maximally separates the data points of different classes. For **binary classification**, this hyperplane is a line (in 2D), a plane (in 3D), or a hyperplane in higher dimensions.\n",
        "   - The **margin** is the distance between the hyperplane and the closest points from each class (called support vectors). SVC aims to maximize this margin to improve generalization, ensuring that the classifier performs well on unseen data.\n",
        "\n",
        "2. **Support Vectors**:\n",
        "   - The **support vectors** are the data points that lie closest to the decision boundary. These are the most important data points for the classifier because they determine the position of the hyperplane.\n",
        "   - Only the support vectors influence the decision boundary; points farther away from the hyperplane do not impact the model. This makes SVC relatively efficient in terms of computation.\n",
        "\n",
        "3. **Hard Margin vs Soft Margin**:\n",
        "   - **Hard Margin SVC**: In cases where the data is perfectly linearly separable (i.e., there is no overlap between classes), SVC constructs a **hard margin** — a decision boundary that separates the classes without any misclassification.\n",
        "   - **Soft Margin SVC**: When the data is not perfectly separable, **soft margin** SVC allows for some misclassification, introducing a penalty for misclassified points. The goal is to balance between maximizing the margin and minimizing classification errors, which is controlled by the **C parameter**. A large \\( C \\) value results in fewer misclassifications, but a smaller margin, while a small \\( C \\) allows more misclassifications but results in a larger margin.\n",
        "\n",
        "4. **Kernel Trick (Non-linear Classification)**:\n",
        "   - **Linear SVC** can only handle linearly separable data, but **SVC** can be extended to work with non-linearly separable data through the **kernel trick**. The kernel trick allows the SVC algorithm to map the data into a higher-dimensional space where a linear separator can be found, even if the data is not linearly separable in the original feature space.\n",
        "   - Common kernel functions include:\n",
        "     - **Polynomial Kernel**\n",
        "     - **Radial Basis Function (RBF) Kernel**\n",
        "     - **Sigmoid Kernel**\n",
        "   \n",
        "   By applying the kernel trick, SVC can classify data that is not linearly separable in the input space but can be separated in a higher-dimensional space.\n",
        "\n",
        "5. **Mathematical Formulation**:\n",
        "   - The **SVC optimization problem** can be formulated as follows:\n",
        "     - Maximize the margin (distance between the support vectors and the hyperplane).\n",
        "     - For linear SVC, the optimization problem becomes:\n",
        "       \\[\n",
        "       \\min_{\\mathbf{w}, b} \\frac{1}{2} \\| \\mathbf{w} \\|^2 \\quad \\text{subject to} \\quad y_i (\\mathbf{w} \\cdot \\mathbf{x}_i + b) \\geq 1, \\quad \\text{for all } i\n",
        "       \\]\n",
        "     - For non-linear SVC using kernels, the optimization problem is adapted by using the kernel function \\( K(\\mathbf{x}_i, \\mathbf{x}_j) \\) instead of the dot product \\( \\mathbf{x}_i \\cdot \\mathbf{x}_j \\), enabling the model to find a decision boundary in higher dimensions.\n",
        "\n",
        "### Steps in Training a Support Vector Classifier:\n",
        "\n",
        "1. **Input Data**: A labeled dataset where each sample has a corresponding class label.\n",
        "2. **Model Selection**: Choose the type of kernel (linear or non-linear) based on whether the data is linearly separable or not.\n",
        "3. **Optimization**: Solve the optimization problem to find the optimal hyperplane (and thus the support vectors) that maximizes the margin.\n",
        "4. **Decision Function**: Once the hyperplane is determined, the SVC classifies new data points by checking which side of the hyperplane they fall on.\n",
        "   \n",
        "### Hyperparameters in SVC:\n",
        "\n",
        "- **C (Regularization Parameter)**: Controls the trade-off between achieving a larger margin (maximizing the distance between classes) and minimizing classification errors. A small \\( C \\) allows for more errors (larger margin), while a large \\( C \\) reduces the errors (smaller margin).\n",
        "- **Kernel**: Defines the type of kernel function to use:\n",
        "  - **'linear'**: A linear kernel (for linearly separable data).\n",
        "  - **'rbf'**: Radial Basis Function kernel (for non-linear decision boundaries).\n",
        "  - **'poly'**: Polynomial kernel.\n",
        "  - **'sigmoid'**: Sigmoid kernel.\n",
        "- **Gamma**: Defines how far the influence of a single training example reaches. It controls the flexibility of the decision boundary in non-linear SVC.\n",
        "\n",
        "### Benefits of Using SVC:\n",
        "- **Effective in High Dimensional Spaces**: SVC is highly effective in cases where the number of dimensions (features) is large, making it suitable for problems with high-dimensional data, such as text classification or image recognition.\n",
        "- **Memory Efficient**: Since SVC uses only support vectors to define the decision boundary, it is memory efficient and works well even with large datasets where only a subset of the data points matter.\n",
        "- **Robust to Overfitting**: SVC can generalize well to unseen data by maximizing the margin, which reduces the risk of overfitting, especially when regularization is properly tuned.\n",
        "\n",
        "### Example Use Cases of SVC:\n",
        "- **Binary Classification Problems**: SVC is particularly suited for binary classification tasks, such as:\n",
        "  - **Spam email detection** (classifying emails as spam or not spam).\n",
        "  - **Image recognition** (classifying images as belonging to different categories).\n",
        "  - **Medical diagnosis** (classifying patients into different disease categories).\n",
        "- **Multi-Class Classification**: While SVC is fundamentally a binary classifier, it can be extended to multi-class classification using strategies like **one-vs-one** or **one-vs-rest**.\n",
        "\n",
        "### Conclusion:\n",
        "\n",
        "In summary, a **Support Vector Classifier (SVC)** is an implementation of **Support Vector Machine (SVM)** that focuses on finding the optimal hyperplane that separates classes in classification tasks. SVC is effective in handling both linearly separable and non-linearly separable data, and it achieves this by using **support vectors** to define the margin and optimize the decision boundary. The flexibility of SVC with different kernels and regularization techniques makes it a powerful and widely used tool for various classification problems."
      ],
      "metadata": {
        "id": "vTHiGn6vR-q5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What is a Support Vector Regressor (SVR)?\n"
      ],
      "metadata": {
        "id": "UILURibXR-lx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A **Support Vector Regressor (SVR)** is an algorithm derived from **Support Vector Machine (SVM)**, but instead of being used for **classification tasks**, it is designed for **regression tasks**. The goal of SVR is to predict continuous numerical values rather than class labels, similar to how linear regression works, but with the added benefit of robustness and flexibility provided by the SVM framework.\n",
        "\n",
        "### Key Concepts of Support Vector Regressor (SVR):\n",
        "\n",
        "1. **Regression Task**:\n",
        "   - Unlike classification, where the goal is to assign a data point to one of several classes, **regression** involves predicting a continuous output (a numerical value) for each input data point.\n",
        "\n",
        "2. **The Goal of SVR**:\n",
        "   - The objective of **SVR** is to find a function that approximates the relationship between the input features and the target variable while maintaining a margin of tolerance (epsilon \\( \\epsilon \\)) around the predicted values.\n",
        "   - It aims to fit the data within a certain threshold, with as few points as possible outside this threshold. This approach helps avoid overfitting, a common problem in regression models.\n",
        "\n",
        "3. **Epsilon-insensitive Loss Function**:\n",
        "   - In SVR, the model tries to predict values that are **within a margin of error** defined by a parameter \\( \\epsilon \\). If the prediction is within this margin, it is considered a correct prediction, and no penalty is assigned.\n",
        "   - Only predictions that deviate more than \\( \\epsilon \\) from the actual value are penalized. This is why SVR is often referred to as an **epsilon-insensitive regression model**.\n",
        "   - The loss function used in SVR is:\n",
        "     \\[\n",
        "     L_{\\epsilon}(y, f(x)) =\n",
        "     \\begin{cases}\n",
        "     0 & \\text{if } |y - f(x)| \\leq \\epsilon \\\\\n",
        "     |y - f(x)| - \\epsilon & \\text{otherwise}\n",
        "     \\end{cases}\n",
        "     \\]\n",
        "     where:\n",
        "     - \\( y \\) is the actual value,\n",
        "     - \\( f(x) \\) is the predicted value,\n",
        "     - \\( \\epsilon \\) is the margin of tolerance.\n",
        "\n",
        "4. **The SVR Optimization Problem**:\n",
        "   - Like SVM, SVR aims to find a function that best fits the data while maintaining a maximum margin. The optimization problem for SVR can be defined as follows:\n",
        "     \\[\n",
        "     \\min_{\\mathbf{w}, b, \\xi_i, \\xi_i^*} \\frac{1}{2} \\| \\mathbf{w} \\|^2 + C \\sum_{i=1}^n (\\xi_i + \\xi_i^*)\n",
        "     \\]\n",
        "     where:\n",
        "     - \\( \\mathbf{w} \\) is the weight vector that defines the hyperplane,\n",
        "     - \\( b \\) is the bias term,\n",
        "     - \\( C \\) is the regularization parameter that controls the trade-off between achieving a low error and maximizing the margin,\n",
        "     - \\( \\xi_i \\) and \\( \\xi_i^* \\) are slack variables that measure the degree of deviation for each data point outside the \\( \\epsilon \\)-insensitive margin.\n",
        "\n",
        "5. **Margin and Support Vectors**:\n",
        "   - In the context of SVR, the margin is defined as the region where the predicted values are within \\( \\epsilon \\) of the actual values.\n",
        "   - The **support vectors** in SVR are the data points that either lie outside the margin (i.e., have an error greater than \\( \\epsilon \\)) or exactly lie on the boundary of the margin. These points are crucial in determining the optimal regression function.\n",
        "\n",
        "6. **The Role of the Regularization Parameter (C)**:\n",
        "   - The **C parameter** in SVR controls the trade-off between achieving a large margin (low bias) and minimizing the error (low variance). A large \\( C \\) value will prioritize minimizing errors (even if it reduces the margin), while a small \\( C \\) value will allow for a wider margin but may result in more errors.\n",
        "\n",
        "7. **Kernels in SVR**:\n",
        "   - Just like in SVM, **kernels** can be used in SVR to map the input data into higher-dimensional feature spaces. This allows SVR to handle non-linear regression problems by using non-linear kernel functions such as:\n",
        "     - **Linear kernel**: For linear regression problems.\n",
        "     - **Polynomial kernel**: For capturing polynomial relationships.\n",
        "     - **Radial Basis Function (RBF) kernel**: For non-linear relationships, often the default choice.\n",
        "     - **Sigmoid kernel**: Based on the sigmoid function.\n",
        "\n",
        "   The kernel trick helps to solve non-linear regression problems by transforming the data into a higher-dimensional space where a linear regression can be applied.\n",
        "\n",
        "### Key Differences Between SVR and SVM:\n",
        "\n",
        "1. **Task Type**:\n",
        "   - **SVM** is used for classification tasks, where the goal is to separate data into different categories.\n",
        "   - **SVR** is used for regression tasks, where the goal is to predict continuous numerical values.\n",
        "\n",
        "2. **Loss Function**:\n",
        "   - SVM uses a **hinge loss function** to penalize misclassification, whereas SVR uses an **epsilon-insensitive loss function** to penalize deviations from the predicted values that exceed a given threshold \\( \\epsilon \\).\n",
        "\n",
        "3. **Output**:\n",
        "   - In SVM, the output is a **class label** (e.g., -1 or +1), while in SVR, the output is a **continuous numerical value**.\n",
        "\n",
        "### How SVR Works:\n",
        "1. **Training**: During training, SVR tries to fit a regression model that minimizes the error within an \\( \\epsilon \\)-margin. Points within the margin (i.e., where the prediction is within \\( \\epsilon \\) of the true value) are not penalized, but points outside the margin are penalized.\n",
        "2. **Support Vectors**: The support vectors are the data points that lie outside the margin and influence the final model.\n",
        "3. **Prediction**: Once the model is trained, it can predict a continuous value for a new input by finding the point on the decision boundary corresponding to the new input.\n",
        "\n",
        "### Hyperparameters in SVR:\n",
        "1. **C**: The regularization parameter that controls the trade-off between fitting the data well (small errors) and having a smooth model (large margin).\n",
        "2. **Epsilon ( \\( \\epsilon \\) )**: Defines the margin of tolerance around the true values. It determines how much deviation from the true value is tolerated before penalty is imposed.\n",
        "3. **Kernel**: Defines the kernel function used to map the input data into a higher-dimensional space (e.g., linear, polynomial, RBF).\n",
        "4. **Gamma** (for non-linear kernels like RBF): Defines the influence of a single training example. A small \\( \\gamma \\) means a far influence, while a large \\( \\gamma \\) means a closer influence.\n",
        "\n",
        "### Benefits of Using SVR:\n",
        "- **Robust to Outliers**: Due to the use of the \\( \\epsilon \\)-insensitive loss function, SVR is less sensitive to outliers than traditional regression models like linear regression.\n",
        "- **Flexible**: SVR can handle both linear and non-linear regression problems using kernels.\n",
        "- **Works Well in High-dimensional Spaces**: Like SVM, SVR is effective in high-dimensional spaces, making it suitable for problems with many features.\n",
        "- **Regularization**: The parameter \\( C \\) allows for a controlled trade-off between bias and variance, helping to avoid overfitting.\n",
        "\n",
        "### Example Use Cases of SVR:\n",
        "- **Stock Price Prediction**: SVR can be used to predict continuous financial values such as stock prices.\n",
        "- **House Price Prediction**: Predicting the price of a house based on various features such as size, location, and number of rooms.\n",
        "- **Temperature Forecasting**: Predicting continuous variables like daily temperatures or weather conditions.\n",
        "\n",
        "### Conclusion:\n",
        "A **Support Vector Regressor (SVR)** is a regression model based on the **Support Vector Machine (SVM)** framework that aims to find a regression function that best fits the data while keeping the predictions within a margin of error, controlled by the \\( \\epsilon \\) parameter. SVR is robust to outliers and flexible enough to handle both linear and non-linear regression problems by using different kernel functions. It is a powerful tool for solving continuous prediction problems, offering advantages in terms of generalization, robustness, and flexibility."
      ],
      "metadata": {
        "id": "M2ebMu-WR-im"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is the Kernel Trick in SVM?\n"
      ],
      "metadata": {
        "id": "CVUq-9eUR-fz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **Kernel Trick** in Support Vector Machines (SVM) is a technique that allows SVM to handle **non-linearly separable data** by implicitly mapping it into a higher-dimensional feature space without needing to compute the coordinates of the data in that space directly. This is achieved by using a **kernel function**, which computes the inner product of data points in the higher-dimensional space.\n",
        "\n",
        "### Why the Kernel Trick is Important:\n",
        "In many real-world problems, the data is not linearly separable in its original feature space. Linear SVM can only separate data that is linearly separable, meaning there exists a hyperplane that can perfectly divide the classes. However, most problems involve non-linear decision boundaries. The kernel trick enables SVM to **learn non-linear decision boundaries** by transforming the data into a higher-dimensional space where the classes become linearly separable, even if they were not in the original feature space.\n",
        "\n",
        "### The Concept of the Kernel Trick:\n",
        "The idea behind the kernel trick is to avoid the explicit mapping of data points into higher-dimensional space, which can be computationally expensive and may require dealing with an infinite number of dimensions (as in the case of the **Radial Basis Function (RBF) kernel**). Instead, the kernel function computes the inner product of data points as if they were mapped into the higher-dimensional space, allowing the SVM to find a linear separator in that higher-dimensional space, without ever actually computing the transformation explicitly.\n",
        "\n",
        "Mathematically, if we have two data points \\( \\mathbf{x}_i \\) and \\( \\mathbf{x}_j \\), the kernel function \\( K(\\mathbf{x}_i, \\mathbf{x}_j) \\) computes the **inner product** of their corresponding images in the higher-dimensional space. For most kernels, this inner product is equivalent to applying the SVM's decision function in the transformed feature space.\n",
        "\n",
        "### General Form of the Kernel Trick:\n",
        "Given that the decision function in a higher-dimensional space involves computing inner products between data points, the kernel trick allows us to replace this inner product with a **kernel function** \\( K(\\mathbf{x}_i, \\mathbf{x}_j) \\) without explicitly computing the transformation.\n",
        "\n",
        "The decision function for an SVM in the feature space is typically written as:\n",
        "\\[\n",
        "f(\\mathbf{x}) = \\sum_{i=1}^n \\alpha_i y_i K(\\mathbf{x}_i, \\mathbf{x}) + b\n",
        "\\]\n",
        "Where:\n",
        "- \\( \\alpha_i \\) are the Lagrange multipliers,\n",
        "- \\( y_i \\) are the class labels (+1 or -1),\n",
        "- \\( \\mathbf{x}_i \\) are the support vectors,\n",
        "- \\( b \\) is the bias term,\n",
        "- \\( K(\\mathbf{x}_i, \\mathbf{x}) \\) is the kernel function that computes the inner product in the higher-dimensional space.\n",
        "\n",
        "### Common Types of Kernel Functions:\n",
        "1. **Linear Kernel**:\n",
        "   - The **linear kernel** is used when the data is already linearly separable in its original feature space.\n",
        "   - The kernel function is simply the **dot product** of the two vectors:\n",
        "     \\[\n",
        "     K(\\mathbf{x}_i, \\mathbf{x}_j) = \\mathbf{x}_i \\cdot \\mathbf{x}_j\n",
        "     \\]\n",
        "   - This is equivalent to using no transformation at all and is the simplest kernel.\n",
        "\n",
        "2. **Polynomial Kernel**:\n",
        "   - The **polynomial kernel** is used when the data can be separated by a polynomial decision boundary.\n",
        "   - The kernel function is:\n",
        "     \\[\n",
        "     K(\\mathbf{x}_i, \\mathbf{x}_j) = (\\mathbf{x}_i \\cdot \\mathbf{x}_j + c)^d\n",
        "     \\]\n",
        "     Where \\( c \\) is a constant and \\( d \\) is the degree of the polynomial.\n",
        "   - This kernel allows for the creation of polynomial decision boundaries, which can handle more complex, curved relationships between the data points.\n",
        "\n",
        "3. **Radial Basis Function (RBF) Kernel** (also known as the **Gaussian Kernel**):\n",
        "   - The **RBF kernel** is the most commonly used kernel, especially when the data is highly non-linear.\n",
        "   - The kernel function is:\n",
        "     \\[\n",
        "     K(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp\\left(-\\frac{\\|\\mathbf{x}_i - \\mathbf{x}_j\\|^2}{2\\sigma^2}\\right)\n",
        "     \\]\n",
        "     Where \\( \\sigma \\) is a free parameter that controls the width of the kernel.\n",
        "   - The RBF kernel is powerful because it can map data into an infinite-dimensional feature space, allowing SVM to handle highly non-linear decision boundaries.\n",
        "\n",
        "4. **Sigmoid Kernel**:\n",
        "   - The **sigmoid kernel** is similar to the activation function used in neural networks.\n",
        "   - The kernel function is:\n",
        "     \\[\n",
        "     K(\\mathbf{x}_i, \\mathbf{x}_j) = \\tanh(\\mathbf{x}_i \\cdot \\mathbf{x}_j + c)\n",
        "     \\]\n",
        "   - It is less commonly used compared to the RBF and polynomial kernels but can still be useful in certain situations.\n",
        "\n",
        "5. **Custom Kernels**:\n",
        "   - In some cases, users may define their own kernel functions based on domain knowledge and the specific nature of the data.\n",
        "\n",
        "### Advantages of the Kernel Trick:\n",
        "1. **Non-linearity**: The kernel trick allows SVM to handle non-linear decision boundaries by mapping data into a higher-dimensional feature space where the classes may become linearly separable.\n",
        "2. **Efficient Computation**: By using the kernel function, SVM avoids the computationally expensive process of explicitly transforming data points into higher-dimensional spaces. This makes it feasible to solve problems with complex, non-linear relationships without needing to work in high-dimensional spaces directly.\n",
        "3. **Flexibility**: Different types of kernel functions (e.g., linear, polynomial, RBF) provide flexibility in dealing with various types of data distributions, making SVM adaptable to a wide range of problems.\n",
        "4. **Implicit Mapping**: The kernel trick enables SVM to perform a non-linear transformation implicitly, so you don’t need to manually compute the transformation or work in the higher-dimensional space explicitly.\n",
        "\n",
        "### Mathematical Insight Behind the Kernel Trick:\n",
        "Suppose you want to map data from the input space to a higher-dimensional feature space, say \\( \\Phi(\\mathbf{x}) \\). For example, a polynomial kernel corresponds to the mapping \\( \\Phi(\\mathbf{x}) = \\left[ x_1^2, x_1 x_2, x_2^2 \\right] \\) in a higher-dimensional space. However, directly computing this transformation can be computationally expensive. Instead, the kernel function \\( K(\\mathbf{x}_i, \\mathbf{x}_j) \\) computes the dot product of the transformed points \\( \\Phi(\\mathbf{x}_i) \\cdot \\Phi(\\mathbf{x}_j) \\), which gives the same result as if we had mapped the data explicitly but without actually having to compute the transformation.\n",
        "\n",
        "### Example Use Case:\n",
        "For example, in **image recognition**, the data (images) can be highly complex and non-linear. Using an **RBF kernel** allows SVM to map the image features into a higher-dimensional space, where a linear separator can be found, even though the images themselves may not be linearly separable in the original feature space.\n",
        "\n",
        "### Conclusion:\n",
        "The **kernel trick** is a fundamental concept in **Support Vector Machines (SVM)** that enables the algorithm to solve complex non-linear problems efficiently. By using kernel functions like **RBF**, **polynomial**, and others, SVM can map data into a higher-dimensional space where a linear decision boundary can be found, without the need to explicitly compute the transformation. This makes SVM a powerful tool for both **linear** and **non-linear** classification and regression problems."
      ],
      "metadata": {
        "id": "hwPdD-nvR-am"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Compare Linear Kernel, Polynomial Kernel, and RBF Kernel.\n"
      ],
      "metadata": {
        "id": "90tQ81hhR-XM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When using **Support Vector Machines (SVM)** for classification or regression, the choice of kernel is crucial, as it determines how the algorithm maps the input data into higher-dimensional spaces and finds the optimal decision boundary. The three most commonly used kernels in SVM are the **Linear Kernel**, **Polynomial Kernel**, and **Radial Basis Function (RBF) Kernel**. Each has its strengths and is suited for different types of problems.\n",
        "\n",
        "Let's compare these three kernels based on various characteristics:\n",
        "\n",
        "### 1. **Linear Kernel**\n",
        "#### Definition:\n",
        "The **linear kernel** is the simplest kernel function. It does not involve any transformation of the input data and is used when the data is linearly separable (or nearly so).\n",
        "\n",
        "Mathematically:\n",
        "\\[\n",
        "K(\\mathbf{x}_i, \\mathbf{x}_j) = \\mathbf{x}_i \\cdot \\mathbf{x}_j\n",
        "\\]\n",
        "where \\( \\mathbf{x}_i \\) and \\( \\mathbf{x}_j \\) are the feature vectors of the two data points.\n",
        "\n",
        "#### Characteristics:\n",
        "- **Use Case**: The linear kernel is ideal for linearly separable data, where a straight line (or hyperplane in higher dimensions) can separate the different classes.\n",
        "- **Computational Complexity**: Since no mapping to a higher-dimensional space is performed, the linear kernel is computationally efficient and fast.\n",
        "- **Decision Boundary**: The decision boundary is a **linear** hyperplane (a straight line in 2D, a plane in 3D, etc.).\n",
        "- **Interpretability**: The model is easier to interpret, as it provides a simple linear decision surface.\n",
        "\n",
        "#### Pros:\n",
        "- **Fast to train and predict** for linearly separable data.\n",
        "- **Less prone to overfitting** when the data is truly linear.\n",
        "- **Efficient** with large datasets.\n",
        "\n",
        "#### Cons:\n",
        "- **Limited to linear data**: If the data is not linearly separable, the linear kernel will perform poorly compared to non-linear kernels.\n",
        "\n",
        "#### Example Use Cases:\n",
        "- Text classification problems (e.g., spam vs. non-spam).\n",
        "- Linearly separable data in high-dimensional spaces (e.g., simple regression tasks).\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Polynomial Kernel**\n",
        "#### Definition:\n",
        "The **polynomial kernel** computes the similarity between data points in a higher-dimensional feature space, where the data is mapped to all possible monomials of the input features, up to a specified degree \\( d \\).\n",
        "\n",
        "Mathematically:\n",
        "\\[\n",
        "K(\\mathbf{x}_i, \\mathbf{x}_j) = (\\mathbf{x}_i \\cdot \\mathbf{x}_j + c)^d\n",
        "\\]\n",
        "Where:\n",
        "- \\( c \\) is a constant that controls the shift of the kernel.\n",
        "- \\( d \\) is the degree of the polynomial (e.g., \\( d = 2 \\) corresponds to quadratic features).\n",
        "\n",
        "#### Characteristics:\n",
        "- **Use Case**: The polynomial kernel is used when the decision boundary between classes is non-linear but can be represented by a polynomial (e.g., quadratic, cubic, etc.).\n",
        "- **Computational Complexity**: The polynomial kernel is more computationally expensive than the linear kernel, especially for higher degrees, as it involves calculating higher-dimensional polynomial terms.\n",
        "- **Decision Boundary**: The decision boundary is a **curved polynomial** (e.g., quadratic or cubic) depending on the degree \\( d \\).\n",
        "- **Flexibility**: By adjusting the degree \\( d \\), the polynomial kernel can capture a variety of non-linear patterns.\n",
        "\n",
        "#### Pros:\n",
        "- **Can capture polynomial relationships** between the data, providing flexibility for more complex patterns than the linear kernel.\n",
        "- **Suitable for medium-sized datasets** where the data may have a polynomial relationship but not an overly complex one.\n",
        "\n",
        "#### Cons:\n",
        "- **Prone to overfitting**: As \\( d \\) increases, the model may overfit, especially if the data is noisy or small.\n",
        "- **Computationally expensive**: Higher-degree polynomials require more computations, which can become inefficient for large datasets.\n",
        "- **Limited flexibility for very complex data**: It may not capture all non-linear relationships as well as the RBF kernel.\n",
        "\n",
        "#### Example Use Cases:\n",
        "- Polynomial regression problems.\n",
        "- Problems where the decision boundary is a smooth curve (e.g., quadratic decision surfaces in classification tasks).\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Radial Basis Function (RBF) Kernel**\n",
        "#### Definition:\n",
        "The **RBF kernel** (also known as the **Gaussian kernel**) is the most commonly used kernel in SVM. It computes the similarity between data points in a transformed infinite-dimensional space by measuring the distance between them.\n",
        "\n",
        "Mathematically:\n",
        "\\[\n",
        "K(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp\\left(-\\frac{\\|\\mathbf{x}_i - \\mathbf{x}_j\\|^2}{2\\sigma^2}\\right)\n",
        "\\]\n",
        "Where:\n",
        "- \\( \\|\\mathbf{x}_i - \\mathbf{x}_j\\|^2 \\) is the squared Euclidean distance between the two data points.\n",
        "- \\( \\sigma \\) is a free parameter that controls the width of the Gaussian function.\n",
        "\n",
        "#### Characteristics:\n",
        "- **Use Case**: The RBF kernel is highly effective for most non-linear problems where the decision boundary is complex and cannot be captured by a simple polynomial. It is often the default choice when you are unsure which kernel to use.\n",
        "- **Computational Complexity**: The RBF kernel is more computationally expensive than the linear kernel, but it is generally more flexible and can handle complex relationships in data.\n",
        "- **Decision Boundary**: The decision boundary is highly **non-linear**, allowing SVM to create complex curves and boundaries.\n",
        "- **Flexibility**: The RBF kernel is very flexible and can map data into an infinite-dimensional space, allowing it to handle highly non-linear problems.\n",
        "\n",
        "#### Pros:\n",
        "- **Highly flexible and powerful**: It can model complex, non-linear decision boundaries.\n",
        "- **Effective for complex datasets**: Suitable for real-world datasets with non-linear relationships, especially when other kernels fail.\n",
        "- **Handles high-dimensional spaces efficiently**: The kernel trick allows SVM to work in high-dimensional feature spaces without explicitly computing the transformation.\n",
        "\n",
        "#### Cons:\n",
        "- **Sensitive to the choice of \\( \\sigma \\)**: A small \\( \\sigma \\) leads to a narrow decision region, and a large \\( \\sigma \\) leads to a smoother decision boundary that may not capture the data well. Tuning \\( \\sigma \\) can be tricky.\n",
        "- **Prone to overfitting**: If \\( \\sigma \\) is too small, the model may overfit the training data.\n",
        "- **Computationally expensive**: It requires more time and resources compared to the linear kernel, especially for large datasets.\n",
        "\n",
        "#### Example Use Cases:\n",
        "- Image classification (e.g., facial recognition).\n",
        "- Non-linear decision boundary problems like XOR, digit recognition, etc.\n",
        "\n",
        "---\n",
        "\n",
        "### Summary Comparison:\n",
        "\n",
        "| Kernel            | Decision Boundary | Use Case                           | Pros                                      | Cons                                      |\n",
        "|-------------------|-------------------|------------------------------------|-------------------------------------------|-------------------------------------------|\n",
        "| **Linear Kernel**  | Linear            | Linearly separable data            | Fast, efficient, simple                   | Only works for linear data                |\n",
        "| **Polynomial Kernel** | Polynomial (curved) | Moderate complexity, non-linear data | Flexible, captures polynomial relationships | Prone to overfitting, computationally expensive |\n",
        "| **RBF Kernel**     | Highly non-linear | Complex, non-linear data           | Very flexible, handles complex data well  | Sensitive to \\( \\sigma \\), computationally expensive |\n",
        "\n",
        "### Conclusion:\n",
        "- **Linear Kernel** is best for simple, linearly separable data.\n",
        "- **Polynomial Kernel** is useful when the decision boundary is polynomial (e.g., quadratic or cubic), and you want more flexibility than a linear kernel but still with manageable complexity.\n",
        "- **RBF Kernel** is the most versatile and widely used, especially for complex, non-linear datasets, but it requires careful tuning of the \\( \\sigma \\) parameter and can be computationally intensive.\n",
        "\n",
        "The choice of kernel depends on the complexity of the data and the nature of the decision boundary. For simple problems, a linear kernel might suffice, while for more complex, non-linear problems, the RBF kernel is often the best choice."
      ],
      "metadata": {
        "id": "BH7SUIGuR-UJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What is the effect of the C parameter in SVM?\n"
      ],
      "metadata": {
        "id": "gzapcRM9R-RD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **C parameter** in Support Vector Machines (SVM) plays a crucial role in controlling the trade-off between achieving a low error on the training data and maintaining a simpler (or smoother) decision boundary. It essentially determines the **penalty for misclassification** and influences how the model fits the data.\n",
        "\n",
        "### Effect of the **C** Parameter in SVM:\n",
        "\n",
        "The **C parameter** controls the **regularization** in SVM. It acts as a **penalty parameter** that adjusts the decision boundary based on the cost of misclassification. The value of **C** has a direct impact on the model’s bias-variance trade-off, influencing the generalization ability of the SVM.\n",
        "\n",
        "#### 1. **High C (Low Regularization)**\n",
        "- **Interpretation**: When \\( C \\) is **large**, the model tries to **minimize the classification error** on the training data, meaning it aims to correctly classify every training example, even if that means creating a complex decision boundary.\n",
        "- **Effect**:\n",
        "  - The model **pays a high penalty** for misclassifying training data points, leading to fewer misclassifications (even at the expense of a more complex boundary).\n",
        "  - The decision boundary may become **overfitted** to the training data, which can result in poor generalization to unseen test data.\n",
        "  - The SVM will **tolerate less margin**, and the support vectors will be tightly packed around the decision boundary.\n",
        "\n",
        "- **When to Use**: A large \\( C \\) value is useful when the training data is relatively clean (i.e., there are few outliers or misclassifications) and you want to build a model that fits the training data as closely as possible.\n",
        "\n",
        "#### 2. **Low C (High Regularization)**\n",
        "- **Interpretation**: When \\( C \\) is **small**, the model is more **tolerant of misclassification**. It allows some errors in order to have a simpler decision boundary, with the aim of improving generalization.\n",
        "- **Effect**:\n",
        "  - The model **allows more misclassifications** in order to increase the margin between classes, resulting in a smoother decision boundary.\n",
        "  - A low \\( C \\) value leads to a **larger margin** but might not perfectly classify every training example.\n",
        "  - The decision boundary is more likely to **generalize better** to unseen data because it is less sensitive to noise and outliers in the training data.\n",
        "  \n",
        "- **When to Use**: A small \\( C \\) is useful when the data is noisy, contains outliers, or when generalization to new data is more important than fitting the training data exactly.\n",
        "\n",
        "### Mathematical Insight:\n",
        "The SVM optimization problem involves finding a **hyperplane** that separates the classes with the maximum margin. This optimization can be expressed as:\n",
        "\n",
        "\\[\n",
        "\\min \\left( \\frac{1}{2} \\| w \\|^2 \\right)\n",
        "\\]\n",
        "subject to:\n",
        "\\[\n",
        "y_i (w \\cdot x_i + b) \\geq 1 - \\xi_i \\quad \\text{for all } i\n",
        "\\]\n",
        "Where:\n",
        "- \\( w \\) is the weight vector (defining the hyperplane),\n",
        "- \\( x_i \\) is a training sample,\n",
        "- \\( y_i \\) is the label (either +1 or -1),\n",
        "- \\( \\xi_i \\) is the slack variable (representing the misclassification of the \\(i\\)-th data point).\n",
        "\n",
        "The **C** parameter is introduced as part of the objective function to balance the margin size and classification errors:\n",
        "\\[\n",
        "\\min \\left( \\frac{1}{2} \\| w \\|^2 + C \\sum_{i=1}^n \\xi_i \\right)\n",
        "\\]\n",
        "Here, \\( C \\) controls the **penalty** for misclassified points, and adjusting its value alters the trade-off between maximizing the margin and minimizing the misclassification error.\n",
        "\n",
        "#### 3. **Impact on the Decision Boundary and Support Vectors**:\n",
        "- **High C**: The SVM will focus on minimizing the training error, and it might create a highly complex boundary that fits the training data perfectly but risks overfitting.\n",
        "- **Low C**: The SVM will allow for more misclassifications, but the decision boundary will be smoother, possibly leading to better performance on unseen data (less overfitting).\n",
        "\n",
        "### Summary of the Effect of **C**:\n",
        "\n",
        "| **C Value**   | **Regularization** | **Effect on Margin**        | **Model Complexity**         | **Risk of Overfitting** |\n",
        "|---------------|---------------------|-----------------------------|------------------------------|-------------------------|\n",
        "| **High C**    | Low regularization  | Smaller margin, tighter to the data points | More complex, low bias       | High (overfitting)      |\n",
        "| **Low C**     | High regularization | Larger margin, more tolerance for misclassifications | Simpler, higher bias        | Low (underfitting)     |\n",
        "\n",
        "### Practical Tuning:\n",
        "The choice of **C** is problem-dependent, and it is often tuned through cross-validation. Typically, a small \\( C \\) value is preferred when dealing with noisy data, while a larger \\( C \\) is useful for cleaner, well-separated data. It’s important to try different values of **C** and evaluate the model performance on a validation set to find the optimal value that balances the trade-off between bias and variance.\n",
        "\n",
        "### Conclusion:\n",
        "The **C parameter** in SVM is a critical hyperparameter that controls the **trade-off between the margin size and classification error**. A high value of **C** leads to a model that tries to classify all training points correctly but may overfit, while a low value of **C** results in a simpler model that might not classify the training data perfectly but generalizes better. Fine-tuning **C** through cross-validation is key to achieving good model performance."
      ],
      "metadata": {
        "id": "v3c_F2HuR-Hm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. What is the role of the Gamma parameter in RBF Kernel SVM?\n"
      ],
      "metadata": {
        "id": "th1JK2FpR-FA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **Gamma** parameter in **Radial Basis Function (RBF) Kernel** SVM plays a crucial role in determining the **shape** and **complexity** of the decision boundary that the SVM learns. It controls the influence of a single training sample on the decision boundary and how the RBF kernel computes the similarity between data points.\n",
        "\n",
        "### Mathematical Definition of Gamma:\n",
        "For an RBF kernel, the kernel function is given by:\n",
        "\n",
        "\\[\n",
        "K(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp\\left(-\\frac{\\|\\mathbf{x}_i - \\mathbf{x}_j\\|^2}{2 \\gamma^2}\\right)\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- \\( \\|\\mathbf{x}_i - \\mathbf{x}_j\\|^2 \\) is the squared Euclidean distance between the two data points \\( \\mathbf{x}_i \\) and \\( \\mathbf{x}_j \\).\n",
        "- **Gamma** (\\( \\gamma \\)) is a positive real number that defines the width of the Gaussian function and determines how far the influence of a single training point reaches.\n",
        "\n",
        "### Role and Effect of Gamma in RBF Kernel:\n",
        "Gamma has a significant impact on how the SVM model fits the data. It controls the **spread** or **reach** of the influence of each training point on the decision boundary.\n",
        "\n",
        "#### 1. **High Gamma (Small value of \\( \\gamma \\))**:\n",
        "- **Interpretation**: A **high value of Gamma** means that the **Gaussian function is narrow**, so each training point has a very **local** influence.\n",
        "- **Effect on Model**:\n",
        "  - The model will try to fit the training data **more closely**, meaning the decision boundary will be **complex** and will follow the noise in the data.\n",
        "  - It is prone to **overfitting** since the decision boundary can become highly intricate, even for small deviations in data.\n",
        "  - The influence of each training point is restricted to a very small region, leading to sharp decision boundaries.\n",
        "\n",
        "- **When to Use**: A high Gamma is useful when the data is relatively simple and well-separated, and you want to capture small-scale patterns in the data.\n",
        "\n",
        "#### 2. **Low Gamma (Large value of \\( \\gamma \\))**:\n",
        "- **Interpretation**: A **low value of Gamma** means that the **Gaussian function is wide**, so each training point has a **broader influence**.\n",
        "- **Effect on Model**:\n",
        "  - The model will create a **smoother decision boundary**, and it will try to fit the general trend of the data rather than the specifics.\n",
        "  - A low Gamma value leads to **underfitting**, as the decision boundary is too simple to capture more complex relationships in the data.\n",
        "  - It helps to create a more **generalized model** that performs well on unseen data.\n",
        "\n",
        "- **When to Use**: A low Gamma is useful when the data is noisy or has complex relationships that need a smoother, more generalized decision boundary.\n",
        "\n",
        "### Summary of the Effects of Gamma:\n",
        "\n",
        "| **Gamma Value**   | **Influence of Data Points**   | **Model Complexity**          | **Risk of Overfitting** | **Risk of Underfitting** |\n",
        "|-------------------|--------------------------------|-------------------------------|-------------------------|--------------------------|\n",
        "| **High Gamma**    | Localized (points have local influence) | Complex, tight decision boundary | High (overfitting)       | Low                      |\n",
        "| **Low Gamma**     | Broad influence (points have global influence) | Smoother decision boundary   | Low                      | High (underfitting)      |\n",
        "\n",
        "### Intuition Behind Gamma:\n",
        "- **High Gamma**: When Gamma is high, the SVM model treats every point as highly influential only for nearby points. The decision boundary bends sharply to adapt to the specifics of each point, which can lead to overfitting.\n",
        "- **Low Gamma**: When Gamma is low, the influence of each data point spreads across the entire feature space, leading to a smoother, simpler decision boundary. This can result in underfitting if the decision boundary is too simple for the data.\n",
        "\n",
        "### Tuning Gamma:\n",
        "- **Gamma Selection**: The optimal value of Gamma is typically found through **cross-validation**. A good choice of Gamma helps to balance the bias-variance trade-off.\n",
        "- **Effect on the Margin**: A higher Gamma leads to a narrower margin, and a lower Gamma results in a wider margin.\n",
        "\n",
        "### Practical Considerations:\n",
        "- **Gamma and C Parameter Relationship**: Both the **C** (regularization) and **Gamma** parameters need to be tuned together. If you set \\( C \\) high (low regularization), the model will try to classify every point correctly, and high \\( \\gamma \\) can lead to overfitting. On the other hand, if \\( C \\) is low (high regularization), the model might not learn enough from the training data, and the low \\( \\gamma \\) might underfit the model.\n",
        "\n",
        "- **Scaling of Data**: Before tuning Gamma, it's important to **scale your features**. Since Gamma is sensitive to the scale of the data, it works best when all features are on the same scale, often achieved by standardizing the data.\n",
        "\n",
        "### Conclusion:\n",
        "- **Gamma** plays a critical role in determining how the SVM classifies the data by defining the influence of individual data points. A **high Gamma** leads to a more complex, overfitted model, while a **low Gamma** results in a smoother, potentially underfitted model.\n",
        "- Tuning Gamma properly, along with the **C** parameter, is key to achieving optimal model performance and ensuring that the SVM generalizes well to new, unseen data."
      ],
      "metadata": {
        "id": "JPHDFHFuR-BE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n"
      ],
      "metadata": {
        "id": "I37zO_lKR99a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **Naïve Bayes classifier** is a probabilistic machine learning algorithm based on **Bayes' Theorem** and the assumption that the features (or attributes) of a dataset are **independent** of each other, given the class label. Despite this simplifying assumption, it has been found to be surprisingly effective, especially for certain types of problems, such as text classification.\n",
        "\n",
        "### Bayes' Theorem:\n",
        "Bayes' Theorem is a fundamental principle in probability theory that describes the relationship between conditional probabilities. In the context of the Naïve Bayes classifier, Bayes' Theorem is used to update the probability estimate for a class given the observed features.\n",
        "\n",
        "The theorem states:\n",
        "\\[\n",
        "P(C_k | X) = \\frac{P(X | C_k) P(C_k)}{P(X)}\n",
        "\\]\n",
        "Where:\n",
        "- \\( P(C_k | X) \\) is the **posterior probability** of class \\( C_k \\) given the features \\( X \\).\n",
        "- \\( P(X | C_k) \\) is the **likelihood** of observing the features \\( X \\) given class \\( C_k \\).\n",
        "- \\( P(C_k) \\) is the **prior probability** of class \\( C_k \\).\n",
        "- \\( P(X) \\) is the **evidence** (the probability of observing the features across all classes), which is a normalizing constant.\n",
        "\n",
        "The goal of Naïve Bayes is to predict the class \\( C_k \\) that maximizes the posterior probability \\( P(C_k | X) \\). In practice, since \\( P(X) \\) is constant for all classes, the Naïve Bayes classifier simplifies to:\n",
        "\\[\n",
        "P(C_k | X) \\propto P(C_k) \\prod_{i=1}^n P(x_i | C_k)\n",
        "\\]\n",
        "Where:\n",
        "- \\( x_i \\) represents the individual features of the input \\( X \\),\n",
        "- \\( P(x_i | C_k) \\) is the **likelihood** of feature \\( x_i \\) given class \\( C_k \\),\n",
        "- \\( P(C_k) \\) is the **prior** probability of class \\( C_k \\).\n",
        "\n",
        "This simplification allows the classifier to compute the posterior probability for each class and select the class with the highest value.\n",
        "\n",
        "### Why is it called \"Naïve\"?\n",
        "The term **\"Naïve\"** refers to the **assumption of feature independence** in the model. Specifically, the Naïve Bayes classifier assumes that:\n",
        "- The features (or attributes) are conditionally independent given the class label.\n",
        "\n",
        "This is a **strong and unrealistic assumption** because, in many real-world datasets, features often have complex dependencies with one another. For example, in a spam email classification problem, words like \"free\" and \"offer\" are likely correlated, but Naïve Bayes treats them as if they were independent when computing the likelihood.\n",
        "\n",
        "Despite this \"naïve\" assumption, the classifier can perform very well in practice, especially when the independence assumption is approximately valid or when the features contribute individually to the class label.\n",
        "\n",
        "### Types of Naïve Bayes Classifiers:\n",
        "There are different types of Naïve Bayes classifiers based on the nature of the features (whether they are continuous or discrete):\n",
        "\n",
        "1. **Multinomial Naïve Bayes**:\n",
        "   - Used when the features are **discrete** and typically arise from counting occurrences, such as word counts in text classification (e.g., spam detection, sentiment analysis).\n",
        "   - The likelihood term \\( P(x_i | C_k) \\) is modeled as a multinomial distribution.\n",
        "\n",
        "2. **Gaussian Naïve Bayes**:\n",
        "   - Used when the features are **continuous** and assumed to follow a **Gaussian (normal) distribution**.\n",
        "   - The likelihood \\( P(x_i | C_k) \\) is modeled using a Gaussian distribution:\n",
        "   \\[\n",
        "   P(x_i | C_k) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(x_i - \\mu)^2}{2\\sigma^2} \\right)\n",
        "   \\]\n",
        "   Where \\( \\mu \\) is the mean and \\( \\sigma^2 \\) is the variance of the feature \\( x_i \\) for class \\( C_k \\).\n",
        "\n",
        "3. **Bernoulli Naïve Bayes**:\n",
        "   - Used when the features are binary (e.g., word presence or absence in text classification tasks).\n",
        "   - The likelihood \\( P(x_i | C_k) \\) is modeled as a **Bernoulli distribution**.\n",
        "\n",
        "### Why is Naïve Bayes Effective?\n",
        "Even though the independence assumption is often unrealistic, **Naïve Bayes** performs well for several reasons:\n",
        "- **Simplicity**: It is easy to implement, requires relatively few resources, and has low computational cost.\n",
        "- **Good performance on text data**: Naïve Bayes is particularly effective for text classification problems (like spam filtering and sentiment analysis), where the features (words) are relatively independent, and the model can learn well from frequencies or probabilities.\n",
        "- **Handles high-dimensional data well**: Naïve Bayes classifiers can perform surprisingly well even when the number of features is large, as in text classification tasks where you have a large vocabulary.\n",
        "- **Works well with small datasets**: Due to its simplicity, Naïve Bayes does not require large amounts of training data to perform well, especially in the case of categorical or discrete features.\n",
        "\n",
        "### Example:\n",
        "For example, in a **spam email classification** task, the features could be the presence or absence of certain words in the email (like \"free\", \"win\", \"offer\"). The Naïve Bayes classifier would calculate the probability of an email being spam given the words present, assuming that the presence of each word is independent of the others.\n",
        "\n",
        "### Advantages of Naïve Bayes:\n",
        "- **Simple and fast**: The algorithm is fast to train and test, even on large datasets.\n",
        "- **Works well with high-dimensional data**: Particularly effective for text classification.\n",
        "- **Handles missing data**: Can handle missing values by ignoring features with missing values during training.\n",
        "- **Probabilistic output**: It provides probabilities for each class, which can be useful in applications where uncertainty or confidence levels matter.\n",
        "\n",
        "### Disadvantages of Naïve Bayes:\n",
        "- **Independence assumption**: The \"naïve\" assumption that all features are independent can be incorrect and limit its performance if the features are highly correlated.\n",
        "- **Sensitive to imbalanced data**: Naïve Bayes may perform poorly when the class distribution is heavily skewed unless proper handling (such as balancing or adjusting priors) is done.\n",
        "\n",
        "### Conclusion:\n",
        "The **Naïve Bayes classifier** is a simple, probabilistic classifier based on Bayes' Theorem and the assumption of feature independence. The term **\"naïve\"** comes from this assumption, which is often unrealistic in real-world data, but the classifier can still perform very well, especially when the features are indeed relatively independent or when the model is applied to text classification tasks."
      ],
      "metadata": {
        "id": "8WX2xkmuR96a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What is Bayes' Theorem?\n"
      ],
      "metadata": {
        "id": "oeQDFF-QR935"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bayes' Theorem** is a fundamental principle in probability theory that describes the relationship between conditional probabilities. It provides a way to update the probability of a hypothesis (or event) based on new evidence or data.\n",
        "\n",
        "Bayes' Theorem is named after the Reverend **Thomas Bayes**, who first introduced it in the 18th century. The theorem allows us to calculate the **posterior probability** of an event based on prior knowledge and new evidence.\n",
        "\n",
        "### Mathematical Form of Bayes' Theorem:\n",
        "The theorem is expressed as:\n",
        "\n",
        "\\[\n",
        "P(H | E) = \\frac{P(E | H) \\cdot P(H)}{P(E)}\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- \\( P(H | E) \\) is the **posterior probability**: the probability of the hypothesis \\( H \\) being true, given the evidence \\( E \\).\n",
        "- \\( P(E | H) \\) is the **likelihood**: the probability of observing the evidence \\( E \\) given that the hypothesis \\( H \\) is true.\n",
        "- \\( P(H) \\) is the **prior probability**: the initial probability of the hypothesis \\( H \\) before seeing the evidence \\( E \\).\n",
        "- \\( P(E) \\) is the **evidence**: the total probability of observing the evidence \\( E \\), which can be calculated as the sum of the likelihood of all possible hypotheses.\n",
        "\n",
        "### Intuition Behind Bayes' Theorem:\n",
        "- **Prior Probability** \\( P(H) \\): This is our belief about the hypothesis \\( H \\) before seeing any new evidence. It's based on prior knowledge or assumptions.\n",
        "- **Likelihood** \\( P(E | H) \\): This tells us how likely the evidence is, assuming the hypothesis \\( H \\) is true.\n",
        "- **Posterior Probability** \\( P(H | E) \\): This is the updated probability of the hypothesis being true after incorporating the new evidence \\( E \\). It is the probability we are trying to calculate.\n",
        "- **Evidence** \\( P(E) \\): This normalizing constant ensures that the posterior probabilities sum to 1. It's calculated by considering all possible hypotheses.\n",
        "\n",
        "### Example: Disease Diagnosis\n",
        "Suppose you want to calculate the probability that a patient has a particular disease given a positive test result. The components in Bayes' Theorem would be:\n",
        "\n",
        "- **Hypothesis \\( H \\)**: The patient has the disease.\n",
        "- **Evidence \\( E \\)**: The patient tests positive for the disease.\n",
        "\n",
        "Now, to apply Bayes' Theorem:\n",
        "- **Prior Probability \\( P(H) \\)**: The probability that a randomly selected person has the disease (this might be known from statistics or medical history).\n",
        "- **Likelihood \\( P(E | H) \\)**: The probability that the test returns a positive result if the person has the disease (this is the sensitivity of the test).\n",
        "- **Evidence \\( P(E) \\)**: The probability of getting a positive test result in the population, which includes both people who have the disease and those who do not (this is often derived from both the sensitivity and specificity of the test).\n",
        "\n",
        "Using Bayes' Theorem, you can calculate the **posterior probability** that the patient actually has the disease given that they tested positive.\n",
        "\n",
        "### Applications of Bayes' Theorem:\n",
        "Bayes' Theorem is used in various fields and applications, including:\n",
        "\n",
        "1. **Machine Learning**:\n",
        "   - In **Naïve Bayes classifiers**, Bayes' Theorem is used to classify data based on prior knowledge and feature evidence.\n",
        "   - In **Bayesian inference**, it is used to update the probability of a hypothesis given new data.\n",
        "\n",
        "2. **Medical Diagnosis**:\n",
        "   - Bayes' Theorem helps update the probability of a disease diagnosis based on test results or symptoms.\n",
        "\n",
        "3. **Spam Filtering**:\n",
        "   - Bayes' Theorem can be used to classify emails as spam or not based on the occurrence of certain words or features in the email.\n",
        "\n",
        "4. **Risk Assessment**:\n",
        "   - In finance or insurance, Bayes' Theorem is used to assess the probability of a certain financial event occurring, given prior data and new market conditions.\n",
        "\n",
        "5. **Decision Making**:\n",
        "   - It is used in decision theory to update beliefs and make informed decisions under uncertainty.\n",
        "\n",
        "### Example Calculation:\n",
        "Let’s walk through a simple example:\n",
        "\n",
        "You are trying to determine if someone has a particular disease based on a positive test result.\n",
        "\n",
        "- **Prior Probability**: Suppose the disease affects 1% of the population, so \\( P(\\text{Disease}) = 0.01 \\).\n",
        "- **Likelihood**: The test has a sensitivity (true positive rate) of 95%, so \\( P(\\text{Positive Test} | \\text{Disease}) = 0.95 \\).\n",
        "- **Evidence**: The probability of a positive test result, which is a combination of true positives and false positives, say \\( P(\\text{Positive Test}) = 0.10 \\).\n",
        "\n",
        "Using Bayes' Theorem:\n",
        "\\[\n",
        "P(\\text{Disease} | \\text{Positive Test}) = \\frac{P(\\text{Positive Test} | \\text{Disease}) \\cdot P(\\text{Disease})}{P(\\text{Positive Test})}\n",
        "\\]\n",
        "Substituting the values:\n",
        "\\[\n",
        "P(\\text{Disease} | \\text{Positive Test}) = \\frac{0.95 \\cdot 0.01}{0.10} = 0.095\n",
        "\\]\n",
        "Thus, even though the test is 95% accurate in detecting the disease, the probability that a person has the disease given a positive test result is still only 9.5%. This is because the disease is rare, and the false positive rate affects the result.\n",
        "\n",
        "### Conclusion:\n",
        "Bayes' Theorem is a powerful tool for updating probabilities based on new evidence. It is foundational in fields like statistics, machine learning, and decision-making, where we need to make inferences or predictions based on uncertain or incomplete information. The ability to revise beliefs (probabilities) as new data arrives is what makes Bayes' Theorem so valuable in many practical applications."
      ],
      "metadata": {
        "id": "41zqqsesR9zZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naive Bayes.\n"
      ],
      "metadata": {
        "id": "a9k_n_joR9wm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **Naïve Bayes** classifier is a family of probabilistic classifiers based on **Bayes' Theorem**. There are different variations of Naïve Bayes, each suited to different types of data. The three most common variants are **Gaussian Naïve Bayes**, **Multinomial Naïve Bayes**, and **Bernoulli Naïve Bayes**. They differ in how they model the distribution of the features (independent variables) for each class.\n",
        "\n",
        "### 1. **Gaussian Naïve Bayes (GNB)**:\n",
        "- **Used For**: Continuous data where the features are assumed to follow a **Gaussian (Normal) distribution**.\n",
        "- **Assumption**: The features are normally distributed within each class. This means that for each class, the features are assumed to follow a bell-shaped curve (Gaussian distribution).\n",
        "- **Likelihood Estimation**: For each feature \\( x_i \\), the likelihood \\( P(x_i | C_k) \\) is modeled as a Gaussian distribution with a mean \\( \\mu \\) and variance \\( \\sigma^2 \\) specific to each class \\( C_k \\):\n",
        "  \\[\n",
        "  P(x_i | C_k) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp \\left( -\\frac{(x_i - \\mu)^2}{2\\sigma^2} \\right)\n",
        "  \\]\n",
        "- **When to Use**: This is used when the data consists of continuous variables (e.g., height, weight, age) and can be assumed to follow a normal distribution. It's commonly used for classification tasks involving continuous data, such as predicting whether a person has a disease based on continuous medical measurements.\n",
        "\n",
        "#### Key Points:\n",
        "- Assumes features are **independent** and **normally distributed**.\n",
        "- Works well when the features are continuous.\n",
        "- Suitable for problems where you can reasonably assume that the data fits a normal distribution.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Multinomial Naïve Bayes (MNB)**:\n",
        "- **Used For**: Discrete count data, especially for text classification problems (e.g., spam detection, sentiment analysis).\n",
        "- **Assumption**: The features are discrete and follow a **Multinomial distribution**. The Multinomial distribution is commonly used to model counts or frequencies.\n",
        "- **Likelihood Estimation**: For each class \\( C_k \\), the likelihood of a feature \\( x_i \\) (such as the count of a word in text) is modeled by a multinomial distribution:\n",
        "  \\[\n",
        "  P(x_i | C_k) = \\frac{(N_{i,k})!}{\\prod_{i=1}^m N_{i,k}!} \\prod_{i=1}^m p_i^{N_{i,k}}\n",
        "  \\]\n",
        "  Where \\( N_{i,k} \\) is the count of feature \\( x_i \\) in class \\( C_k \\), and \\( p_i \\) is the probability of feature \\( x_i \\) occurring in class \\( C_k \\).\n",
        "- **When to Use**: Multinomial Naïve Bayes is most commonly used in **text classification** tasks, where the features represent the frequencies or counts of words in a document. It assumes that each feature is the count of occurrences of a word or token, and the distribution of these word counts follows a multinomial distribution.\n",
        "\n",
        "#### Key Points:\n",
        "- Assumes features are **discrete** counts or frequencies (e.g., number of occurrences of a word).\n",
        "- Suitable for text classification or any other problem where features represent the frequency of events.\n",
        "- Works well for problems where the features represent **counts** or **frequencies**.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Bernoulli Naïve Bayes (BNB)**:\n",
        "- **Used For**: Binary data, where features are binary (i.e., they represent the presence or absence of a particular feature).\n",
        "- **Assumption**: Each feature follows a **Bernoulli distribution**. For each feature \\( x_i \\), it is assumed to be a binary random variable (either 0 or 1), representing the absence or presence of a particular attribute.\n",
        "- **Likelihood Estimation**: For each class \\( C_k \\), the likelihood \\( P(x_i | C_k) \\) is modeled using a Bernoulli distribution:\n",
        "  \\[\n",
        "  P(x_i | C_k) = p_i^{x_i} (1 - p_i)^{(1 - x_i)}\n",
        "  \\]\n",
        "  Where:\n",
        "  - \\( x_i \\) is the binary value of feature \\( x_i \\) (0 or 1),\n",
        "  - \\( p_i \\) is the probability of feature \\( x_i \\) being 1 given the class \\( C_k \\).\n",
        "- **When to Use**: Bernoulli Naïve Bayes is appropriate when your features are **binary**, such as in the case of document classification, where each feature could represent the presence (1) or absence (0) of a specific word in a document.\n",
        "\n",
        "#### Key Points:\n",
        "- Assumes features are **binary** (0 or 1), representing the presence or absence of a particular attribute.\n",
        "- Suitable for binary classification tasks or text classification problems where the features represent binary events (such as the presence or absence of a word).\n",
        "- Works well for problems where the data consists of **binary attributes**.\n",
        "\n",
        "---\n",
        "\n",
        "### Summary of Differences:\n",
        "\n",
        "| **Characteristic**            | **Gaussian Naïve Bayes**                 | **Multinomial Naïve Bayes**                 | **Bernoulli Naïve Bayes**                     |\n",
        "|-------------------------------|-----------------------------------------|-------------------------------------------|---------------------------------------------|\n",
        "| **Data Type**                 | Continuous (real-valued features)       | Discrete counts (e.g., word frequencies)   | Binary (0 or 1, presence or absence)        |\n",
        "| **Distribution Assumption**   | Features are **normally distributed**   | Features follow a **multinomial distribution** | Features follow a **Bernoulli distribution** |\n",
        "| **Likelihood Estimation**     | Gaussian (Normal) distribution          | Multinomial distribution of counts        | Bernoulli distribution (binary outcomes)    |\n",
        "| **Common Use Case**           | Continuous data (e.g., predicting diseases based on medical measurements) | Text classification with counts (e.g., spam detection, sentiment analysis) | Text classification with binary features (e.g., word presence/absence) |\n",
        "| **Examples**                  | Medical data, stock prices, age prediction | Document classification, text categorization | Document classification, spam detection    |\n",
        "\n",
        "### When to Choose Each:\n",
        "- **Gaussian Naïve Bayes**: Choose this when your features are continuous and you can assume they follow a normal distribution.\n",
        "- **Multinomial Naïve Bayes**: This is a good choice when dealing with **discrete count data**, particularly in text classification tasks where the features represent word counts or term frequencies.\n",
        "- **Bernoulli Naïve Bayes**: Best used when your features are binary, i.e., the presence or absence of certain characteristics (e.g., presence of specific words in a document).\n",
        "\n",
        "Each of these variants of Naïve Bayes has its strengths, and the choice of which one to use depends on the nature of your data and the problem you're trying to solve."
      ],
      "metadata": {
        "id": "vNODuIu0R9tu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naive Bayes.\n"
      ],
      "metadata": {
        "id": "2GSPXm6SWTgH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **Naïve Bayes** classifier is a family of probabilistic classifiers that are based on **Bayes' Theorem**, and they differ mainly in how they handle the features (input variables). The three most commonly used variants of Naïve Bayes are **Gaussian Naïve Bayes**, **Multinomial Naïve Bayes**, and **Bernoulli Naïve Bayes**. Each variant is suited for different types of data.\n",
        "\n",
        "### 1. **Gaussian Naïve Bayes (GNB)**:\n",
        "- **Used For**: Continuous data where the features are assumed to follow a **Gaussian (Normal) distribution**.\n",
        "- **Assumption**: The features are normally distributed within each class. This means that for each class, the features are assumed to follow a bell-shaped curve (Gaussian distribution).\n",
        "- **Likelihood Estimation**: For each feature \\( x_i \\), the likelihood \\( P(x_i | C_k) \\) is modeled as a Gaussian distribution with a mean \\( \\mu \\) and variance \\( \\sigma^2 \\) specific to each class \\( C_k \\):\n",
        "  \\[\n",
        "  P(x_i | C_k) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp \\left( -\\frac{(x_i - \\mu)^2}{2\\sigma^2} \\right)\n",
        "  \\]\n",
        "- **When to Use**: Gaussian Naïve Bayes is used when the data consists of **continuous variables** (e.g., height, weight, age) and can be assumed to follow a normal distribution. It's commonly used in scenarios where features are continuous and fit the assumption of normality, such as predicting whether a person has a disease based on continuous medical measurements.\n",
        "\n",
        "#### Key Points:\n",
        "- Assumes features are **independent** and **normally distributed**.\n",
        "- Works well with **continuous data**.\n",
        "- Often used for tasks like medical diagnosis with continuous variables.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Multinomial Naïve Bayes (MNB)**:\n",
        "- **Used For**: Discrete count data, especially for text classification problems (e.g., spam detection, sentiment analysis).\n",
        "- **Assumption**: The features are discrete and follow a **Multinomial distribution**. This is commonly used to model the occurrence of events (such as word counts).\n",
        "- **Likelihood Estimation**: For each class \\( C_k \\), the likelihood of a feature \\( x_i \\) (such as the count of a word in text) is modeled by a multinomial distribution:\n",
        "  \\[\n",
        "  P(x_i | C_k) = \\frac{(N_{i,k})!}{\\prod_{i=1}^m N_{i,k}!} \\prod_{i=1}^m p_i^{N_{i,k}}\n",
        "  \\]\n",
        "  Where \\( N_{i,k} \\) is the count of feature \\( x_i \\) in class \\( C_k \\), and \\( p_i \\) is the probability of feature \\( x_i \\) occurring in class \\( C_k \\).\n",
        "- **When to Use**: Multinomial Naïve Bayes is most commonly used for **text classification** tasks, where the features represent the frequencies or counts of words in a document. It assumes that each feature is the count of occurrences of a word or token, and the distribution of these word counts follows a multinomial distribution.\n",
        "\n",
        "#### Key Points:\n",
        "- Assumes features are **discrete** counts or frequencies (e.g., number of occurrences of a word).\n",
        "- Suitable for **text classification** and other problems where features represent the frequency of events (such as word counts).\n",
        "- Works well with **discrete count data**.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Bernoulli Naïve Bayes (BNB)**:\n",
        "- **Used For**: Binary data, where features are binary (i.e., they represent the presence or absence of a particular feature).\n",
        "- **Assumption**: Each feature follows a **Bernoulli distribution**. For each feature \\( x_i \\), it is assumed to be a binary random variable (either 0 or 1), representing the absence or presence of a particular attribute.\n",
        "- **Likelihood Estimation**: For each class \\( C_k \\), the likelihood \\( P(x_i | C_k) \\) is modeled using a Bernoulli distribution:\n",
        "  \\[\n",
        "  P(x_i | C_k) = p_i^{x_i} (1 - p_i)^{(1 - x_i)}\n",
        "  \\]\n",
        "  Where:\n",
        "  - \\( x_i \\) is the binary value of feature \\( x_i \\) (0 or 1),\n",
        "  - \\( p_i \\) is the probability of feature \\( x_i \\) being 1 given the class \\( C_k \\).\n",
        "- **When to Use**: Bernoulli Naïve Bayes is appropriate when your features are **binary**, such as in the case of document classification, where each feature could represent the presence (1) or absence (0) of a specific word in a document.\n",
        "\n",
        "#### Key Points:\n",
        "- Assumes features are **binary** (0 or 1), representing the presence or absence of a particular attribute.\n",
        "- Suitable for **binary classification tasks** or text classification problems where the features represent binary events (such as the presence or absence of a word).\n",
        "- Works well with **binary features**.\n",
        "\n",
        "---\n",
        "\n",
        "### Summary of Differences:\n",
        "\n",
        "| **Characteristic**            | **Gaussian Naïve Bayes**                 | **Multinomial Naïve Bayes**                 | **Bernoulli Naïve Bayes**                     |\n",
        "|-------------------------------|-----------------------------------------|-------------------------------------------|---------------------------------------------|\n",
        "| **Data Type**                 | Continuous (real-valued features)       | Discrete counts (e.g., word frequencies)   | Binary (0 or 1, presence or absence)        |\n",
        "| **Distribution Assumption**   | Features are **normally distributed**   | Features follow a **multinomial distribution** | Features follow a **Bernoulli distribution** |\n",
        "| **Likelihood Estimation**     | Gaussian (Normal) distribution          | Multinomial distribution of counts        | Bernoulli distribution (binary outcomes)    |\n",
        "| **Common Use Case**           | Continuous data (e.g., predicting diseases based on medical measurements) | Text classification with counts (e.g., spam detection, sentiment analysis) | Text classification with binary features (e.g., word presence/absence) |\n",
        "| **Examples**                  | Medical data, stock prices, age prediction | Document classification, text categorization | Document classification, spam detection    |\n",
        "\n",
        "### When to Choose Each:\n",
        "- **Gaussian Naïve Bayes**: Choose this when your features are continuous and you can assume they follow a normal distribution.\n",
        "- **Multinomial Naïve Bayes**: This is a good choice when dealing with **discrete count data**, particularly in text classification tasks where the features represent word counts or term frequencies.\n",
        "- **Bernoulli Naïve Bayes**: Best used when your features are binary, i.e., the presence or absence of certain characteristics (e.g., presence of specific words in a document).\n",
        "\n",
        "Each of these variants of Naïve Bayes has its strengths, and the choice of which one to use depends on the nature of your data and the problem you're trying to solve."
      ],
      "metadata": {
        "id": "eDGR0KwdR9pX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. When should you use Gaussian Naive Bayes over other variants?\n"
      ],
      "metadata": {
        "id": "lWQXdjyBR9md"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gaussian Naïve Bayes** (GNB) is a specific variant of the Naïve Bayes classifier that assumes that the features follow a **Gaussian (Normal) distribution** within each class. It is most effective in scenarios where the data has certain characteristics that make this assumption reasonable. Here's when you should consider using **Gaussian Naïve Bayes** over other variants like **Multinomial Naïve Bayes** or **Bernoulli Naïve Bayes**:\n",
        "\n",
        "### When to Use Gaussian Naïve Bayes:\n",
        "1. **Continuous Features**:\n",
        "   - **Gaussian Naïve Bayes** works best when your features are **continuous** (i.e., real-valued data) such as age, height, weight, temperature, etc.\n",
        "   - If your dataset has continuous attributes, and you can reasonably assume that the features for each class are distributed normally (i.e., follow a bell curve), **Gaussian Naïve Bayes** is a good fit.\n",
        "\n",
        "2. **Assumption of Normality Holds**:\n",
        "   - If you can reasonably assume that the features for each class are normally distributed, **Gaussian Naïve Bayes** will perform well. For example, if you're working with data where attributes like age or income are expected to follow a normal distribution, this model would be appropriate.\n",
        "   - For instance, in problems like **medical diagnostics** where you expect that features like blood pressure or cholesterol levels are normally distributed across healthy and unhealthy individuals, **Gaussian Naïve Bayes** would be suitable.\n",
        "\n",
        "3. **Simple and Fast Baseline Model**:\n",
        "   - If you need a **simple, fast baseline model** to quickly evaluate performance on a classification task, **Gaussian Naïve Bayes** can be a great starting point. It requires only the computation of mean and variance for each feature within each class, which makes it computationally efficient and easy to implement.\n",
        "\n",
        "4. **Class-Conditional Independence Assumption**:\n",
        "   - Like all Naïve Bayes classifiers, **Gaussian Naïve Bayes** assumes that the features are **independent** given the class label. If this assumption holds reasonably well in your dataset, then **Gaussian Naïve Bayes** is a good choice, as it simplifies the classification process and often performs well despite the strong assumption of independence.\n",
        "\n",
        "5. **Small Data or Limited Training Samples**:\n",
        "   - **Gaussian Naïve Bayes** is often effective in situations with small datasets or when there are relatively few training samples. Since it makes strong assumptions about the data distribution, it can still perform well even with limited data, provided that the normality assumption holds.\n",
        "\n",
        "---\n",
        "\n",
        "### When **Not** to Use Gaussian Naïve Bayes:\n",
        "- **Discrete or Binary Data**: If your features are **discrete counts** or **binary data** (like text data where you count the number of occurrences of specific words), other variants such as **Multinomial Naïve Bayes** or **Bernoulli Naïve Bayes** would be more appropriate. These models are specifically designed for discrete feature types (word counts, binary features, etc.).\n",
        "- **Data with Skewed Distributions**: If the features do not follow a normal distribution and are highly skewed (e.g., income data where the distribution is right-skewed), then the Gaussian assumption may not hold well, and the performance of **Gaussian Naïve Bayes** could suffer.\n",
        "- **Outliers**: If the data contains **outliers**, they could have a disproportionately large effect on the mean and variance, which would negatively affect the performance of **Gaussian Naïve Bayes**. Other methods (e.g., tree-based methods) may handle outliers better.\n",
        "\n",
        "---\n",
        "\n",
        "### Summary of When to Use Gaussian Naïve Bayes:\n",
        "- When your data consists of **continuous features**.\n",
        "- When the features are assumed to follow a **normal (Gaussian) distribution** within each class.\n",
        "- When you need a **simple and fast baseline model** for classification.\n",
        "- When you can reasonably assume **class-conditional independence** (features are independent given the class).\n",
        "- When you have **small datasets** or limited training samples.\n",
        "\n",
        "In contrast, if your data consists of **discrete counts or binary features**, or if your features don't follow a normal distribution, you should explore **Multinomial Naïve Bayes** or **Bernoulli Naïve Bayes**, as they are more suitable for those types of data."
      ],
      "metadata": {
        "id": "7kkMcxqPR9jw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What are the key assumptions made by Naïve Bayes?\n"
      ],
      "metadata": {
        "id": "JaR0Hr7kR9EE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **Naïve Bayes** classifier is a simple and effective probabilistic classifier based on **Bayes' Theorem**, and it makes a few key assumptions. These assumptions are essential to how the model works and differentiate it from other machine learning algorithms. Here are the key assumptions made by Naïve Bayes:\n",
        "\n",
        "### 1. **Conditional Independence Assumption**:\n",
        "   - **Naïve Bayes assumes that the features (or attributes) are conditionally independent** given the class label. This means that for each data point, given the class label, the features do not influence each other.\n",
        "   - Mathematically, for a set of features \\( X = (x_1, x_2, \\dots, x_n) \\) and class label \\( C \\), the model assumes:\n",
        "     \\[\n",
        "     P(x_1, x_2, \\dots, x_n | C) = \\prod_{i=1}^{n} P(x_i | C)\n",
        "     \\]\n",
        "     This means that the likelihood of the features given the class is the product of the likelihoods of each individual feature given the class.\n",
        "   - **Key Point**: This assumption is \"naïve\" because, in reality, features often have some degree of correlation with each other. However, despite this strong assumption, Naïve Bayes often performs surprisingly well in practice.\n",
        "\n",
        "### 2. **Class Conditional Independence**:\n",
        "   - Each feature \\( x_i \\) is assumed to be independent of the other features, conditioned on the class \\( C \\). This simplifies the calculation of the likelihood for each class since the joint likelihood of the features can be broken down into a product of individual feature likelihoods.\n",
        "   - This is often a **simplifying assumption** that makes the model computationally efficient but can lead to errors when features are highly correlated.\n",
        "\n",
        "### 3. **Feature Distribution Assumptions** (Variant Dependent):\n",
        "   - Depending on the specific variant of Naïve Bayes (Gaussian, Multinomial, or Bernoulli), the model assumes different types of distributions for the features:\n",
        "     - **Gaussian Naïve Bayes**: Assumes that the features are **normally distributed** (i.e., follow a Gaussian distribution) for each class. It models each feature \\( x_i \\) within a class as a Gaussian random variable.\n",
        "     - **Multinomial Naïve Bayes**: Assumes that the features represent **discrete counts** (e.g., word counts in text classification), and that these counts follow a **multinomial distribution**. It is commonly used for text classification problems.\n",
        "     - **Bernoulli Naïve Bayes**: Assumes that the features are **binary** (e.g., the presence or absence of a word in a document), and each feature follows a **Bernoulli distribution**.\n",
        "\n",
        "### 4. **Independence of Data Points**:\n",
        "   - **Naïve Bayes** assumes that the **data points (instances)** are independent of each other. This means that the presence of one data point in the dataset does not affect the presence of another.\n",
        "   - This assumption may not hold in certain scenarios, such as when data points are temporally or spatially correlated (e.g., time series data or spatial data).\n",
        "\n",
        "### 5. **Fixed Class Probabilities**:\n",
        "   - Naïve Bayes assumes that the **class probabilities** (i.e., the prior probabilities of each class) are fixed and known. These probabilities are typically estimated from the training data by calculating the relative frequency of each class.\n",
        "   - The class priors are used to weight the likelihoods in Bayes' Theorem:\n",
        "     \\[\n",
        "     P(C | X) = \\frac{P(X | C) P(C)}{P(X)}\n",
        "     \\]\n",
        "   - Since \\( P(X) \\) is constant for all classes, it is often ignored in the classification process, and the model focuses on maximizing the product of \\( P(X | C) \\) and \\( P(C) \\).\n",
        "\n",
        "### 6. **Simplicity in Handling Missing Data**:\n",
        "   - Naïve Bayes can handle **missing data** in some cases by simply **ignoring the missing feature values** when calculating the likelihood for that data point. This is possible because of the independence assumption, where each feature is considered separately.\n",
        "   - However, this does not mean Naïve Bayes is optimal for handling missing data — it just makes it more robust compared to some other models, which may fail when data is missing.\n",
        "\n",
        "---\n",
        "\n",
        "### Summary of Key Assumptions:\n",
        "1. **Conditional Independence**: Features are assumed to be independent given the class label.\n",
        "2. **Class Conditional Independence**: Each feature is conditionally independent of the others, given the class.\n",
        "3. **Feature Distributions**:\n",
        "   - Gaussian Naïve Bayes assumes normal (Gaussian) distribution for features.\n",
        "   - Multinomial Naïve Bayes assumes features follow a multinomial distribution (typically for count data).\n",
        "   - Bernoulli Naïve Bayes assumes binary features, following a Bernoulli distribution.\n",
        "4. **Independence of Data Points**: Each data point is assumed to be independent of others.\n",
        "5. **Fixed Class Probabilities**: The prior probabilities of each class are fixed and estimated from the training data.\n",
        "6. **Handling Missing Data**: Naïve Bayes can handle missing data by ignoring the missing features in the calculation.\n",
        "\n",
        "These assumptions make **Naïve Bayes** simple and computationally efficient, but they also mean that the model may not always be accurate in cases where the assumptions do not hold. Despite these strong assumptions, Naïve Bayes often performs surprisingly well, especially for text classification and other tasks with relatively independent features."
      ],
      "metadata": {
        "id": "n2rKwQ6hW4Fs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. What are the advantages and disadvantages of Naive Bayes?\n"
      ],
      "metadata": {
        "id": "sr7u1nRmW3rN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Advantages of Naïve Bayes:**\n",
        "\n",
        "1. **Simplicity and Speed**:\n",
        "   - **Naïve Bayes** is a very simple and easy-to-understand algorithm. It has a straightforward mathematical foundation based on **Bayes' Theorem**.\n",
        "   - **Training** and **predicting** with Naïve Bayes is computationally very efficient, making it **fast** even with large datasets. The model scales well with data.\n",
        "   - Since Naïve Bayes assumes conditional independence between features, the model can be trained quickly as it does not have to consider interactions between the features.\n",
        "\n",
        "2. **Works Well with High Dimensionality**:\n",
        "   - **Naïve Bayes** performs well when the number of features is large, such as in **text classification** problems (e.g., spam detection, sentiment analysis) where each word in the document can be treated as a feature.\n",
        "   - This is especially useful in natural language processing (NLP) tasks where the number of features (words) can be enormous.\n",
        "\n",
        "3. **Handles Missing Data**:\n",
        "   - **Naïve Bayes** can handle **missing data** fairly well. In case of missing features, the model simply ignores the feature during classification.\n",
        "   - The conditional independence assumption allows for partial data to still be classified correctly, without needing imputation or complex handling for missing values.\n",
        "\n",
        "4. **Good Performance with Small Datasets**:\n",
        "   - **Naïve Bayes** is robust even when there is limited training data. It can give **reasonably good results** even on small datasets because it assumes strong independence between features, which simplifies the learning process.\n",
        "\n",
        "5. **Probabilistic Output**:\n",
        "   - Naïve Bayes provides a **probabilistic output**, which can be useful in scenarios where you want not only the class prediction but also the confidence of that prediction (i.e., the probability of belonging to each class).\n",
        "   \n",
        "6. **Works Well with Multiclass Classification**:\n",
        "   - Naïve Bayes works well for **multiclass classification** tasks, as it can easily handle more than two classes without any extra modifications. The same model can be applied to datasets with multiple class labels (such as categorizing news articles into different categories).\n",
        "\n",
        "---\n",
        "\n",
        "### **Disadvantages of Naïve Bayes:**\n",
        "\n",
        "1. **Strong Independence Assumption**:\n",
        "   - The biggest limitation of **Naïve Bayes** is its **conditional independence assumption**. In reality, features are often correlated, which leads to performance degradation when this assumption does not hold. For example, in real-world datasets, the assumption that features like age and income are independent may not be valid.\n",
        "   - This **\"naïve\" assumption** of independence can lead to lower performance compared to more complex models like decision trees or support vector machines when the features are highly dependent.\n",
        "\n",
        "2. **Poor Performance with Highly Correlated Features**:\n",
        "   - If the features are highly correlated, Naïve Bayes can perform poorly because it assumes that each feature contributes independently to the classification. This is problematic when the features have dependencies.\n",
        "   - For example, in image classification tasks, pixel values (features) are often correlated, and Naïve Bayes may not capture these dependencies well.\n",
        "\n",
        "3. **Limited to Simple Probability Models**:\n",
        "   - **Naïve Bayes** assumes a simple probability distribution for the features, which may not always be accurate for complex data. For instance, using **Gaussian Naïve Bayes** assumes that the features are normally distributed, which might not be the case for many real-world datasets.\n",
        "   - If the feature distributions deviate significantly from the assumptions (e.g., if the data is multimodal, skewed, or has outliers), **Naïve Bayes** might perform poorly compared to other classifiers.\n",
        "\n",
        "4. **Sensitivity to Imbalanced Data**:\n",
        "   - **Naïve Bayes** can be sensitive to **class imbalance** in the dataset. If one class is significantly underrepresented, the model may be biased toward predicting the majority class. This can result in poor classification performance for the minority class.\n",
        "   - In cases of imbalanced datasets, Naïve Bayes can overestimate the probability of the majority class, leading to inaccurate predictions.\n",
        "\n",
        "5. **Difficulty in Estimating Conditional Probabilities**:\n",
        "   - In cases where a feature has a **zero frequency** (i.e., a particular feature value doesn't occur in the training data for a given class), Naïve Bayes can struggle. This problem is known as the **zero-frequency problem**.\n",
        "   - This issue is often handled using **smoothing techniques**, like **Laplace Smoothing**, but this still represents a potential issue when dealing with unseen feature combinations.\n",
        "\n",
        "6. **Not Suitable for Regression**:\n",
        "   - Naïve Bayes is inherently a **classification algorithm** and does not handle regression tasks well. If you need a model for continuous outcomes (e.g., predicting house prices), Naïve Bayes is not a suitable choice.\n",
        "\n",
        "7. **Over-simplification of Complex Data**:\n",
        "   - The **strong assumptions** made by Naïve Bayes (independence between features and specific feature distributions) often make the model overly simplistic for complex data patterns.\n",
        "   - In cases where interactions between features are important, more sophisticated models (like **Random Forests**, **Support Vector Machines**, or **Neural Networks**) may perform better.\n",
        "\n",
        "---\n",
        "\n",
        "### **Summary of Advantages and Disadvantages of Naïve Bayes:**\n",
        "\n",
        "| **Advantages**                         | **Disadvantages**                                |\n",
        "|----------------------------------------|-------------------------------------------------|\n",
        "| Simple and easy to understand          | Assumes strong independence between features     |\n",
        "| Fast and computationally efficient     | Poor performance with highly correlated features |\n",
        "| Works well with high-dimensional data  | Assumes feature distributions are simple (e.g., Gaussian) |\n",
        "| Good for small datasets                | Can struggle with imbalanced datasets            |\n",
        "| Handles missing data                   | Zero-frequency problem (needs smoothing)         |\n",
        "| Probabilistic output                   | Not suitable for regression tasks                |\n",
        "| Effective for multiclass classification | Over-simplification of complex data patterns     |\n",
        "\n",
        "---\n",
        "\n",
        "### **When to Use Naïve Bayes**:\n",
        "- **Naïve Bayes** is a great choice when:\n",
        "  - Your features are mostly **independent**.\n",
        "  - You are working with **text classification** or **spam filtering** problems where feature independence often holds well (e.g., words in a document).\n",
        "  - You need a **simple and fast model** that gives probabilistic outputs.\n",
        "  - You have a **small or medium-sized dataset**.\n",
        "  - You want a **baseline model** to quickly test and evaluate.\n",
        "\n",
        "### **When Not to Use Naïve Bayes**:\n",
        "- If your features are highly **correlated** or interact in complex ways.\n",
        "- If your data does not follow the **assumed distributions** (e.g., normal distribution for Gaussian Naïve Bayes).\n",
        "- If your dataset has **imbalanced classes** and you are concerned about bias.\n"
      ],
      "metadata": {
        "id": "wEqumuPiWaz5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. Why is Naive Bayes a good choice for text classification?\n"
      ],
      "metadata": {
        "id": "_jz69VZ4WbgZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Naïve Bayes is a popular and effective choice for **text classification** due to several key reasons related to the structure of text data and the assumptions of the model. Here's why Naïve Bayes works well in text classification tasks:\n",
        "\n",
        "### 1. **Conditional Independence Assumption Works Well in Text**:\n",
        "   - **Naïve Bayes** assumes that features (i.e., words in a document) are **conditionally independent** given the class label. In text classification, this assumption often works surprisingly well, even though it's not strictly true.\n",
        "   - Words in a document are often treated as independent from each other, which simplifies the classification process. Even though words can be correlated in natural language (e.g., \"machine\" and \"learning\"), the model still performs effectively by assuming these correlations don't exist.\n",
        "   - For example, in **spam email detection**, the presence of one word (e.g., \"free\") doesn't depend on the presence of another (e.g., \"offer\"), even though in practice, certain word pairs may be correlated. **Naïve Bayes** handles this assumption well in practice, producing good classification results.\n",
        "\n",
        "### 2. **Effective for High-Dimensional Data (Sparse Features)**:\n",
        "   - **Text data** is typically **high-dimensional** because each unique word in a document becomes a feature (word), and most documents have only a small subset of all possible words. This creates a **sparse** feature space (most features have zero values for a given document).\n",
        "   - **Naïve Bayes** is computationally efficient with high-dimensional data because it focuses on individual feature probabilities (word counts or presence), rather than considering complex interactions between features.\n",
        "   - For example, in **document classification** or **spam filtering**, even if you have a vocabulary of thousands of words, Naïve Bayes will only compute the probabilities for the words that appear in the document, making it scalable.\n",
        "\n",
        "### 3. **Works Well with Discrete (Count) Data**:\n",
        "   - Text data is often represented as **discrete count data** (e.g., word frequency or the presence/absence of words). **Multinomial Naïve Bayes** is particularly well-suited for text classification because it models the probability of a word occurring in a document given the class (i.e., how likely it is that a document is spam given the presence of certain words).\n",
        "   - It uses the **multinomial distribution**, which is ideal for counting the occurrences of discrete features, like word counts.\n",
        "\n",
        "### 4. **Simplicity and Efficiency**:\n",
        "   - **Naïve Bayes** is a **simple** and **fast** algorithm that works well as a **baseline model** for text classification tasks.\n",
        "   - Training a Naïve Bayes model on text data involves calculating simple **probabilities** (the likelihood of each word given a class and the prior probability of each class), which can be done very efficiently, even for large datasets.\n",
        "   - Text classification problems often involve a lot of data (e.g., thousands of documents), and Naïve Bayes is able to scale effectively, making it a good choice when you need a **fast and scalable model**.\n",
        "\n",
        "### 5. **Handles Missing Data (Words Not Present in Documents)**:\n",
        "   - Naïve Bayes can handle **missing features** or **zero-frequency problems** (i.e., when a word does not appear in the training data for a given class) by using **Laplace smoothing** (or add-one smoothing). This smoothing technique ensures that even words not seen during training can still contribute a small probability, helping avoid zero-probability issues.\n",
        "   - This is particularly useful in text classification, where new words may appear in test documents that were not present in the training set.\n",
        "\n",
        "### 6. **Probabilistic Output**:\n",
        "   - Naïve Bayes gives **probabilistic predictions**, which can be useful for text classification tasks where you may want to not just classify the document but also measure the **confidence** of the prediction.\n",
        "   - For example, in **sentiment analysis**, knowing the probability of a document being \"positive\" or \"negative\" can be more useful than a binary decision. Naïve Bayes provides these probabilities directly.\n",
        "\n",
        "### 7. **Multiclass Classification**:\n",
        "   - Naïve Bayes is well-suited for **multiclass classification** tasks, which is common in text classification (e.g., categorizing news articles into multiple topics such as politics, sports, or entertainment).\n",
        "   - Naïve Bayes can easily extend to handle more than two classes by calculating the likelihood of each class independently and selecting the class with the highest probability.\n",
        "\n",
        "### 8. **Interpretability**:\n",
        "   - Naïve Bayes models are easy to interpret. The feature (word) probabilities can provide insights into which words are most indicative of a particular class.\n",
        "   - In text classification tasks like **spam detection**, it's easy to see which words (e.g., \"free\", \"offer\", \"win\") are highly predictive of the \"spam\" class.\n",
        "\n",
        "---\n",
        "\n",
        "### Example of Text Classification using Naïve Bayes:\n",
        "\n",
        "Consider a **spam email classification** problem. In this case, Naïve Bayes uses words like \"free\", \"offer\", \"winner\", etc., as features. Even if the words are **correlated** (i.e., certain words often appear together in spam emails), Naïve Bayes can still classify the email effectively by assuming the features are independent.\n",
        "\n",
        "- **Step 1**: Naïve Bayes calculates the probability of each word occurring in both spam and non-spam emails (the likelihood).\n",
        "- **Step 2**: It calculates the prior probability of an email being spam or not (the prior class probability).\n",
        "- **Step 3**: For a new email, Naïve Bayes computes the likelihood of the email being spam or not spam based on the observed words in the email, using the conditional probabilities learned during training.\n",
        "\n",
        "---\n",
        "\n",
        "### Summary: Why Naïve Bayes is Good for Text Classification:\n",
        "1. **Conditional Independence**: Works well with the assumption that words in a document are conditionally independent, which simplifies the computation and is reasonable for text data.\n",
        "2. **High Dimensionality**: Efficiently handles high-dimensional data, which is typical in text classification where the vocabulary can be large.\n",
        "3. **Discrete Data**: Well-suited for discrete count data (like word counts) commonly used in text classification tasks.\n",
        "4. **Simplicity and Efficiency**: It's a simple, fast, and computationally efficient algorithm that scales well with large datasets.\n",
        "5. **Multiclass Capability**: Easily handles multiclass problems, which are common in text classification (e.g., topic categorization).\n",
        "6. **Smoothing for Missing Data**: Handles unseen words and missing features using smoothing techniques like Laplace smoothing.\n",
        "\n",
        "Because of these factors, **Naïve Bayes** is often one of the go-to algorithms for text classification tasks, especially when you need a fast, efficient, and interpretable model."
      ],
      "metadata": {
        "id": "pLz2JDOzXTDO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. Compare SVM and Naïve Bayes for classification tasks."
      ],
      "metadata": {
        "id": "9MflKztQXS-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sure! Here's a comparison of **Support Vector Machines (SVM)** and **Naïve Bayes** for classification tasks across several key dimensions:\n",
        "\n",
        "---\n",
        "\n",
        "### 🔍 1. **Algorithmic Approach**\n",
        "\n",
        "- **SVM (Support Vector Machine):**\n",
        "  - A **discriminative classifier** that finds the optimal hyperplane that best separates classes in the feature space.\n",
        "  - Focuses on **maximizing the margin** between data points of different classes.\n",
        "\n",
        "- **Naïve Bayes:**\n",
        "  - A **generative classifier** based on **Bayes' Theorem** with the **naïve assumption** that features are independent.\n",
        "  - Calculates the **probability of each class** given the input and chooses the one with the highest probability.\n",
        "\n",
        "---\n",
        "\n",
        "### ⚙️ 2. **Assumptions**\n",
        "\n",
        "- **SVM:**\n",
        "  - Makes **no assumption** about the data distribution.\n",
        "  - Can work well even when features are **correlated**.\n",
        "\n",
        "- **Naïve Bayes:**\n",
        "  - Assumes that all features are **independent** given the class label (which is often not true in practice).\n",
        "\n",
        "---\n",
        "\n",
        "### 📈 3. **Performance**\n",
        "\n",
        "- **SVM:**\n",
        "  - Performs well in **high-dimensional** spaces and for **complex decision boundaries**.\n",
        "  - Generally **better accuracy** than Naïve Bayes for many real-world tasks.\n",
        "\n",
        "- **Naïve Bayes:**\n",
        "  - Can outperform SVM on **small datasets** or when the **independence assumption holds** approximately.\n",
        "  - Very effective for **text classification**, like spam detection or sentiment analysis.\n",
        "\n",
        "---\n",
        "\n",
        "### ⚡ 4. **Speed & Complexity**\n",
        "\n",
        "- **SVM:**\n",
        "  - Training can be **computationally expensive**, especially on large datasets.\n",
        "  - Slower than Naïve Bayes during training.\n",
        "\n",
        "- **Naïve Bayes:**\n",
        "  - **Very fast** to train and predict.\n",
        "  - Efficient and scalable with large datasets.\n",
        "\n",
        "---\n",
        "\n",
        "### 🛠️ 5. **Interpretability**\n",
        "\n",
        "- **SVM:**\n",
        "  - More difficult to interpret — it's a **black box** model.\n",
        "  \n",
        "- **Naïve Bayes:**\n",
        "  - Easy to interpret — provides **probabilistic outputs** and feature importance is clear.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 6. **Use Cases**\n",
        "\n",
        "- **SVM:**\n",
        "  - Image classification, bioinformatics, handwriting recognition.\n",
        "  \n",
        "- **Naïve Bayes:**\n",
        "  - Text classification, spam filtering, recommendation systems.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ Summary Table\n",
        "\n",
        "| Aspect               | SVM                                | Naïve Bayes                         |\n",
        "|----------------------|-------------------------------------|--------------------------------------|\n",
        "| Type                 | Discriminative                     | Generative                          |\n",
        "| Assumptions          | No strong assumptions              | Feature independence                |\n",
        "| Speed                | Slower training                    | Very fast                           |\n",
        "| Accuracy             | Often higher                       | Good with small data or text        |\n",
        "| Interpretability     | Low                                | High                                |\n",
        "| Best for             | High-dimensional, non-linear data  | Text classification, simple problems|\n",
        "\n",
        "---\n",
        "\n",
        "Let me know if you’d like this as a study card, chart, or visual summary!"
      ],
      "metadata": {
        "id": "Vs_PeqVvXS2E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. How does Laplace Smoothing help in Naive Bayes?\n"
      ],
      "metadata": {
        "id": "wMRDFt0YXSyO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great question!\n",
        "\n",
        "### 🧂 **Laplace Smoothing in Naïve Bayes** (also called **Add-One Smoothing**)\n",
        "\n",
        "---\n",
        "\n",
        "### 🔍 **The Problem:**\n",
        "In Naïve Bayes, we calculate the **probability of a feature given a class**:  \n",
        "\\[\n",
        "P(\\text{word}|\\text{class}) = \\frac{\\text{count(word in class)}}{\\text{total words in class}}\n",
        "\\]\n",
        "\n",
        "But what if a word **never appeared** in the training data for a certain class?\n",
        "\n",
        "🧨 That makes `P(word|class) = 0`, which **kills the entire probability** for that class because probabilities are multiplied together.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ **The Solution: Laplace Smoothing**\n",
        "\n",
        "To fix this, we **add 1** to every word count:\n",
        "\n",
        "\\[\n",
        "P(\\text{word}|\\text{class}) = \\frac{\\text{count(word in class)} + 1}{\\text{total words in class} + V}\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- `V` = total number of unique words in the vocabulary.\n",
        "\n",
        "This ensures:\n",
        "- **No probability is ever zero**\n",
        "- Prevents the model from **overreacting** to unseen words\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 **Why It Works:**\n",
        "- Introduces a small, equal probability for unseen words.\n",
        "- Improves model **generalization**, especially for small or sparse datasets.\n",
        "\n",
        "---\n",
        "\n",
        "### 📌 **In Summary:**\n",
        "**Laplace Smoothing helps Naïve Bayes** by:\n",
        "- Avoiding zero probabilities\n",
        "- Handling **unseen features/words**\n",
        "- Making the classifier more robust and stable\n",
        "\n",
        "Let me know if you want a simple code example or analogy to go along with this!"
      ],
      "metadata": {
        "id": "Z1d5Q71pXSt8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. Write a Python program to train an SVM Classifier on the Iris dataset and evaluate accuracy.\n"
      ],
      "metadata": {
        "id": "xeesjVmkXSkH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data    # Features\n",
        "y = iris.target  # Labels\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create the SVM classifier (you can try different kernels: 'linear', 'poly', 'rbf', 'sigmoid')\n",
        "svm_model = SVC(kernel='linear')  # linear kernel\n",
        "\n",
        "# Train the model\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of SVM classifier on Iris dataset: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0M_fgTt6cdHO",
        "outputId": "9a49852e-5c17-43d9-b859-701347ac8a6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of SVM classifier on Iris dataset: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies.\n"
      ],
      "metadata": {
        "id": "l9eVlyQHXSVA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = datasets.load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split into train and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train SVM with Linear kernel\n",
        "svm_linear = SVC(kernel='linear')\n",
        "svm_linear.fit(X_train, y_train)\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
        "\n",
        "# Train SVM with RBF kernel\n",
        "svm_rbf = SVC(kernel='rbf')\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "# Compare accuracies\n",
        "print(f\"Accuracy with Linear Kernel: {accuracy_linear:.2f}\")\n",
        "print(f\"Accuracy with RBF Kernel:    {accuracy_rbf:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipgPXIPUcosa",
        "outputId": "21b46bf3-5ceb-4cf3-ba3b-4dcc6011c75b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with Linear Kernel: 1.00\n",
            "Accuracy with RBF Kernel:    0.81\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. Write a Python program to train an SVM Regressor (SVR) on a housing dataset and evaluate it using Mean Squared Error (MSE).\n"
      ],
      "metadata": {
        "id": "uS019FnBXR2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature scaling is important for SVR\n",
        "scaler_X = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "\n",
        "X_train_scaled = scaler_X.fit_transform(X_train)\n",
        "X_test_scaled = scaler_X.transform(X_test)\n",
        "\n",
        "y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).ravel()  # Flatten after scaling\n",
        "y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1)).ravel()\n",
        "\n",
        "# Create and train the SVR model\n",
        "svr = SVR(kernel='rbf')  # You can try 'linear' or 'poly' as well\n",
        "svr.fit(X_train_scaled, y_train_scaled)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_scaled = svr.predict(X_test_scaled)\n",
        "\n",
        "# Inverse transform the scaled predictions to get values in original scale\n",
        "y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).ravel()\n",
        "\n",
        "# Evaluate using Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error of SVR on California Housing dataset: {mse:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Mb1uZpWc2jE",
        "outputId": "818f7358-df68-4605-9fbf-64eb6433348a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error of SVR on California Housing dataset: 0.35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. Write a Python program to train an SVM Classifier with a Polynomial Kernel and visualize the decision boundary.\n"
      ],
      "metadata": {
        "id": "wfHv_bRCc3Fm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Generate synthetic 2D classification data\n",
        "X, y = make_classification(n_samples=200, n_features=2, n_informative=2,\n",
        "                           n_redundant=0, n_clusters_per_class=1, random_state=42)\n",
        "\n",
        "# Split into train and test sets (not strictly necessary for visualization)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train SVM with polynomial kernel\n",
        "svm_poly = SVC(kernel='poly', degree=3, C=1.0)\n",
        "svm_poly.fit(X_train, y_train)\n",
        "\n",
        "# Create a mesh to plot the decision boundary\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 500),\n",
        "                     np.linspace(y_min, y_max, 500))\n",
        "\n",
        "# Predict over the mesh grid\n",
        "Z = svm_poly.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "# Plot the decision boundary\n",
        "plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.coolwarm)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, edgecolors='k')\n",
        "plt.title(\"SVM with Polynomial Kernel (degree=3)\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "kHauzrgnc3jh",
        "outputId": "7f783ac6-fb9c-47da-f311-68592faa268d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Write a Python program to train a Gaussian Naive Bayes classifier on the Breast Cancer dataset and evaluate accuracy,\n"
      ],
      "metadata": {
        "id": "-pqNeVEedQUx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Gaussian Naive Bayes model\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of Gaussian Naive Bayes on Breast Cancer dataset: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euTrKMXKdQro",
        "outputId": "f76b8875-f3bb-4846-fece-b317456e4302"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Gaussian Naive Bayes on Breast Cancer dataset: 0.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "26. Write a Python program to train a Multinomial Naïve Bayes classifier for text classification using the 20 Newsgroups dataset."
      ],
      "metadata": {
        "id": "mSCLpbi0dTiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the 20 Newsgroups dataset (you can also select specific categories)\n",
        "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "# Extract features and labels\n",
        "X = newsgroups.data\n",
        "y = newsgroups.target\n",
        "\n",
        "# Convert text documents to TF-IDF feature matrix\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
        "X_tfidf = vectorizer.fit_transform(X)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Multinomial Naive Bayes classifier\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of Multinomial Naive Bayes on 20 Newsgroups dataset: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJm-bIdJdTD1",
        "outputId": "6f8eb059-d68e-4b48-a765-1c8bb394b443"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Multinomial Naive Bayes on 20 Newsgroups dataset: 0.72\n"
          ]
        }
      ]
    }
  ]
}