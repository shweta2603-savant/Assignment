{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Logistic Regression, and how does it differ from Linear Regression?\n"
      ],
      "metadata": {
        "id": "MXUPGPjVNIup"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKYgS1HYNCyi"
      },
      "outputs": [],
      "source": [
        "### What is **Logistic Regression**?\n",
        "\n",
        "**Logistic Regression** is a statistical model used for **binary classification** tasks, where the goal is to predict one of two possible outcomes (e.g., \"yes\" vs. \"no\", \"spam\" vs. \"not spam\", etc.). Despite its name, logistic regression is a **classification** algorithm, not a regression algorithm.\n",
        "\n",
        "Logistic Regression works by applying the **logistic function** (also known as the **sigmoid function**) to the output of a linear model. This function maps any real-valued number to a value between 0 and 1, which represents the probability of a certain class.\n",
        "\n",
        "#### The logistic (sigmoid) function is defined as:\n",
        "\\[\n",
        "\\text{Sigmoid}(z) = \\frac{1}{1 + e^{-z}}\n",
        "\\]\n",
        "Where:\n",
        "- \\( z = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n \\) is the linear combination of the input features (just like in linear regression).\n",
        "\n",
        "The output of the logistic function is interpreted as the **probability** that a given input belongs to a particular class. If the output is greater than 0.5, the class is typically predicted as 1 (positive class); otherwise, it is predicted as 0 (negative class).\n",
        "\n",
        "---\n",
        "\n",
        "### How is **Logistic Regression** Different from **Linear Regression**?\n",
        "\n",
        "#### 1. **Purpose**:\n",
        "   - **Linear Regression**: Used for **predicting continuous values**. For example, predicting house prices, stock prices, etc.\n",
        "   - **Logistic Regression**: Used for **binary classification** (predicting categorical outcomes). For example, predicting whether an email is spam or not, whether a customer will churn, etc.\n",
        "\n",
        "#### 2. **Output**:\n",
        "   - **Linear Regression**: The output is a continuous numeric value, which can be any real number (positive or negative).\n",
        "     \\[\n",
        "     y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n\n",
        "     \\]\n",
        "   - **Logistic Regression**: The output is a **probability** between 0 and 1 (after applying the sigmoid function). This probability is then used to assign a class label (0 or 1) based on a threshold (usually 0.5).\n",
        "     \\[\n",
        "     p(y=1|X) = \\frac{1}{1 + e^{-z}}\n",
        "     \\]\n",
        "     Where \\( z \\) is the linear combination of the features.\n",
        "\n",
        "#### 3. **Modeling Approach**:\n",
        "   - **Linear Regression**: The goal is to **minimize the mean squared error (MSE)** between the predicted and actual values.\n",
        "   - **Logistic Regression**: The goal is to **maximize the likelihood** of the model parameters (using the **log-likelihood function**). This is typically done using **maximum likelihood estimation (MLE)**.\n",
        "\n",
        "#### 4. **Equation**:\n",
        "   - **Linear Regression**: The prediction is a linear combination of input features.\n",
        "     \\[\n",
        "     y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n\n",
        "     \\]\n",
        "   - **Logistic Regression**: The prediction is a transformed value from a linear combination of input features, passed through the sigmoid function.\n",
        "     \\[\n",
        "     p(y=1|X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n)}}\n",
        "     \\]\n",
        "\n",
        "#### 5. **Loss Function**:\n",
        "   - **Linear Regression**: Uses **Mean Squared Error (MSE)**.\n",
        "     \\[\n",
        "     \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
        "     \\]\n",
        "   - **Logistic Regression**: Uses **Log Loss (Cross-Entropy Loss)**.\n",
        "     \\[\n",
        "     \\text{Log Loss} = - \\frac{1}{n} \\sum_{i=1}^{n} \\left[y_i \\log(p) + (1 - y_i) \\log(1 - p)\\right]\n",
        "     \\]\n",
        "     Where \\( p \\) is the predicted probability.\n",
        "\n",
        "#### 6. **Assumptions**:\n",
        "   - **Linear Regression**: Assumes that the relationship between the independent and dependent variables is linear.\n",
        "   - **Logistic Regression**: Assumes that the relationship between the independent variables and the log-odds of the dependent variable is linear.\n",
        "\n",
        "---\n",
        "\n",
        "### Summary Table:\n",
        "\n",
        "| **Aspect**                 | **Linear Regression**                                 | **Logistic Regression**                                  |\n",
        "|----------------------------|-------------------------------------------------------|----------------------------------------------------------|\n",
        "| **Task**                   | Predict continuous values (regression)                | Predict categorical values (binary classification)        |\n",
        "| **Output**                 | Continuous numeric value                              | Probability between 0 and 1                              |\n",
        "| **Activation Function**    | None (direct linear combination)                      | Sigmoid function (logistic function)                     |\n",
        "| **Loss Function**          | Mean Squared Error (MSE)                              | Log Loss (Cross-Entropy Loss)                             |\n",
        "| **Equation**               | \\( y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots \\) | \\( p(y=1|X) = \\frac{1}{1 + e^{-z}} \\)                    |\n",
        "| **Use Case**               | Predict values like price, age, height, etc.          | Classify into two categories (e.g., spam vs. not spam)    |\n",
        "\n",
        "---\n",
        "\n",
        "### Key Takeaways:\n",
        "- **Linear Regression** is used for predicting **continuous** values, while **Logistic Regression** is used for **binary classification**.\n",
        "- The main difference lies in the output: Linear Regression produces continuous values, while Logistic Regression outputs probabilities, which are then mapped to binary classes (0 or 1).\n",
        "- The **sigmoid function** in logistic regression allows us to model probabilities and is particularly useful in classification problems where we need to classify instances into two categories.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the mathematical equation of Logistic Regression?\n"
      ],
      "metadata": {
        "id": "lTDfuk8ZNkab"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The mathematical equation of **Logistic Regression** is based on the **logistic (sigmoid) function**. The goal of logistic regression is to model the probability that a given input belongs to a particular class (usually binary classes: 0 or 1).\n",
        "\n",
        "### **Logistic Regression Equation**:\n",
        "\n",
        "1. **Linear Combination of Features**:\n",
        "   First, logistic regression computes a **linear combination** of the input features (similar to linear regression):\n",
        "   \\[\n",
        "   z = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n\n",
        "   \\]\n",
        "   Where:\n",
        "   - \\( \\beta_0 \\) is the intercept (bias).\n",
        "   - \\( \\beta_1, \\beta_2, \\dots, \\beta_n \\) are the coefficients (weights) of the features.\n",
        "   - \\( x_1, x_2, \\dots, x_n \\) are the input features.\n",
        "\n",
        "2. **Apply Sigmoid Function**:\n",
        "   The output \\( z \\) from the linear combination is passed through the **sigmoid function** (also known as the logistic function), which maps the value of \\( z \\) to a probability between 0 and 1:\n",
        "   \\[\n",
        "   \\hat{p}(y = 1 | X) = \\frac{1}{1 + e^{-z}}\n",
        "   \\]\n",
        "   Where:\n",
        "   - \\( \\hat{p}(y = 1 | X) \\) is the predicted probability that the output is class 1 (positive class).\n",
        "   - \\( e \\) is the base of the natural logarithm (approximately 2.718).\n",
        "   - \\( z \\) is the linear combination of the features, as calculated earlier.\n",
        "\n",
        "   This sigmoid function ensures that the output is a value between 0 and 1, which can be interpreted as a probability.\n",
        "\n",
        "3. **Prediction**:\n",
        "   - If \\( \\hat{p}(y = 1 | X) \\geq 0.5 \\), we predict class **1** (positive class).\n",
        "   - If \\( \\hat{p}(y = 1 | X) < 0.5 \\), we predict class **0** (negative class).\n",
        "\n",
        "### Final Equation of Logistic Regression:\n",
        "\n",
        "The logistic regression model can thus be summarized by the following equation:\n",
        "\\[\n",
        "\\hat{p}(y = 1 | X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n)}}\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- \\( \\hat{p}(y = 1 | X) \\) is the predicted probability that the target \\( y \\) is 1 (class 1).\n",
        "- \\( \\beta_0 \\) is the intercept term.\n",
        "- \\( \\beta_1, \\beta_2, \\dots, \\beta_n \\) are the coefficients of the features.\n",
        "- \\( x_1, x_2, \\dots, x_n \\) are the input features.\n",
        "\n",
        "### **Log-Loss Function**:\n",
        "The parameters \\( \\beta_0, \\beta_1, \\dots, \\beta_n \\) of the logistic regression model are typically estimated by **maximum likelihood estimation** (MLE), which maximizes the likelihood of the model given the training data. This is done by minimizing the **log-loss** or **binary cross-entropy** function:\n",
        "\\[\n",
        "\\text{Log Loss} = - \\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(\\hat{p}_i) + (1 - y_i) \\log(1 - \\hat{p}_i) \\right]\n",
        "\\]\n",
        "Where:\n",
        "- \\( N \\) is the number of training samples.\n",
        "- \\( y_i \\) is the actual class label (0 or 1) of the \\( i \\)-th sample.\n",
        "- \\( \\hat{p}_i \\) is the predicted probability for the \\( i \\)-th sample.\n",
        "\n",
        "### Key Points:\n",
        "- The **sigmoid function** ensures that the model’s output is a probability.\n",
        "- The **log-likelihood function** is used to find the best-fitting coefficients that minimize the error (or loss) in predicting the probability.\n",
        "\n"
      ],
      "metadata": {
        "id": "BlkXVv5nNkE2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Why do we use the Sigmoid function in Logistic Regression?\n"
      ],
      "metadata": {
        "id": "kVKuK4Z7Nj4q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **Sigmoid function** (also known as the **logistic function**) is used in **Logistic Regression** for several important reasons. Here's a detailed explanation of why it's essential:\n",
        "\n",
        "### 1. **Maps Output to a Probability (Between 0 and 1)**\n",
        "\n",
        "The primary reason for using the sigmoid function is that it transforms the raw output of the linear combination of input features (which can be any real number) into a **probability** that lies between 0 and 1.\n",
        "\n",
        "- **Raw Linear Model Output**: A linear regression model produces values ranging from negative to positive infinity, depending on the input. This isn't suitable for classification, as we want a probability.\n",
        "  \n",
        "- **Sigmoid Transformation**: The sigmoid function maps the output of the linear combination to a range between 0 and 1. This transformed output is interpreted as the **probability** that the observation belongs to the positive class (typically \\( y = 1 \\)).\n",
        "\n",
        "   The sigmoid function is defined as:\n",
        "   \\[\n",
        "   \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "   \\]\n",
        "   Where:\n",
        "   - \\( z \\) is the linear combination of input features (\\( z = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n \\)).\n",
        "   - The result of \\( \\sigma(z) \\) is always between 0 and 1, which we interpret as a probability.\n",
        "\n",
        "   This output allows the model to give a prediction of the form: \"There is a **p** probability that the target is class 1, and \\( 1 - p \\) probability that the target is class 0.\"\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Binary Classification Decision Making**\n",
        "\n",
        "In **binary classification**, we want to predict one of two classes (e.g., 0 or 1, true or false).\n",
        "\n",
        "- After applying the sigmoid function, we get a probability score, and then we can convert that probability into a class prediction:\n",
        "  - If the probability is greater than or equal to 0.5, the model predicts **class 1**.\n",
        "  - If the probability is less than 0.5, the model predicts **class 0**.\n",
        "\n",
        "The decision threshold (usually 0.5) is arbitrary but effective in dividing the probability into two classes. The sigmoid function is ideal for this purpose because it naturally maps any real-valued number to a value in the range [0, 1].\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Smooth Gradient for Optimization**\n",
        "\n",
        "The sigmoid function has a **smooth, continuous, and differentiable curve**, making it **amenable to optimization** using gradient-based algorithms, such as **gradient descent**.\n",
        "\n",
        "- **Differentiability**: The fact that the sigmoid function is differentiable means we can calculate its gradient (slope), which is crucial for optimization. Gradient descent relies on this gradient to minimize the loss function (e.g., **log-loss**).\n",
        "  \n",
        "- **Gradient Behavior**: The derivative of the sigmoid function is given by:\n",
        "  \\[\n",
        "  \\sigma'(z) = \\sigma(z) \\cdot (1 - \\sigma(z))\n",
        "  \\]\n",
        "  This property ensures that the gradient is **always positive** and never zero (except at the extreme values of \\( z \\)). This smooth gradient helps in convergence during training, which ensures that the model finds the best parameters efficiently.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Non-linearity and Flexibility**\n",
        "\n",
        "Although **Logistic Regression** is based on a linear model, the use of the **sigmoid function** introduces **non-linearity** into the model. This enables the model to **handle more complex decision boundaries**.\n",
        "\n",
        "- **Linear Model Without Sigmoid**: Without the sigmoid function, logistic regression would behave similarly to linear regression, where the output would be a continuous number. This is not suitable for binary classification tasks.\n",
        "  \n",
        "- **Non-linearity With Sigmoid**: By applying the sigmoid function, we can classify observations into distinct categories (0 or 1). The decision boundary between the classes becomes **non-linear**, which makes it possible to use logistic regression for classification problems.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **Interpretation as Odds and Log-Odds**\n",
        "\n",
        "In logistic regression, the **sigmoid function** provides a way to model the **log-odds** of an event occurring. This is related to how **odds ratios** are often interpreted in statistical analysis:\n",
        "\n",
        "- The **log-odds** are the natural logarithm of the odds:\n",
        "  \\[\n",
        "  \\text{log-odds} = \\log \\left( \\frac{p}{1 - p} \\right)\n",
        "  \\]\n",
        "  Where \\( p \\) is the probability predicted by the logistic regression model (i.e., \\( \\hat{p}(y = 1 | X) \\)).\n",
        "\n",
        "- The **log-odds** are modeled as a linear combination of the input features:\n",
        "  \\[\n",
        "  \\log \\left( \\frac{p}{1 - p} \\right) = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_n x_n\n",
        "  \\]\n",
        "\n",
        "- Applying the sigmoid function to the log-odds gives the **probability** of the event occurring:\n",
        "  \\[\n",
        "  p = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\dots + \\beta_n x_n)}}\n",
        "  \\]\n",
        "\n",
        "This log-odds interpretation is useful for understanding the relationship between the predictors and the outcome, and it also forms the basis for interpreting the coefficients in logistic regression.\n",
        "\n",
        "---\n",
        "\n",
        "### Summary: Why Use the Sigmoid Function?\n",
        "\n",
        "- **Probability Output**: The sigmoid function maps the model's raw output to a probability between 0 and 1, which is necessary for binary classification tasks.\n",
        "- **Decision Boundaries**: It enables the model to classify data points into two categories based on a probability threshold (usually 0.5).\n",
        "- **Optimization**: The sigmoid function is smooth and differentiable, making it suitable for gradient-based optimization methods like gradient descent.\n",
        "- **Log-Odds Interpretation**: It provides an intuitive way of modeling odds and log-odds, which is fundamental in statistical analysis.\n",
        "\n",
        "In summary, the **sigmoid function** is crucial in logistic regression because it allows us to convert a linear prediction into a probability, enabling the model to perform classification tasks effectively."
      ],
      "metadata": {
        "id": "Vq_VszJ_NjVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is the cost function of Logistic Regression?\n"
      ],
      "metadata": {
        "id": "O5xS5qQkNjSS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **cost function** of **Logistic Regression** is designed to measure how well the model's predictions match the actual data. The goal is to minimize this cost function during training, thereby improving the model's accuracy. In the case of logistic regression, the cost function is called **log loss** or **binary cross-entropy loss**.\n",
        "\n",
        "### Cost Function of Logistic Regression (Log Loss)\n",
        "\n",
        "Given a set of **n** training examples, where each training example consists of input features \\( \\mathbf{X} = (x_1, x_2, \\dots, x_n) \\) and the true target values \\( y_i \\in \\{0, 1\\} \\), the cost function in logistic regression is formulated as follows:\n",
        "\n",
        "\\[\n",
        "J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(\\hat{y}^{(i)}) + (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)}) \\right]\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- \\( J(\\theta) \\) is the **cost function** (also called the **log loss** or **binary cross-entropy**).\n",
        "- \\( m \\) is the total number of training examples.\n",
        "- \\( y^{(i)} \\) is the true class label for the \\( i \\)-th training example (either 0 or 1).\n",
        "- \\( \\hat{y}^{(i)} \\) is the predicted probability that \\( y^{(i)} = 1 \\), i.e., the model's predicted probability for the positive class for the \\( i \\)-th example. This is the output of the **sigmoid function**.\n",
        "\n",
        "The sigmoid function output \\( \\hat{y}^{(i)} \\) is given by:\n",
        "\\[\n",
        "\\hat{y}^{(i)} = \\frac{1}{1 + e^{-\\theta^T x^{(i)}}}\n",
        "\\]\n",
        "Where \\( \\theta^T x^{(i)} \\) is the linear combination of the input features for the \\( i \\)-th example, \\( \\theta \\) is the vector of model parameters (coefficients), and \\( x^{(i)} \\) is the vector of input features.\n",
        "\n",
        "---\n",
        "\n",
        "### Explanation of the Terms in the Cost Function\n",
        "\n",
        "1. **First Term** (\\( y^{(i)} \\log(\\hat{y}^{(i)}) \\)):\n",
        "   - This term is used when the true label \\( y^{(i)} \\) is **1**.\n",
        "   - The cost will be high if the predicted probability \\( \\hat{y}^{(i)} \\) is low for class 1 (i.e., the model is confident that the class is 0 when it should be 1).\n",
        "\n",
        "2. **Second Term** (\\( (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)}) \\)):\n",
        "   - This term is used when the true label \\( y^{(i)} \\) is **0**.\n",
        "   - The cost will be high if the predicted probability \\( \\hat{y}^{(i)} \\) is high for class 1 (i.e., the model is confident that the class is 1 when it should be 0).\n",
        "\n",
        "### Why This Cost Function?\n",
        "\n",
        "The **log loss** function is used in logistic regression because it encourages the model to output probabilities that are close to the true class labels.\n",
        "\n",
        "- If the model predicts a probability of 1 for an example that is actually class 1, the **log loss** will be very small (close to zero), meaning the model has made a correct and confident prediction.\n",
        "- If the model predicts a probability of 0 for an example that is actually class 1, the **log loss** will be very large, meaning the model has made an incorrect prediction with high confidence.\n",
        "- Similarly, for class 0, the model should predict probabilities close to 0 for class 0, and close to 1 for class 1 to minimize the cost.\n",
        "\n",
        "### Optimization (Minimization)\n",
        "\n",
        "The goal during the training of logistic regression is to find the optimal model parameters \\( \\theta \\) that minimize the **log loss**. This is typically done using **gradient descent** or other optimization algorithms.\n",
        "\n",
        "### Intuition Behind the Cost Function\n",
        "\n",
        "The **log loss** is a **penalization** for incorrect predictions:\n",
        "- The cost function heavily penalizes confident but incorrect predictions, particularly when the model is highly confident in a wrong classification.\n",
        "- The cost is lower for correct predictions, especially when the model is very confident (i.e., the predicted probability is near 0 for class 0, and near 1 for class 1).\n",
        "- The cost is minimized when the model's predictions closely match the true class labels.\n",
        "\n",
        "### Cost Function in Multi-Class Logistic Regression (Softmax Regression)\n",
        "\n",
        "For **multi-class classification** (more than two classes), the cost function becomes a generalization of the binary cross-entropy and is known as **categorical cross-entropy loss**, which uses the **softmax function** instead of the sigmoid function. However, for binary classification tasks, the **binary cross-entropy loss** (log loss) is the standard cost function.\n",
        "\n",
        "### Summary\n",
        "\n",
        "- The **cost function** in logistic regression is the **log loss** (or **binary cross-entropy loss**), which measures the difference between the true labels and the predicted probabilities.\n",
        "- The function penalizes incorrect predictions, especially those made with high confidence, and rewards the model for correct predictions with high confidence.\n",
        "- The objective is to **minimize** this cost function during training to optimize the model parameters (\\( \\theta \\)).\n",
        "\n"
      ],
      "metadata": {
        "id": "fTcSZVrvNjNr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What is Regularization in Logistic Regression? Why is it needed?\n"
      ],
      "metadata": {
        "id": "gqIFHiVbNjKm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **What is Regularization in Logistic Regression?**\n",
        "\n",
        "**Regularization** in **Logistic Regression** is a technique used to prevent the model from **overfitting** by adding a penalty term to the cost function. The penalty discourages the model from becoming too complex and helps it generalize better to unseen data.\n",
        "\n",
        "In logistic regression, the **regularization term** is added to the original cost function (log loss) to penalize large values of the model coefficients (\\( \\theta \\)).\n",
        "\n",
        "There are two common types of regularization used in logistic regression:\n",
        "\n",
        "1. **L2 Regularization (Ridge Regularization)**\n",
        "2. **L1 Regularization (Lasso Regularization)**\n",
        "\n",
        "---\n",
        "\n",
        "### **Why is Regularization Needed?**\n",
        "\n",
        "Regularization is needed in logistic regression for the following reasons:\n",
        "\n",
        "#### 1. **Preventing Overfitting**\n",
        "   - **Overfitting** occurs when a model learns the details and noise in the training data to such an extent that it negatively impacts the performance of the model on new data.\n",
        "   - Regularization helps to ensure that the model doesn't become too complex by making the model's coefficients small or zero, which forces the model to learn more general patterns instead of memorizing the training data.\n",
        "   - **Without regularization**, the model may have very large weights (coefficients), which means it's too sensitive to the input data, leading to poor generalization on new or unseen data.\n",
        "\n",
        "#### 2. **Improving Model Generalization**\n",
        "   - Regularization helps to **smooth** the model, preventing it from fitting every fluctuation in the training data.\n",
        "   - It encourages the model to find a balance between fitting the data well and keeping the model simple, leading to better performance on test data (i.e., higher generalization).\n",
        "\n",
        "#### 3. **Handling Multicollinearity**\n",
        "   - **Multicollinearity** happens when two or more features in the dataset are highly correlated. In such cases, the model can have unstable estimates of the coefficients (large fluctuations), making it difficult to interpret.\n",
        "   - Regularization helps to **shrink the coefficients** of correlated features, making the model more stable and reducing the impact of multicollinearity.\n",
        "\n",
        "---\n",
        "\n",
        "### **Types of Regularization**\n",
        "\n",
        "#### 1. **L2 Regularization (Ridge Regression)**\n",
        "\n",
        "L2 regularization adds a penalty term that is proportional to the **square of the magnitude of the coefficients** to the cost function. The cost function with L2 regularization is:\n",
        "\n",
        "\\[\n",
        "J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(\\hat{y}^{(i)}) + (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)}) \\right] + \\lambda \\sum_{j=1}^{n} \\theta_j^2\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- \\( \\lambda \\) is the regularization parameter that controls the strength of the regularization.\n",
        "- \\( \\sum_{j=1}^{n} \\theta_j^2 \\) is the sum of the squares of the model coefficients (excluding the intercept).\n",
        "\n",
        "The **L2 regularization term** penalizes large weights, encouraging the model to keep the weights smaller. This can help the model generalize better and prevent overfitting, especially when the number of features is large.\n",
        "\n",
        "#### 2. **L1 Regularization (Lasso Regression)**\n",
        "\n",
        "L1 regularization adds a penalty term that is proportional to the **absolute value of the coefficients** to the cost function. The cost function with L1 regularization is:\n",
        "\n",
        "\\[\n",
        "J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(\\hat{y}^{(i)}) + (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)}) \\right] + \\lambda \\sum_{j=1}^{n} |\\theta_j|\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- \\( \\lambda \\) is the regularization parameter.\n",
        "- \\( \\sum_{j=1}^{n} |\\theta_j| \\) is the sum of the absolute values of the coefficients.\n",
        "\n",
        "The **L1 regularization** term has the effect of **shrinking some coefficients to exactly zero**, effectively **performing feature selection**. This is useful when dealing with high-dimensional data, as it helps to select the most important features and ignore irrelevant ones.\n",
        "\n",
        "#### 3. **Elastic Net Regularization**\n",
        "\n",
        "Elastic Net is a hybrid of **L1** and **L2 regularization**, and it combines the benefits of both. It is particularly useful when there are many correlated features in the dataset. The cost function with Elastic Net regularization is:\n",
        "\n",
        "\\[\n",
        "J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(\\hat{y}^{(i)}) + (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)}) \\right] + \\lambda_1 \\sum_{j=1}^{n} |\\theta_j| + \\lambda_2 \\sum_{j=1}^{n} \\theta_j^2\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- \\( \\lambda_1 \\) and \\( \\lambda_2 \\) are regularization parameters controlling the strength of L1 and L2 penalties, respectively.\n",
        "\n",
        "Elastic Net is useful when the number of predictors exceeds the number of observations and when there is multicollinearity.\n",
        "\n",
        "---\n",
        "\n",
        "### **How Regularization Works:**\n",
        "\n",
        "1. **Training without Regularization**:\n",
        "   - When you train a logistic regression model **without regularization**, the algorithm tries to minimize the cost function without any constraint on the size of the coefficients. This can lead to **overfitting**, especially when the model is too complex or the dataset is noisy.\n",
        "   \n",
        "2. **Training with Regularization**:\n",
        "   - When you train a logistic regression model **with regularization**, the algorithm minimizes the cost function **plus a penalty term** that discourages large coefficients.\n",
        "   - The penalty discourages the model from giving too much importance to any particular feature, thus preventing the model from overfitting to noise or irrelevant patterns in the data.\n",
        "\n",
        "   - The **strength of the penalty** is controlled by the regularization parameter \\( \\lambda \\):\n",
        "     - A **large value of \\( \\lambda \\)** applies a strong penalty, forcing the coefficients to be very small (more regularization).\n",
        "     - A **small value of \\( \\lambda \\)** applies a weak penalty, making the regularization effect less significant (less regularization).\n",
        "     - If \\( \\lambda = 0 \\), there is no regularization (equivalent to standard logistic regression without regularization).\n",
        "\n",
        "---\n",
        "\n",
        "### **Choosing the Regularization Parameter (λ)**\n",
        "\n",
        "- The regularization parameter \\( \\lambda \\) needs to be chosen carefully because it controls the trade-off between **bias** and **variance**:\n",
        "  - **High \\( \\lambda \\)** (strong regularization): Forces the model to have small coefficients, which may lead to high bias (underfitting).\n",
        "  - **Low \\( \\lambda \\)** (weak regularization): Allows the model to fit the training data more closely, but it may cause overfitting and high variance.\n",
        "\n",
        "- To select the optimal value of \\( \\lambda \\), **cross-validation** is often used. A common approach is to try different values of \\( \\lambda \\) and choose the one that minimizes the **cross-validation error**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Summary: Why is Regularization Needed in Logistic Regression?**\n",
        "\n",
        "- **Prevents Overfitting**: Regularization helps prevent the model from overfitting the training data, especially when the number of features is large.\n",
        "- **Improves Generalization**: By adding a penalty to large coefficients, regularization helps the model generalize better to new, unseen data.\n",
        "- **Feature Selection**: L1 regularization (Lasso) can shrink some coefficients to zero, effectively removing irrelevant features and simplifying the model.\n",
        "- **Stabilizes the Model**: Regularization helps reduce the variance of the model, making it more robust and stable, particularly in the presence of multicollinearity.\n",
        "\n",
        "By using regularization, logistic regression can achieve a better balance between fitting the training data and generalizing to new data, leading to improved performance on test data.\n"
      ],
      "metadata": {
        "id": "xMtD0EEDNjEW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Explain the difference between Lasso, Ridge, and Elastic Net regression.\n",
        "\n"
      ],
      "metadata": {
        "id": "tQv423OANi-S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lasso, Ridge, and Elastic Net are three different types of **regularized linear regression models** that are used to prevent overfitting and improve the model’s ability to generalize to new data. They differ primarily in how they penalize the model’s coefficients during training, each with unique characteristics that can be more or less useful depending on the data and the problem at hand.\n",
        "\n",
        "### 1. **Ridge Regression (L2 Regularization)**\n",
        "\n",
        "#### **Definition**:\n",
        "- **Ridge regression** adds a penalty proportional to the **square of the magnitude of the coefficients** (L2 norm) to the cost function.\n",
        "  \n",
        "#### **Cost Function** (with Ridge):\n",
        "\\[\n",
        "J(\\theta) = \\text{MSE} + \\lambda \\sum_{j=1}^{n} \\theta_j^2\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- \\( \\lambda \\) is the regularization parameter (controls the strength of the penalty).\n",
        "- \\( \\theta_j \\) are the model's coefficients.\n",
        "- **MSE (Mean Squared Error)** is the usual cost function used in linear regression.\n",
        "\n",
        "#### **Key Features**:\n",
        "- **Penalizes large coefficients**: Ridge regression penalizes the square of the coefficients. The larger the coefficients, the greater the penalty, which helps in controlling overfitting.\n",
        "- **All coefficients are shrunk towards zero**: Ridge will not shrink coefficients exactly to zero (i.e., it does not perform feature selection).\n",
        "- **Useful when**: The dataset has many features that contribute to the prediction, but you want to avoid overfitting.\n",
        "\n",
        "#### **When to Use Ridge**:\n",
        "- When all features are expected to contribute to the prediction, or if there is no strong reason to exclude any features.\n",
        "- Ridge is good when there is **multicollinearity** (when features are highly correlated), as it helps stabilize the solution.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Lasso Regression (L1 Regularization)**\n",
        "\n",
        "#### **Definition**:\n",
        "- **Lasso regression** adds a penalty proportional to the **absolute value of the coefficients** (L1 norm) to the cost function.\n",
        "  \n",
        "#### **Cost Function** (with Lasso):\n",
        "\\[\n",
        "J(\\theta) = \\text{MSE} + \\lambda \\sum_{j=1}^{n} |\\theta_j|\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- \\( \\lambda \\) is the regularization parameter (controls the strength of the penalty).\n",
        "- \\( \\theta_j \\) are the model's coefficients.\n",
        "- **MSE (Mean Squared Error)** is the usual cost function used in linear regression.\n",
        "\n",
        "#### **Key Features**:\n",
        "- **Encourages sparsity**: Lasso has the property of **shrinking some coefficients to zero**, effectively performing **feature selection**. This means that Lasso can be used to select a subset of the most important features, making the model simpler and easier to interpret.\n",
        "- **Variable selection**: Lasso can completely eliminate some features by setting their corresponding coefficients to zero, making it particularly useful when you suspect that only a subset of features is important.\n",
        "- **Useful when**: There are many irrelevant features in the dataset that should be excluded.\n",
        "\n",
        "#### **When to Use Lasso**:\n",
        "- When you believe that **only a few features are important**, or you want to automatically select a subset of features.\n",
        "- When you have a high-dimensional dataset (many features), Lasso helps in simplifying the model by reducing the number of features.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Elastic Net Regression**\n",
        "\n",
        "#### **Definition**:\n",
        "- **Elastic Net** is a **combination of both Lasso (L1) and Ridge (L2) regularization**. It uses both the absolute and squared coefficients to calculate the penalty term.\n",
        "\n",
        "#### **Cost Function** (with Elastic Net):\n",
        "\\[\n",
        "J(\\theta) = \\text{MSE} + \\lambda_1 \\sum_{j=1}^{n} |\\theta_j| + \\lambda_2 \\sum_{j=1}^{n} \\theta_j^2\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- \\( \\lambda_1 \\) and \\( \\lambda_2 \\) are the regularization parameters for L1 and L2 regularization, respectively.\n",
        "- **MSE (Mean Squared Error)** is the usual cost function used in linear regression.\n",
        "\n",
        "#### **Key Features**:\n",
        "- **Combines Lasso and Ridge**: Elastic Net introduces the flexibility of using both the **L1** (Lasso) and **L2** (Ridge) penalties. It works well when there are multiple correlated features.\n",
        "- **Tuning Parameters**: Elastic Net has two hyperparameters \\( \\lambda_1 \\) (for L1) and \\( \\lambda_2 \\) (for L2). This allows you to control the balance between Lasso and Ridge regularization.\n",
        "- **Useful when**: You have a dataset with **highly correlated features**. In such cases, Lasso may fail to select one feature from a group of highly correlated features, but Elastic Net can perform better by selecting and shrinking them appropriately.\n",
        "\n",
        "#### **When to Use Elastic Net**:\n",
        "- When the dataset has **high multicollinearity** (many highly correlated features).\n",
        "- When you need to combine the benefits of both Lasso and Ridge regularization.\n",
        "- Elastic Net is a good option when you are unsure whether Lasso or Ridge would be more effective, as it allows you to use both.\n",
        "\n",
        "---\n",
        "\n",
        "### **Comparison Table**\n",
        "\n",
        "| Feature                          | **Ridge Regression**                             | **Lasso Regression**                             | **Elastic Net**                               |\n",
        "|-----------------------------------|--------------------------------------------------|--------------------------------------------------|------------------------------------------------|\n",
        "| **Regularization Type**           | L2 Regularization                               | L1 Regularization                               | Combination of L1 and L2 Regularization       |\n",
        "| **Penalizes**                     | Square of coefficients                          | Absolute value of coefficients                  | Combination of square and absolute values     |\n",
        "| **Feature Selection**             | No, all features are kept                       | Yes, features can be set to zero                | Yes, but allows correlated features to stay   |\n",
        "| **Effect on Coefficients**        | Shrinks coefficients towards zero, but not exactly zero | Shrinks some coefficients to exactly zero      | Shrinks coefficients, some may be exactly zero |\n",
        "| **Suitable for**                  | High-dimensional data with many features        | Data with many irrelevant features              | Data with correlated features                 |\n",
        "| **Handling Multicollinearity**    | Works well with correlated features             | May select one of the correlated features and eliminate others | Performs better with correlated features      |\n",
        "| **Hyperparameters**               | One parameter \\( \\lambda \\) controlling the strength of the penalty | One parameter \\( \\lambda \\) controlling the strength of the penalty | Two parameters: \\( \\lambda_1 \\) (L1) and \\( \\lambda_2 \\) (L2) |\n",
        "| **Interpretability**              | Not as interpretable as Lasso                   | More interpretable due to automatic feature selection | Similar to Ridge and Lasso, with flexibility |\n",
        "\n",
        "---\n",
        "\n",
        "### **When to Use Which?**\n",
        "\n",
        "- **Ridge Regression**: Use Ridge when you have a large number of features and you believe all of them contribute to the prediction, and when you need to address multicollinearity.\n",
        "- **Lasso Regression**: Use Lasso when you suspect that only a small subset of features are important, or when you want to automatically perform feature selection.\n",
        "- **Elastic Net**: Use Elastic Net when you have many correlated features and you're unsure whether to use Lasso or Ridge. Elastic Net combines the benefits of both Lasso and Ridge.\n",
        "\n",
        "---\n",
        "\n",
        "### **Summary**\n",
        "\n",
        "- **Ridge**: Penalizes the squared coefficients; good for datasets with many features and multicollinearity.\n",
        "- **Lasso**: Penalizes the absolute coefficients; performs feature selection by shrinking some coefficients to zero.\n",
        "- **Elastic Net**: A combination of both Lasso and Ridge; works well when you have correlated features and need the flexibility of both regularizations.\n",
        "\n"
      ],
      "metadata": {
        "id": "PWAo5yJHNi7T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. When should we use Elastic Net instead of Lasso or Ridge?\n"
      ],
      "metadata": {
        "id": "bcL_kPPXNi2a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **When to Use Elastic Net Instead of Lasso or Ridge**\n",
        "\n",
        "Elastic Net is a regularization technique that combines the properties of both **Lasso** (L1 regularization) and **Ridge** (L2 regularization), allowing you to leverage the benefits of both. It is particularly useful when the dataset or problem presents certain characteristics that make Lasso or Ridge individually less effective.\n",
        "\n",
        "Here are some key scenarios when you should use **Elastic Net** instead of **Lasso** or **Ridge**:\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **When There Are Highly Correlated Features**\n",
        "   - **Elastic Net** is especially useful when you have **highly correlated features** in your dataset. In such cases, Lasso might arbitrarily select one feature and eliminate others in the correlated group, which can be problematic when multiple features are equally important. Ridge regression, on the other hand, shrinks coefficients without eliminating any features, but still includes all features in the model.\n",
        "   - **Elastic Net** is able to handle correlations by **shrinking correlated features together** and selecting only one representative from a group of correlated features, making it a good option for such datasets.\n",
        "\n",
        "   **Why Elastic Net?**\n",
        "   - Elastic Net balances the benefits of **Lasso** (which can exclude irrelevant features) and **Ridge** (which handles correlated features well). It retains multiple correlated features but may still zero out irrelevant ones.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **When the Number of Features Is Greater than the Number of Observations**\n",
        "   - If the number of features (variables) is greater than the number of observations (data points), both **Lasso** and **Ridge** might not perform well on their own. Lasso could select too few features, while Ridge might not perform effective feature selection.\n",
        "   - **Elastic Net** works well in this case because it combines Lasso's feature selection ability and Ridge's stabilization of coefficients, allowing it to select the most important features and prevent overfitting.\n",
        "\n",
        "   **Why Elastic Net?**\n",
        "   - In scenarios where there are a lot of features but relatively fewer observations, Elastic Net can balance regularization and feature selection, leading to better generalization and more stable models.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **When You Are Unsure Whether to Use Lasso or Ridge**\n",
        "   - Sometimes, it is not clear whether **Lasso** or **Ridge** will work better for a given problem. Elastic Net provides a **flexible approach** that combines both Lasso and Ridge regularization.\n",
        "   - By using Elastic Net, you can take advantage of both types of regularization and fine-tune the balance between L1 and L2 penalties to see which works better for your dataset.\n",
        "\n",
        "   **Why Elastic Net?**\n",
        "   - Elastic Net allows you to **experiment with both L1 and L2 penalties** at the same time. It has two hyperparameters \\( \\lambda_1 \\) and \\( \\lambda_2 \\), so it gives you more control over the trade-off between Lasso and Ridge regularization, and you can adjust it based on your dataset's performance.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **When You Have Multiple Interactions Among Features**\n",
        "   - If your dataset has **interaction terms** or **complex relationships** between features, **Ridge** might perform better because it tends to shrink coefficients while keeping them all in the model. **Lasso**, however, might select only a few features and exclude others that could be important in the interactions.\n",
        "   - **Elastic Net** performs better when there are both main effects and interaction terms. The combined penalties from L1 and L2 regularization allow it to better capture interactions and complex relationships while still performing feature selection.\n",
        "\n",
        "   **Why Elastic Net?**\n",
        "   - Elastic Net helps capture both individual features and interactions between features, especially in datasets with complex relationships.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **When You Want the Benefits of Both Feature Selection and Shrinking Coefficients**\n",
        "   - **Lasso** can perform **feature selection** by setting some coefficients to exactly zero, making it useful for sparse models where only a few features are relevant. However, Lasso can behave erratically when features are highly correlated.\n",
        "   - **Ridge** shrinks coefficients but does not perform feature selection. It is useful when all features contribute to the prediction.\n",
        "   - **Elastic Net** offers a **hybrid approach**, performing both **feature selection** (like Lasso) and **shrinkage** (like Ridge). This combination is particularly useful when you want to create simpler models while still retaining important features from correlated groups.\n",
        "\n",
        "   **Why Elastic Net?**\n",
        "   - Elastic Net is a **compromise** that gives you the **sparsity of Lasso** while also benefiting from the **stability of Ridge**. This makes it ideal when you want a balance of feature selection and coefficient shrinkage, especially for data with many features.\n",
        "\n",
        "---\n",
        "\n",
        "### **Summary of When to Use Elastic Net**\n",
        "\n",
        "1. **High Correlation Between Features**: Elastic Net handles multicollinearity better than Lasso and performs better than Ridge when dealing with correlated features.\n",
        "2. **More Features Than Observations**: Elastic Net performs well when the number of features exceeds the number of observations, preventing overfitting.\n",
        "3. **Uncertainty Between Lasso and Ridge**: If you’re unsure whether to use Lasso or Ridge, Elastic Net allows you to use both regularizations and find the best combination.\n",
        "4. **Interaction Effects**: Elastic Net is useful when you have both main effects and interaction terms among features.\n",
        "5. **Desire for Feature Selection and Shrinkage**: Elastic Net provides both feature selection (like Lasso) and coefficient shrinkage (like Ridge), offering a more balanced approach.\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Advantages of Elastic Net Over Lasso and Ridge**\n",
        "- **Combines the strengths of Lasso and Ridge**: Elastic Net provides a balance between **feature selection** and **shrinkage**, allowing it to work effectively in a wider range of scenarios.\n",
        "- **More robust for correlated features**: It stabilizes Lasso’s tendency to pick one feature from a group of correlated features by grouping them together.\n",
        "- **Better generalization**: Elastic Net can provide better generalization in cases where Lasso or Ridge alone might fail, especially in high-dimensional, correlated datasets.\n",
        "\n",
        "---\n",
        "\n",
        "In conclusion, **Elastic Net** is a versatile regularization technique that is **particularly useful when dealing with correlated features, high-dimensional data, or when you are unsure whether to use Lasso or Ridge**. It combines the best of both worlds and can improve model performance and stability in such situations."
      ],
      "metadata": {
        "id": "H0v0O-jnPjLM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is the impact of the regularization parameter (1) in Logistic Regression?"
      ],
      "metadata": {
        "id": "i3jHiPwaRWAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "The **regularization parameter** (denoted as \\( \\lambda \\) or \\( C \\), depending on the specific implementation) in **Logistic Regression** plays a crucial role in controlling the **complexity of the model** and helps in **preventing overfitting**. It does this by penalizing the size of the coefficients, which can have a significant impact on the model’s behavior, performance, and interpretability.\n",
        "\n",
        "### **Impact of the Regularization Parameter (λ or C)**\n",
        "\n",
        "In **Logistic Regression**, the regularization term is added to the cost function (or loss function). The regularization parameter controls how much penalty is applied to the model’s coefficients. Let's break it down based on whether you are using **L1 regularization** (Lasso) or **L2 regularization** (Ridge):\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **Regularization in Logistic Regression**:\n",
        "\n",
        "The **cost function** for logistic regression with regularization looks like this:\n",
        "\n",
        "\\[\n",
        "J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left( y^{(i)} \\log(h_\\theta(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h_\\theta(x^{(i)})) \\right) + \\lambda \\cdot \\text{Penalty}\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- \\( m \\) is the number of training examples.\n",
        "- \\( h_\\theta(x) \\) is the logistic regression hypothesis (predicted probability).\n",
        "- \\( \\lambda \\) is the regularization parameter that controls the strength of the regularization.\n",
        "- The **penalty term** depends on the type of regularization (L1 or L2).\n",
        "\n",
        "#### L1 Regularization (Lasso):\n",
        "For L1 regularization, the penalty term is:\n",
        "\n",
        "\\[\n",
        "\\lambda \\sum_{j=1}^{n} |\\theta_j|\n",
        "\\]\n",
        "\n",
        "#### L2 Regularization (Ridge):\n",
        "For L2 regularization, the penalty term is:\n",
        "\n",
        "\\[\n",
        "\\lambda \\sum_{j=1}^{n} \\theta_j^2\n",
        "\\]\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Effects of the Regularization Parameter (\\( \\lambda \\) or \\( C \\))**:\n",
        "\n",
        "#### **Increasing \\( \\lambda \\) (Strong Regularization)**:\n",
        "- **Larger penalty on coefficients**: As \\( \\lambda \\) increases, the model is penalized more heavily for large coefficients. This forces the model to produce smaller coefficients (shrinking them toward zero).\n",
        "- **More Regularization**: With larger values of \\( \\lambda \\), the model becomes **simpler** and **less flexible**. It may underfit the data, meaning it might not capture important patterns in the data because it is overly constrained.\n",
        "- **Sparsity (in the case of L1)**: For **L1 regularization**, increasing \\( \\lambda \\) can cause more coefficients to be shrunk to zero, effectively removing some features entirely from the model.\n",
        "- **Smaller \\( R^2 \\) on Training Data**: The model might have a lower \\( R^2 \\) (or a higher error) on the training data because it is overly constrained, but this can result in better generalization on unseen data.\n",
        "\n",
        "#### **Decreasing \\( \\lambda \\) (Weak Regularization)**:\n",
        "- **Smaller penalty on coefficients**: As \\( \\lambda \\) decreases, the penalty on the coefficients becomes weaker, allowing them to take larger values.\n",
        "- **Less Regularization**: With smaller values of \\( \\lambda \\), the model can fit the data more closely, which can lead to **overfitting** if the model is too complex (i.e., it memorizes noise in the data instead of generalizing to new, unseen data).\n",
        "- **More Flexibility**: The model is free to take any values for the coefficients, which could allow it to fit the training data very well. However, this may result in poor performance on the test set because the model has learned noise from the training data.\n",
        "- **Higher \\( R^2 \\) on Training Data**: The model will have a higher \\( R^2 \\) (or a lower error) on the training data because it fits the data more closely, but it will likely perform poorly on new data (test set) due to overfitting.\n",
        "\n",
        "#### **Interpretation of Regularization Parameter \\( C \\)**:\n",
        "- In **scikit-learn**, the regularization parameter is often denoted as **\\( C \\)**, which is the **inverse of \\( \\lambda \\)**. This means:\n",
        "  - **Higher \\( C \\)** = **weaker regularization** (similar to decreasing \\( \\lambda \\)).\n",
        "  - **Lower \\( C \\)** = **stronger regularization** (similar to increasing \\( \\lambda \\)).\n",
        "\n",
        "---\n",
        "\n",
        "### **Effect on Model Behavior**:\n",
        "\n",
        "#### **1. Underfitting vs. Overfitting**:\n",
        "- **High \\( \\lambda \\) or low \\( C \\)**:\n",
        "  - The model will likely **underfit** the data because it is too constrained by the regularization.\n",
        "  - It may not capture complex patterns in the data.\n",
        "  - It generally has **lower variance** but **higher bias**.\n",
        "\n",
        "- **Low \\( \\lambda \\) or high \\( C \\)**:\n",
        "  - The model may **overfit** the data because it is allowed to fit the training data too closely.\n",
        "  - It will have **lower bias** but **higher variance**.\n",
        "\n",
        "The goal is to find an optimal balance by tuning \\( \\lambda \\) (or \\( C \\)) so that the model neither overfits nor underfits the data. This is typically done through **cross-validation**.\n",
        "\n",
        "#### **2. Feature Selection (Lasso Regularization)**:\n",
        "- **L1 Regularization (Lasso)** allows some coefficients to be **shrunk to exactly zero**.\n",
        "  - **High \\( \\lambda \\)** can cause more coefficients to become exactly zero, which helps in **feature selection** by excluding irrelevant or redundant features.\n",
        "  - **Low \\( \\lambda \\)** will not shrink coefficients to zero, and all features will contribute to the model.\n",
        "\n",
        "#### **3. Stability of Coefficients (Ridge Regularization)**:\n",
        "- **L2 Regularization (Ridge)** does not set coefficients to exactly zero, but it **shrinks them** towards zero, which can make the model more stable and less sensitive to small changes in the data.\n",
        "  - **High \\( \\lambda \\)** will shrink all coefficients toward zero, making the model less complex and reducing the chance of overfitting.\n",
        "  - **Low \\( \\lambda \\)** allows coefficients to take large values, increasing the complexity of the model.\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion:**\n",
        "\n",
        "The **regularization parameter** \\( \\lambda \\) (or \\( C \\)) in Logistic Regression determines how much regularization is applied to the model. The key impact of this parameter is:\n",
        "- **Higher \\( \\lambda \\)** (or **lower \\( C \\)**) results in **stronger regularization**, which shrinks the coefficients toward zero and reduces overfitting, but may lead to underfitting if set too high.\n",
        "- **Lower \\( \\lambda \\)** (or **higher \\( C \\)**) results in **weaker regularization**, which allows the model to fit the training data more closely but may lead to overfitting if set too low.\n",
        "\n",
        "Finding the optimal value of \\( \\lambda \\) (or \\( C \\)) is crucial for building a well-generalized model that performs well on both training and test data. This is typically achieved through **cross-validation**."
      ],
      "metadata": {
        "id": "btsQhbo1RWiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What are the key assumptions of Logistic Regression?"
      ],
      "metadata": {
        "id": "6qIULYJWRW6C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Key Assumptions of Logistic Regression\n",
        "\n",
        "**Logistic Regression** is a popular algorithm used for binary classification problems. While it is a flexible and widely-used model, it still comes with certain assumptions that need to be considered for optimal performance. Here are the key assumptions of logistic regression:\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **Linearity of the Logit (Log-Odds)**\n",
        "   - Logistic regression assumes that there is a **linear relationship** between the independent variables (predictors) and the **logit** (log-odds) of the dependent variable.\n",
        "   - The logit is the natural logarithm of the odds ratio, and this relationship can be expressed as:\n",
        "     \\[\n",
        "     \\log\\left(\\frac{P(y=1)}{1 - P(y=1)}\\right) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_n X_n\n",
        "     \\]\n",
        "     Where:\n",
        "     - \\( P(y=1) \\) is the probability of the event occurring.\n",
        "     - \\( X_1, X_2, \\dots, X_n \\) are the independent variables.\n",
        "     - \\( \\beta_0, \\beta_1, \\dots, \\beta_n \\) are the model parameters.\n",
        "   - This assumption means that the relationship between the **predictors** and the **log-odds** of the target is linear, but not necessarily the target variable itself. It allows logistic regression to model a binary outcome based on linear combinations of predictors.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Independence of Observations**\n",
        "   - The **observations** in the dataset must be **independent** of each other. This means that there should be no correlation between the data points.\n",
        "   - Logistic regression does not work well when there are **correlated observations**. For example, in a time-series problem or hierarchical data (such as repeated measurements from the same individuals), the assumption of independence may be violated.\n",
        "   - If independence is violated, techniques like **Generalized Estimating Equations (GEE)** or **Mixed-effects models** may be more appropriate.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **No or Little Multicollinearity**\n",
        "   - Logistic regression assumes that the independent variables are **not highly correlated** with each other, i.e., **multicollinearity** should be minimal.\n",
        "   - High multicollinearity means that one predictor can be linearly predicted from the others with high accuracy, leading to **unstable estimates** of the regression coefficients. This can make the model difficult to interpret and may lead to inflated standard errors.\n",
        "   - You can check for multicollinearity using the **Variance Inflation Factor (VIF)** or **Correlation Matrix**.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **No Perfect Separation**\n",
        "   - Logistic regression assumes that there is **no perfect separation** between the classes.\n",
        "   - **Perfect separation** occurs when the predictor variables completely separate the classes (e.g., all observations of class 1 have a value of \\( X_1 > 10 \\), and all observations of class 0 have \\( X_1 < 10 \\)). In this case, the model will fail to converge or provide misleading results because the maximum likelihood estimates are undefined or infinite.\n",
        "   - This assumption can be tested by checking the model's convergence behavior during fitting.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **Large Sample Size**\n",
        "   - Logistic regression performs best with a **large sample size** to produce reliable and stable estimates. In cases where the sample size is small, the model may overfit or fail to converge.\n",
        "   - In general, logistic regression is considered to be **reliable** if the number of observations is at least **10 times the number of predictor variables** (this is a rule of thumb). The larger the sample size, the more reliable the estimates of the regression coefficients.\n",
        "\n",
        "---\n",
        "\n",
        "### 6. **No Extreme Outliers**\n",
        "   - Logistic regression assumes that there are **no extreme outliers** in the predictor variables that can heavily influence the model's parameters.\n",
        "   - While logistic regression is not as sensitive to outliers as linear regression, extreme outliers can still distort the relationship between the predictors and the outcome variable, leading to inaccurate predictions.\n",
        "   - Outliers can be detected using **box plots** or **Z-scores** and can be treated by removing or transforming the data.\n",
        "\n",
        "---\n",
        "\n",
        "### 7. **Homoscedasticity of Errors**\n",
        "   - While logistic regression does not have a strict assumption of **homoscedasticity** (constant variance of residuals) like linear regression, the **errors should be independent and identically distributed (i.i.d.)**.\n",
        "   - This means that the model’s errors should not depend on the predictor values and should follow a distribution similar across all observations.\n",
        "   - In practice, this assumption is less critical for logistic regression than for linear regression, but deviations can still affect the model's performance.\n",
        "\n",
        "---\n",
        "\n",
        "### 8. **Binary Outcome**\n",
        "   - Logistic regression is a **binary classification algorithm** and assumes that the dependent variable is **binary** (i.e., it has only two possible outcomes: 0 and 1, True and False, etc.).\n",
        "   - For problems with more than two classes, you would need to use **multinomial logistic regression** or **other methods** designed for multiclass classification.\n",
        "\n",
        "---\n",
        "\n",
        "### **Summary of Key Assumptions**:\n",
        "1. **Linearity of the Logit**: The relationship between predictors and the log-odds is linear.\n",
        "2. **Independence of Observations**: Data points must be independent of each other.\n",
        "3. **No or Little Multicollinearity**: Predictors should not be highly correlated.\n",
        "4. **No Perfect Separation**: There should be no perfect separation of classes based on predictor variables.\n",
        "5. **Large Sample Size**: Logistic regression works best with a sufficiently large sample size.\n",
        "6. **No Extreme Outliers**: Outliers can distort model coefficients.\n",
        "7. **Homoscedasticity of Errors**: Residuals should have constant variance and be independent.\n",
        "8. **Binary Outcome**: The target variable should be binary for standard logistic regression.\n",
        "\n",
        "---\n",
        "\n",
        "These assumptions are important to keep in mind when fitting a logistic regression model. If any of the assumptions are violated, it can lead to biased estimates, poor model performance, or incorrect inferences. In such cases, techniques like **regularization**, **transformations**, or using different models (e.g., **multinomial logistic regression** for multiclass problems) may be needed."
      ],
      "metadata": {
        "id": "CJ_GPs6fRXS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What are some alternatives to Logistic Regression for classification tasks?\n"
      ],
      "metadata": {
        "id": "eAjR2VVcSBu5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great question! While **Logistic Regression** is a strong baseline for binary classification tasks—simple, interpretable, and efficient—it may not always be the best choice, especially when the data is non-linear or high-dimensional. Here are some powerful **alternatives to Logistic Regression** for classification tasks:\n",
        "\n",
        "---\n",
        "\n",
        "### 🔁 **1. Decision Trees**\n",
        "- **Pros**: Easy to interpret, handles both numerical and categorical data, captures non-linear relationships.\n",
        "- **Cons**: Prone to overfitting.\n",
        "- **Best For**: Situations where model interpretability is crucial.\n",
        "\n",
        "---\n",
        "\n",
        "### 🐘 **2. Random Forest**\n",
        "- **Type**: Ensemble of Decision Trees (Bagging).\n",
        "- **Pros**: Reduces overfitting, handles large datasets and high dimensionality well.\n",
        "- **Cons**: Less interpretable than a single tree.\n",
        "- **Best For**: Most general-purpose classification problems.\n",
        "\n",
        "---\n",
        "\n",
        "### 💡 **3. Gradient Boosting Machines (e.g., XGBoost, LightGBM, CatBoost)**\n",
        "- **Type**: Ensemble method using boosting.\n",
        "- **Pros**: Highly accurate, handles missing data, supports categorical features.\n",
        "- **Cons**: Can be slow to train, hyperparameter tuning needed.\n",
        "- **Best For**: Complex datasets where accuracy is more important than interpretability.\n",
        "\n",
        "---\n",
        "\n",
        "### 📈 **4. Support Vector Machines (SVM)**\n",
        "- **Pros**: Effective in high-dimensional spaces, good with non-linear decision boundaries (using kernel trick).\n",
        "- **Cons**: Computationally intensive with large datasets, hard to interpret.\n",
        "- **Best For**: Small to medium datasets, especially when margin separation is crucial.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 **5. Neural Networks (Deep Learning)**\n",
        "- **Pros**: Can model highly complex relationships, works well with large datasets.\n",
        "- **Cons**: Requires a lot of data, less interpretable, longer training time.\n",
        "- **Best For**: Image, text, speech, or other unstructured data.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧮 **6. K-Nearest Neighbors (KNN)**\n",
        "- **Pros**: Simple, no training phase, works well when the decision boundary is irregular.\n",
        "- **Cons**: Slow prediction with large datasets, sensitive to irrelevant features and feature scaling.\n",
        "- **Best For**: Simple problems, low-dimensional data.\n",
        "\n",
        "---\n",
        "\n",
        "### 🎲 **7. Naive Bayes**\n",
        "- **Pros**: Very fast, works well with text (e.g., spam detection), handles high-dimensional data.\n",
        "- **Cons**: Assumes feature independence (rarely true), not good with highly correlated features.\n",
        "- **Best For**: Text classification, document categorization.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧮 **8. Linear Discriminant Analysis (LDA)**\n",
        "- **Pros**: Works well when classes are normally distributed and have equal covariance.\n",
        "- **Cons**: Assumptions can be limiting, less flexible than non-linear models.\n",
        "- **Best For**: Situations where data meets LDA assumptions.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧪 Bonus: **Rule-Based Classifiers**\n",
        "- Good for expert systems or decision-making with interpretable rules.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ **When to Use Logistic Regression**\n",
        "- When you need a **quick baseline**.\n",
        "- When **interpretability** matters.\n",
        "- When the relationship between features and outcome is **approximately linear**.\n",
        "- When you’re working with **binary classification** and **moderate-size data**.\n",
        "\n"
      ],
      "metadata": {
        "id": "MR56IBK8SCQf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. What are Classification Evaluation Metrics?\n"
      ],
      "metadata": {
        "id": "MoETiiUVSTPQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great question! When you're working with **classification problems**, evaluating your model's performance correctly is essential. Here’s a breakdown of the most important **Classification Evaluation Metrics** you should know:\n",
        "\n",
        "---\n",
        "\n",
        "## 🔍 **1. Accuracy**\n",
        "- **Definition**: The proportion of correctly classified samples.\n",
        "- **Formula**:\n",
        "  \\[\n",
        "  \\text{Accuracy} = \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN}}\n",
        "  \\]\n",
        "- **Best When**: Classes are **balanced**.\n",
        "- **Caution**: Can be **misleading** with **imbalanced datasets** (e.g., 95% accuracy when model predicts only the majority class).\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **2. Precision (Positive Predictive Value)**\n",
        "- **Definition**: Out of all predicted positives, how many were actually positive?\n",
        "- **Formula**:\n",
        "  \\[\n",
        "  \\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}\n",
        "  \\]\n",
        "- **Best When**: The **cost of false positives** is high (e.g., spam detection).\n",
        "\n",
        "---\n",
        "\n",
        "## 🎯 **3. Recall (Sensitivity / True Positive Rate)**\n",
        "- **Definition**: Out of all actual positives, how many were correctly identified?\n",
        "- **Formula**:\n",
        "  \\[\n",
        "  \\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\n",
        "  \\]\n",
        "- **Best When**: The **cost of false negatives** is high (e.g., cancer detection).\n",
        "\n",
        "---\n",
        "\n",
        "## ⚖️ **4. F1 Score**\n",
        "- **Definition**: The harmonic mean of precision and recall.\n",
        "- **Formula**:\n",
        "  \\[\n",
        "  \\text{F1 Score} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
        "  \\]\n",
        "- **Best When**: You need a balance between **precision and recall**, especially with **imbalanced data**.\n",
        "\n",
        "---\n",
        "\n",
        "## 📊 **5. Confusion Matrix**\n",
        "- **Definition**: A table showing TP, FP, FN, TN counts.\n",
        "- **Gives insight into**: Types of classification errors.\n",
        "\n",
        "|                | Predicted Positive | Predicted Negative |\n",
        "|----------------|--------------------|--------------------|\n",
        "| **Actual Positive** | TP (True Positive)     | FN (False Negative)    |\n",
        "| **Actual Negative** | FP (False Positive)    | TN (True Negative)     |\n",
        "\n",
        "---\n",
        "\n",
        "## 📈 **6. ROC Curve (Receiver Operating Characteristic Curve)**\n",
        "- **Plot**: True Positive Rate (Recall) vs. False Positive Rate.\n",
        "- **Used For**: Visualizing the **trade-off** between sensitivity and specificity.\n",
        "\n",
        "---\n",
        "\n",
        "## 🌈 **7. AUC (Area Under the ROC Curve)**\n",
        "- **Definition**: Measures overall performance across all classification thresholds.\n",
        "- **Range**: 0.5 (no discrimination) to 1.0 (perfect classifier).\n",
        "- **Higher is better**.\n",
        "\n",
        "---\n",
        "\n",
        "## 📊 **8. Log Loss (Logarithmic Loss)**\n",
        "- **Used When**: Your model outputs **probabilities**.\n",
        "- **Penalizes**: Wrong confident predictions heavily.\n",
        "- **Formula**:\n",
        "  \\[\n",
        "  -\\frac{1}{n} \\sum_{i=1}^{n} [y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)]\n",
        "  \\]\n",
        "\n",
        "---\n",
        "\n",
        "## 🔄 **9. Matthews Correlation Coefficient (MCC)**\n",
        "- **Definition**: Takes into account TP, TN, FP, FN and is a good measure for imbalanced data.\n",
        "- **Range**: -1 to +1 (1 = perfect, 0 = random, -1 = total disagreement).\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 Summary Table\n",
        "\n",
        "| Metric     | Best For                            |\n",
        "|------------|-------------------------------------|\n",
        "| Accuracy   | Balanced datasets                   |\n",
        "| Precision  | Minimizing false positives          |\n",
        "| Recall     | Minimizing false negatives          |\n",
        "| F1 Score   | Trade-off between precision/recall  |\n",
        "| ROC-AUC    | Comparing classifiers               |\n",
        "| Log Loss   | Probability-based predictions       |\n",
        "| MCC        | Imbalanced data                     |\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "DaInm-BSSabP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. How does class imbalance affect Logistic Regression?"
      ],
      "metadata": {
        "id": "62JRqFLFSkPY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Class imbalance can **significantly impact the performance** of logistic regression and other classification models. Here's a breakdown of how and why it matters:\n",
        "\n",
        "---\n",
        "\n",
        "### ⚖️ What Is Class Imbalance?\n",
        "\n",
        "Class imbalance occurs when one class (e.g., \"positive\") appears much less frequently than the other (e.g., \"negative\") in the dataset.\n",
        "\n",
        "For example:  \n",
        "- **90% negative**, 10% positive  \n",
        "- Fraud detection, disease diagnosis, spam filtering — all common imbalanced problems\n",
        "\n",
        "---\n",
        "\n",
        "### 🚨 How It Affects Logistic Regression\n",
        "\n",
        "#### 1. **Bias Toward the Majority Class**\n",
        "- Logistic regression aims to **minimize overall error**, so it learns to favor the **majority class**.\n",
        "- It may **predict the majority class for almost all inputs** to maximize accuracy.\n",
        "- This leads to **high accuracy but poor recall** for the minority class.\n",
        "\n",
        "> Example: If 95% of your samples are class 0, the model could predict class 0 every time and still be 95% accurate — but useless for identifying class 1.\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. **Poor Decision Boundary**\n",
        "- With imbalance, the model’s decision boundary may **shift toward the minority class**, misclassifying it more often.\n",
        "- The probability threshold (usually 0.5) may not be optimal — you might need to **adjust it** to improve performance on the minority class.\n",
        "\n",
        "---\n",
        "\n",
        "#### 3. **Misleading Metrics**\n",
        "- **Accuracy** becomes misleading with imbalanced data.\n",
        "- You need to use metrics like:\n",
        "  - **Precision**\n",
        "  - **Recall**\n",
        "  - **F1-score**\n",
        "  - **ROC AUC**\n",
        "  - **Confusion matrix**\n",
        "\n",
        "---\n",
        "\n",
        "### 🧰 How to Handle Class Imbalance in Logistic Regression\n",
        "\n",
        "#### ✅ **1. Use Better Evaluation Metrics**\n",
        "- Focus on **precision**, **recall**, **F1-score**, and **AUC** rather than just accuracy.\n",
        "\n",
        "#### ⚖️ **2. Resample the Dataset**\n",
        "- **Oversampling** the minority class (e.g., SMOTE)\n",
        "- **Undersampling** the majority class\n",
        "- Combine both (hybrid methods)\n",
        "\n",
        "#### 🔧 **3. Adjust Class Weights**\n",
        "- In `scikit-learn`, use:\n",
        "  ```python\n",
        "  LogisticRegression(class_weight='balanced')\n",
        "  ```\n",
        "  This automatically assigns weights inversely proportional to class frequencies.\n",
        "\n",
        "#### 📉 **4. Threshold Tuning**\n",
        "- Adjust the **probability threshold** (from 0.5) to something that balances precision and recall better for the minority class.\n",
        "\n",
        "#### 🧠 **5. Use Ensemble Methods**\n",
        "- Combine logistic regression with **ensemble techniques** like Bagging or Boosting to help model rare cases more effectively.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ Quick Example (Scikit-Learn)\n",
        "```python\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression(class_weight='balanced')\n",
        "model.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 TL;DR\n",
        "- Class imbalance **hurts logistic regression** by making it biased toward the majority class.\n",
        "- Don't rely on accuracy.\n",
        "- Use **resampling**, **class weights**, and **better metrics** to address the issue.\n",
        "\n"
      ],
      "metadata": {
        "id": "_K775blSSk47"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What is Hyperparameter Tuning in Logistic Regression?\n"
      ],
      "metadata": {
        "id": "19rrCHJSSlWh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Awesome question! Let’s break it down. 🔧\n",
        "\n",
        "---\n",
        "\n",
        "## 🎛️ What is **Hyperparameter Tuning** in Logistic Regression?\n",
        "\n",
        "**Hyperparameter tuning** is the process of finding the **best set of parameters** for a machine learning model that aren’t learned from the data, but instead are **set before training**.\n",
        "\n",
        "In **logistic regression**, there are several such **hyperparameters** (like `C`, `penalty`, etc.) that you can tweak to improve performance.\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 Why Tune Hyperparameters?\n",
        "\n",
        "Because the default settings may not be optimal for your dataset.\n",
        "\n",
        "Tuning helps:\n",
        "- Boost model accuracy or AUC\n",
        "- Prevent overfitting or underfitting\n",
        "- Improve generalization\n",
        "\n",
        "---\n",
        "\n",
        "## ⚙️ Common Hyperparameters in Logistic Regression (Scikit-Learn)\n",
        "\n",
        "| Hyperparameter | Description | Effect |\n",
        "|----------------|-------------|--------|\n",
        "| `C` | Inverse of regularization strength | Lower values = stronger regularization |\n",
        "| `penalty` | Type of regularization (`'l1'`, `'l2'`, `'elasticnet'`, `'none'`) | Controls how complexity is penalized |\n",
        "| `solver` | Algorithm to use (`'liblinear'`, `'saga'`, `'lbfgs'`, etc.) | Needed for certain penalties |\n",
        "| `max_iter` | Max number of iterations for convergence | May need to increase for large data |\n",
        "| `class_weight` | Weight balance for classes | Helps with class imbalance |\n",
        "\n",
        "---\n",
        "\n",
        "## 🔍 Example: Hyperparameter Tuning Using Grid Search\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear']  # supports both l1 and l2\n",
        "}\n",
        "\n",
        "# Initialize model\n",
        "logreg = LogisticRegression()\n",
        "\n",
        "# Grid search with 5-fold cross-validation\n",
        "grid = GridSearchCV(logreg, param_grid, cv=5, scoring='accuracy')\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "print(\"Best Hyperparameters:\", grid.best_params_)\n",
        "print(\"Best Score:\", grid.best_score_)\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Fyvg34wUS6st"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🧪 Other Tuning Methods\n",
        "- **RandomizedSearchCV**: Tries random combinations (faster for big grids)\n",
        "- **Bayesian optimization**, **Optuna**, or **Hyperopt**: Smarter search strategies\n",
        "\n",
        "---\n",
        "\n",
        "## 🔑 Summary\n",
        "\n",
        "- **Hyperparameter tuning** in logistic regression helps you control **regularization**, **solver behavior**, and more.\n",
        "- It boosts performance and generalization.\n",
        "- You can use tools like **GridSearchCV** or **RandomizedSearchCV** to find the best settings"
      ],
      "metadata": {
        "id": "bxxX299hTOtK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. What are different solvers in Logistic Regression? Which one should be used?\n"
      ],
      "metadata": {
        "id": "GJ-0P51MTHj8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great question! The **solver** in logistic regression is the algorithm that optimizes the model's cost function to find the best coefficients. Different solvers have different strengths depending on the size of your data, sparsity, and type of regularization.\n",
        "\n",
        "Let’s break them down:\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 **Solvers in Logistic Regression (Scikit-learn)**\n",
        "\n",
        "### 1. **liblinear**\n",
        "- **Type**: Coordinate descent algorithm.\n",
        "- **Supports**: `L1` and `L2` penalties.\n",
        "- **Best For**:\n",
        "  - Small to medium datasets\n",
        "  - Sparse data\n",
        "- **Limitations**:\n",
        "  - Doesn’t support multinomial classification (`multi_class='multinomial'`)\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **lbfgs** *(Limited-memory BFGS)*\n",
        "- **Type**: Quasi-Newton method.\n",
        "- **Supports**: Only `L2` penalty.\n",
        "- **Best For**:\n",
        "  - Large datasets\n",
        "  - Multiclass classification (`multi_class='multinomial'`)\n",
        "- **Default solver in scikit-learn** (as of newer versions)\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **saga**\n",
        "- **Type**: Variant of `sag`, supports **sparse data** and **L1**, **L2**, and **elasticnet** regularization.\n",
        "- **Supports**:\n",
        "  - `L1`, `L2`, `elasticnet`\n",
        "  - Both binary and multinomial problems\n",
        "- **Best For**:\n",
        "  - Large datasets\n",
        "  - Sparse data\n",
        "  - Use with `elasticnet` penalty\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **newton-cg**\n",
        "- **Type**: Newton’s method with conjugate gradient.\n",
        "- **Supports**: Only `L2` penalty.\n",
        "- **Best For**:\n",
        "  - Multiclass problems\n",
        "  - Larger datasets\n",
        "- **Slower** than `lbfgs` in many cases.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **sag** *(Stochastic Average Gradient)*\n",
        "- **Type**: Variant of SGD.\n",
        "- **Supports**: Only `L2` penalty.\n",
        "- **Best For**:\n",
        "  - Very large datasets\n",
        "  - Converges faster for convex problems\n",
        "- **Requires**: Feature scaling for good performance.\n",
        "\n",
        "---\n",
        "\n",
        "## 🧩 Which Solver Should You Use?\n",
        "\n",
        "| Use Case                              | Recommended Solver |\n",
        "|---------------------------------------|--------------------|\n",
        "| Small dataset + binary classification | `liblinear`        |\n",
        "| Large dataset                         | `lbfgs` or `saga`  |\n",
        "| Multiclass classification             | `lbfgs`, `saga`, or `newton-cg` |\n",
        "| L1 regularization                     | `liblinear` or `saga` |\n",
        "| ElasticNet regularization             | `saga`             |\n",
        "| Sparse data                           | `liblinear` or `saga` |\n",
        "\n",
        "---\n",
        "\n",
        "## 🛠️ Example:\n",
        "```python\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression(\n",
        "    solver='saga',        # or 'lbfgs', 'liblinear', etc.\n",
        "    penalty='elasticnet',\n",
        "    l1_ratio=0.5,         # Only used with 'elasticnet'\n",
        "    C=1.0,\n",
        "    max_iter=1000\n",
        ")\n",
        "```\n"
      ],
      "metadata": {
        "id": "UfxcS6fzS6o3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. How is Logistic Regression extended for multiciass classification?"
      ],
      "metadata": {
        "id": "tMVTvmkMS6lH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great question! While **Logistic Regression** is inherently a **binary classifier**, it can be extended to handle **multiclass classification** problems using a few clever strategies.\n",
        "\n",
        "Let’s break it down 👇\n",
        "\n",
        "---\n",
        "\n",
        "## 🎓 **How Logistic Regression Handles Multiclass Classification**\n",
        "\n",
        "### ✅ 1. **One-vs-Rest (OvR)** – aka **One-vs-All**\n",
        "- **Idea**: Train one classifier per class.\n",
        "  - Each classifier predicts: \"Is this class X **vs** all others?\"\n",
        "- For `k` classes → Train `k` binary classifiers.\n",
        "- **During prediction**:\n",
        "  - Compute the **probability** for each classifier\n",
        "  - Choose the class with the **highest probability**\n",
        "\n",
        "> **Scikit-learn default**: `multi_class='ovr'`\n",
        "\n",
        "### ✅ 2. **Multinomial (Softmax) Logistic Regression**\n",
        "- Uses the **softmax** function to compute probabilities across **all classes** simultaneously.\n",
        "- Trains one model to directly predict **all class probabilities** in one go.\n",
        "- Optimizes a **cross-entropy loss** across all classes.\n",
        "  \n",
        "> Better choice when classes are **not independent**, or when **probability calibration** matters.\n",
        "\n",
        "> Use with `multi_class='multinomial'` and solvers like `'lbfgs'`, `'newton-cg'`, or `'saga'`.\n",
        "\n",
        "---\n",
        "\n",
        "## 🧮 Softmax Function\n",
        "\n",
        "Given \\( k \\) classes, and logits \\( z_1, z_2, \\ldots, z_k \\):\n",
        "\n",
        "\\[\n",
        "P(y = j \\mid x) = \\frac{e^{z_j}}{\\sum_{i=1}^{k} e^{z_i}} \\quad \\text{for } j = 1, 2, \\ldots, k\n",
        "\\]\n",
        "\n",
        "Where \\( z_j = \\theta_j^T x \\)\n",
        "\n",
        "---\n",
        "\n",
        "## ⚙️ Example in Scikit-learn\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# For multinomial (softmax) classification\n",
        "model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# For OvR (default)\n",
        "model_ovr = LogisticRegression(multi_class='ovr', solver='liblinear')\n",
        "model_ovr.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 When to Use Which?\n",
        "\n",
        "| Scenario                         | Strategy         |\n",
        "|----------------------------------|------------------|\n",
        "| Independent classes              | One-vs-Rest (OvR) |\n",
        "| Mutually exclusive classes       | Multinomial      |\n",
        "| Small dataset                    | OvR (simpler)    |\n",
        "| Probabilistic prediction matters | Multinomial      |\n",
        "| Using `liblinear` solver         | OvR only         |\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ TL;DR\n",
        "- Logistic regression can handle multiclass via **One-vs-Rest** or **Multinomial (Softmax)**.\n",
        "- **Multinomial + softmax** is more elegant and often better.\n",
        "- Scikit-learn supports both using the `multi_class` parameter."
      ],
      "metadata": {
        "id": "jYqcpmsrS6ev"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What are the advantages and disadvantages of Logistic Regression?"
      ],
      "metadata": {
        "id": "nPraQSKLS6bv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great question! Let’s break down the **advantages** and **disadvantages** of **Logistic Regression** so you can decide when to use it and when to go with something else.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ Advantages of Logistic Regression\n",
        "\n",
        "### 1. **Simple and Interpretable**\n",
        "- Easy to implement, understand, and interpret.\n",
        "- You can clearly see the **influence of each feature** through the coefficients.\n",
        "\n",
        "### 2. **Efficient for Binary Classification**\n",
        "- Performs well when the relationship between features and target is **linear**.\n",
        "- Computationally efficient, especially for **small to medium datasets**.\n",
        "\n",
        "### 3. **Outputs Probabilities**\n",
        "- Predicts **class probabilities**, not just labels.\n",
        "- Useful for **threshold tuning**, **decision-making**, or **ranking**.\n",
        "\n",
        "### 4. **Works Well with Linearly Separable Data**\n",
        "- If the classes are separable with a straight line (or plane), it often performs very well.\n",
        "\n",
        "### 5. **Regularization Support**\n",
        "- Easily integrates **L1 (Lasso)** and **L2 (Ridge)** regularization to avoid overfitting.\n",
        "\n",
        "### 6. **Fast Training**\n",
        "- Much faster to train than more complex models like SVMs or neural networks.\n",
        "\n",
        "---\n",
        "\n",
        "## ❌ Disadvantages of Logistic Regression\n",
        "\n",
        "### 1. **Assumes Linear Relationship**\n",
        "- Assumes a **linear relationship** between independent variables and the **log-odds** of the outcome.\n",
        "- Can perform poorly when the true relationship is **non-linear**.\n",
        "\n",
        "### 2. **Sensitive to Outliers**\n",
        "- Outliers can influence the decision boundary significantly unless handled properly.\n",
        "\n",
        "### 3. **Can Struggle with Complex Patterns**\n",
        "- Can’t capture interactions and non-linear dependencies well without feature engineering or transformations.\n",
        "\n",
        "### 4. **Requires Independent Features**\n",
        "- Assumes little or no **multicollinearity** among the input features.\n",
        "- If features are highly correlated, performance and interpretability can degrade.\n",
        "\n",
        "### 5. **Doesn't Work Well on Small/Unbalanced Datasets Without Tuning**\n",
        "- On imbalanced data, it tends to favor the majority class unless you handle it (e.g., class weights, resampling).\n",
        "\n",
        "---\n",
        "\n",
        "## 🔍 Quick Summary Table\n",
        "\n",
        "| Feature | Logistic Regression |\n",
        "|--------|----------------------|\n",
        "| Type | Linear, Probabilistic |\n",
        "| Use Case | Binary & multiclass classification |\n",
        "| Strengths | Simple, interpretable, fast |\n",
        "| Weaknesses | Linear assumption, outlier sensitive |\n",
        "| Handles Non-linearity? | ❌ (not directly) |\n",
        "| Regularization Support | ✅ L1, L2, ElasticNet |\n",
        "| Probability Output | ✅ Yes |\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 TL;DR:\n",
        "**Logistic regression** is a solid baseline model:  \n",
        "- Great for **interpretable**, fast models.  \n",
        "- Struggles with **non-linear** or complex data patterns.  \n",
        "- Easy to regularize, tune, and apply."
      ],
      "metadata": {
        "id": "2Vo16wiCS6Y5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. What are some use cases of Logistic Regression?"
      ],
      "metadata": {
        "id": "K5_x3JtrS6RQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great question! **Logistic Regression** is one of the most widely used algorithms in classification problems. It’s simple, interpretable, and works well when the relationship between the features and the target is linear in the log-odds.\n",
        "\n",
        "Here are some real-world **use cases** where Logistic Regression shines:\n",
        "\n",
        "---\n",
        "\n",
        "## 🔐 1. **Binary Classification Tasks**\n",
        "### ✅ Examples:\n",
        "- **Spam detection**: Is an email spam or not?\n",
        "- **Credit scoring**: Will a customer **default** on a loan? (yes/no)\n",
        "- **Disease prediction**: Does a patient have **diabetes** or not?\n",
        "- **Customer churn**: Will a customer leave a service?\n",
        "- **Fraud detection**: Is this transaction fraudulent?\n",
        "\n",
        "---\n",
        "\n",
        "## 📊 2. **Marketing & Business Analytics**\n",
        "### ✅ Examples:\n",
        "- **Click-through prediction**: Will a user click on an ad?\n",
        "- **Purchase likelihood**: Will a customer buy a product?\n",
        "- **Lead conversion**: Will a potential customer convert?\n",
        "- **Customer segmentation**: Classify customers based on behavior.\n",
        "\n",
        "---\n",
        "\n",
        "## 🏥 3. **Healthcare & Medical Diagnosis**\n",
        "### ✅ Examples:\n",
        "- **Disease classification**: Heart disease, cancer detection, etc.\n",
        "- **Readmission prediction**: Will a patient be readmitted?\n",
        "- **Mortality risk prediction**: Estimate the probability of survival.\n",
        "\n",
        "---\n",
        "\n",
        "## 🧪 4. **Social Sciences and Surveys**\n",
        "### ✅ Examples:\n",
        "- **Voting prediction**: Will someone vote for candidate A or B?\n",
        "- **Satisfaction prediction**: Will a customer rate a service as “satisfied” or “unsatisfied”?\n",
        "- **Attitude classification**: Is a response positive or negative?\n",
        "\n",
        "---\n",
        "\n",
        "## 🚀 5. **Operations and Risk Management**\n",
        "### ✅ Examples:\n",
        "- **Risk modeling**: Predicting the probability of system failure.\n",
        "- **Loan risk evaluation**: Estimating risk of default or fraud.\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 6. **Multiclass Classification (Extended)**\n",
        "Using **softmax** logistic regression:\n",
        "- **Handwritten digit recognition** (e.g., MNIST)\n",
        "- **Iris flower classification**\n",
        "- **Document classification** into multiple topics\n",
        "\n",
        "---\n",
        "\n",
        "## 🧩 Why Is Logistic Regression So Popular?\n",
        "- Easy to implement and interpret\n",
        "- Outputs probabilities\n",
        "- Regularization support\n",
        "- Serves as a great **baseline model**\n",
        "\n",
        "---\n",
        "\n",
        "## ⚡ TL;DR:\n",
        "Logistic Regression is used when:\n",
        "- You want a **yes/no** decision\n",
        "- You need **interpretable** coefficients\n",
        "- You care about **probabilities** rather than just labels\n",
        "\n",
        "It’s a go-to model in finance, healthcare, marketing, and many other fields.\n",
        "\n"
      ],
      "metadata": {
        "id": "MHOLlDVwS6Ny"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What is the difference between Softmax Regression and Logistic Regression?\n"
      ],
      "metadata": {
        "id": "iR-B4dy-mMdY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The key difference between **Softmax Regression** and **Logistic Regression** lies in the **number of classes** they are designed to handle:\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 **Logistic Regression**\n",
        "- **Use case**: **Binary classification** (two classes)\n",
        "- **Output**: A **single probability** (between 0 and 1) that the input belongs to the **positive class**\n",
        "- **Function**: Uses the **sigmoid function** to map predictions to probabilities:\n",
        "  \\[\n",
        "  P(y=1|x) = \\frac{1}{1 + e^{-(w^Tx + b)}}\n",
        "  \\]\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 **Softmax Regression (Multinomial Logistic Regression)**\n",
        "- **Use case**: **Multiclass classification** (more than two classes)\n",
        "- **Output**: A **probability distribution** across all possible classes\n",
        "- **Function**: Uses the **softmax function**:\n",
        "  \\[\n",
        "  P(y=i|x) = \\frac{e^{w_i^T x}}{\\sum_{j=1}^K e^{w_j^T x}} \\quad \\text{for } i = 1, 2, ..., K\n",
        "  \\]\n",
        "  Where \\(K\\) is the number of classes, and each class has its own set of weights.\n",
        "\n",
        "---\n",
        "\n",
        "### Summary Table:\n",
        "\n",
        "| Feature               | Logistic Regression         | Softmax Regression                  |\n",
        "|----------------------|-----------------------------|-------------------------------------|\n",
        "| Problem type          | Binary classification        | Multiclass classification           |\n",
        "| Output                | Single probability           | Vector of probabilities             |\n",
        "| Activation function   | Sigmoid                      | Softmax                             |\n",
        "| Number of classes     | 2                            | 3 or more                           |\n",
        "\n"
      ],
      "metadata": {
        "id": "bfig1maVmkNb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?\n"
      ],
      "metadata": {
        "id": "qk6T3RGomzTO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great question! Choosing between **One-vs-Rest (OvR)** and **Softmax (Multinomial Logistic Regression)** for multiclass classification depends on your **model requirements**, **data characteristics**, and sometimes **computational efficiency**.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 **One-vs-Rest (OvR)**\n",
        "**What is it?**  \n",
        "OvR trains **one binary classifier per class**, treating each class as the positive class and all others as negative.\n",
        "\n",
        "#### ✅ Pros:\n",
        "- **Simple and interpretable**\n",
        "- Can be used with any **binary classifier**, not just logistic regression\n",
        "- Easier to parallelize training (each classifier is independent)\n",
        "\n",
        "#### ❌ Cons:\n",
        "- Doesn’t model **inter-class relationships** directly\n",
        "- Can give **inconsistent probability outputs** (e.g., more than one class predicted as “positive”)\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 **Softmax (Multinomial Logistic Regression)**\n",
        "**What is it?**  \n",
        "Trains a **single model** that directly outputs a **probability distribution over all classes**, using the softmax function.\n",
        "\n",
        "#### ✅ Pros:\n",
        "- Models **all classes together**, accounting for mutual exclusivity\n",
        "- Produces **coherent probabilities** (sums to 1)\n",
        "- Often more **accurate** in practice for truly multiclass problems\n",
        "\n",
        "#### ❌ Cons:\n",
        "- More complex to implement from scratch\n",
        "- Training can be **slower** for a large number of classes (though not always)\n",
        "\n",
        "---\n",
        "\n",
        "### ⚖️ **When to Choose What**\n",
        "\n",
        "| Situation                             | Recommended Approach        |\n",
        "|--------------------------------------|-----------------------------|\n",
        "| Few classes, fast/simple model needed | **OvR**                     |\n",
        "| Many classes with mutual exclusivity | **Softmax**                 |\n",
        "| Want consistent, probabilistic output | **Softmax**                 |\n",
        "| Using a non-logistic binary classifier (e.g., SVM) | **OvR**        |\n",
        "| Computational constraints, parallelism | **OvR** (can train in parallel) |\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 TL;DR:\n",
        "- Use **Softmax** if you want a single, clean model with proper probability outputs for **mutually exclusive classes**.\n",
        "- Use **OvR** if you’re working with **binary classifiers**, want more flexibility, or need a quick-and-dirty solution.\n",
        "\n"
      ],
      "metadata": {
        "id": "S78tBfeIm5dA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. How do we interpret coefficients in Logistic Regression?\n"
      ],
      "metadata": {
        "id": "VeviMeqmouXl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sure! Here's a clear and concise breakdown of how to **interpret coefficients in Logistic Regression**:\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 Logistic Regression Equation:\n",
        "\\[\n",
        "\\log\\left(\\frac{P(y=1|x)}{1 - P(y=1|x)}\\right) = w_0 + w_1x_1 + w_2x_2 + \\dots + w_nx_n\n",
        "\\]\n",
        "\n",
        "- The left-hand side is the **log-odds** (also called the logit) of the probability of the positive class.\n",
        "- Each \\( w_i \\) is a **coefficient** for the feature \\( x_i \\).\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 How to Interpret a Coefficient \\( w_i \\):\n",
        "\n",
        "#### 1. **In terms of log-odds**:\n",
        "- \\( w_i \\) tells you how much the **log-odds** of the outcome increases (or decreases) when \\( x_i \\) increases by one unit, **holding all other variables constant**.\n",
        "\n",
        "#### 2. **In terms of odds ratio**:\n",
        "- Use \\( e^{w_i} \\) to convert the coefficient to an **odds ratio**.\n",
        "- This tells you how the **odds** of the positive class **multiply** when \\( x_i \\) increases by one unit.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ Examples:\n",
        "\n",
        "| Coefficient \\( w_i \\) | \\( e^{w_i} \\) | Interpretation |\n",
        "|------------------------|----------------|----------------|\n",
        "| \\( w_i = 0.7 \\)        | 2.01           | A 1-unit increase in \\( x_i \\) **doubles the odds** of positive class |\n",
        "| \\( w_i = -0.5 \\)       | 0.61           | A 1-unit increase in \\( x_i \\) **reduces the odds** by 39% |\n",
        "| \\( w_i = 0 \\)          | 1              | No effect on the odds |\n",
        "\n",
        "---\n",
        "\n",
        "### ⚠️ Notes:\n",
        "- Logistic regression works with **odds**, not direct probabilities.\n",
        "- Interpretation depends on how your features are scaled (e.g., standardization changes the meaning).\n",
        "- If you're working with **categorical features**, interpret relative to the **reference category**.\n",
        "\n"
      ],
      "metadata": {
        "id": "h1_Z-geNovIP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "                                                      ***Practical Questions***"
      ],
      "metadata": {
        "id": "Wq7sY8kzyfLF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic Regression, and prints the model accuracy.\n"
      ],
      "metadata": {
        "id": "ZeGX_rBuytjM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "data = load_iris()\n",
        "X = data.data       # Features\n",
        "y = data.target     # Target labels\n",
        "\n",
        "# Step 2: Split dataset into training and testing (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Create and train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)  # Increase max_iter if needed\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 5: Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHdjHga3zG_k",
        "outputId": "075cba53-0334-4f88-834a-1de59b70fdf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression (penalty='11') and print the model accuracy.\n"
      ],
      "metadata": {
        "id": "ml98ItXgzQaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Create Logistic Regression model with L1 regularization\n",
        "model = LogisticRegression(penalty='l1', solver='saga', max_iter=500, multi_class='multinomial')\n",
        "\n",
        "# Step 4: Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"L1-Regularized Logistic Regression Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_opD4P35zZin",
        "outputId": "3af9cda9-bb41-4b37-ee12-122f8d07fe8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L1-Regularized Logistic Regression Accuracy: 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Write a Python program to train Logistic Regression with L2 regularization (Ridge) using LogisticRegression (penalty='12'). Print model accuracy and coefficients.\n"
      ],
      "metadata": {
        "id": "IQSf0GceyvZp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train Logistic Regression with L2 regularization\n",
        "model = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=200, multi_class='multinomial')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predictions and accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 5: Output\n",
        "print(f\"L2-Regularized Logistic Regression Accuracy: {accuracy:.2f}\")\n",
        "print(\"\\nModel Coefficients (per class):\")\n",
        "print(np.round(model.coef_, 3))  # Rounded for readability\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7IzOgPjzkQ0",
        "outputId": "1ffdcdf7-b0f4-4c15-d425-1b1765c681ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L2-Regularized Logistic Regression Accuracy: 1.00\n",
            "\n",
            "Model Coefficients (per class):\n",
            "[[-0.393  0.963 -2.375 -0.999]\n",
            " [ 0.508 -0.255 -0.213 -0.776]\n",
            " [-0.115 -0.708  2.588  1.775]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet').\n"
      ],
      "metadata": {
        "id": "F3UFRe7GyvO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train Logistic Regression with Elastic Net regularization\n",
        "model = LogisticRegression(\n",
        "    penalty='elasticnet',\n",
        "    solver='saga',\n",
        "    l1_ratio=0.5,           # 0.5 = equal L1 and L2\n",
        "    max_iter=500,\n",
        "    multi_class='multinomial'\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 5: Print results\n",
        "print(f\"Elastic Net Logistic Regression Accuracy: {accuracy:.2f}\")\n",
        "print(\"\\nModel Coefficients (per class):\")\n",
        "print(np.round(model.coef_, 3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQ5siva00Tkq",
        "outputId": "b5c447d5-1289-4da0-beb3-c343e1c5eb10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elastic Net Logistic Regression Accuracy: 1.00\n",
            "\n",
            "Model Coefficients (per class):\n",
            "[[ 0.256  1.735 -2.431 -0.619]\n",
            " [ 0.     0.     0.    -0.508]\n",
            " [-1.013 -1.212  2.646  2.108]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr'.\n"
      ],
      "metadata": {
        "id": "9aCRF6wmz8DX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train Logistic Regression with OvR\n",
        "model = LogisticRegression(multi_class='ovr', solver='lbfgs', max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 5: Print results\n",
        "print(f\"Logistic Regression (OvR) Accuracy: {accuracy:.2f}\")\n",
        "print(\"\\nModel Coefficients (per class):\")\n",
        "print(np.round(model.coef_, 3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3XeLYTF0nIF",
        "outputId": "0612e028-b28a-4360-a399-d901730ebbf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression (OvR) Accuracy: 0.97\n",
            "\n",
            "Model Coefficients (per class):\n",
            "[[-0.428  0.888 -2.215 -0.916]\n",
            " [-0.034 -2.044  0.543 -1.018]\n",
            " [-0.389 -0.621  2.776  2.091]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic Regression. Print the best parameters and accuracy.\n",
        "\n"
      ],
      "metadata": {
        "id": "R19rfXebyuVk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Define the model\n",
        "model = LogisticRegression(solver='saga', max_iter=500, multi_class='multinomial')\n",
        "\n",
        "# Step 4: Define the parameter grid\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10],\n",
        "    'penalty': ['l1', 'l2']  # 'saga' solver supports both L1 and L2\n",
        "}\n",
        "\n",
        "# Step 5: Perform grid search\n",
        "grid = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Make predictions and calculate accuracy\n",
        "best_model = grid.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 7: Print results\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(f\"Test Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3trKd7Yb2XuQ",
        "outputId": "a40d0b88-e63a-4bac-d086-55bf137cbef5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 1, 'penalty': 'l1'}\n",
            "Test Accuracy: 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the average accuracy."
      ],
      "metadata": {
        "id": "XOR3UPjd3QZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Define Stratified K-Fold\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Step 3: Define the model\n",
        "model = LogisticRegression(max_iter=200, solver='lbfgs', multi_class='multinomial')\n",
        "\n",
        "# Step 4: Perform cross-validation\n",
        "accuracies = []\n",
        "\n",
        "for fold, (train_index, test_index) in enumerate(skf.split(X, y), 1):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(acc)\n",
        "    print(f\"Fold {fold} Accuracy: {acc:.2f}\")\n",
        "\n",
        "# Step 5: Print average accuracy\n",
        "print(f\"\\nAverage Accuracy: {np.mean(accuracies):.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySWTaQ2V2Y7t",
        "outputId": "7bb0a3da-056d-4613-db55-496e0b435922"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 Accuracy: 1.00\n",
            "Fold 2 Accuracy: 0.97\n",
            "Fold 3 Accuracy: 0.93\n",
            "Fold 4 Accuracy: 1.00\n",
            "Fold 5 Accuracy: 0.93\n",
            "\n",
            "Average Accuracy: 0.97\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "8. Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy."
      ],
      "metadata": {
        "id": "7xx4QdlO2ZWC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load dataset from CSV file\n",
        "# Make sure to replace 'your_file.csv' with the actual filename\n",
        "data = pd.read_csv('target.csv')\n",
        "\n",
        "# Step 2: Separate features and target\n",
        "# Assume the target column is named 'target'; change it if needed\n",
        "X = data.drop('target', axis=1)\n",
        "y = data['target']\n",
        "\n",
        "# Step 3: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 4: Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 6: Print accuracy\n",
        "print(f\"Logistic Regression Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "jaysax1t353F",
        "outputId": "6b6fc645-bd9f-4ce1-f050-398db95eff1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'target.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-2f3997afb946>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Step 1: Load dataset from CSV file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Make sure to replace 'your_file.csv' with the actual filename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'target.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Step 2: Separate features and target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'target.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Write a Python program to apply Randomized SearchCV for tuning hyperparameters (C, penalty, solver) in Logistic Regression. Print the best parameters and accuracy.\n"
      ],
      "metadata": {
        "id": "GIXlxYeFxKlF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Hyperparameter space\n",
        "param_dist = {\n",
        "    'C': np.logspace(-4, 4, 20),\n",
        "    'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
        "    'solver': ['lbfgs', 'liblinear', 'saga', 'newton-cg']\n",
        "}\n",
        "\n",
        "# RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    model,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=20,\n",
        "    scoring='accuracy',\n",
        "    cv=5,\n",
        "    verbose=1,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = random_search.best_estimator_.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print best parameters and accuracy\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "print(\"Best Cross-Validated Accuracy: {:.2f}\".format(random_search.best_score_))\n",
        "print(\"Test Set Accuracy: {:.2f}\".format(accuracy))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6lxPJY3xKE0",
        "outputId": "e5a186bd-5130-4e1d-d6ff-8126a93107e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
            "Best Parameters: {'solver': 'newton-cg', 'penalty': 'l2', 'C': np.float64(1.623776739188721)}\n",
            "Best Cross-Validated Accuracy: 0.96\n",
            "Test Set Accuracy: 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
            "70 fits failed out of a total of 100.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "10 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 71, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "25 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1382, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 436, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'l1', 'elasticnet', 'l2'} or None. Got 'none' instead.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "5 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1382, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 436, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'elasticnet', 'l2', 'l1'} or None. Got 'none' instead.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "15 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1203, in fit\n",
            "    raise ValueError(\"l1_ratio must be specified when penalty is elasticnet.\")\n",
            "ValueError: l1_ratio must be specified when penalty is elasticnet.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "10 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 63, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "5 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 63, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or None penalties, got elasticnet penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [0.95833333 0.95833333        nan        nan        nan        nan\n",
            "        nan        nan 0.95              nan        nan        nan\n",
            " 0.33333333        nan        nan 0.93333333        nan        nan\n",
            " 0.85833333        nan]\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy.\n"
      ],
      "metadata": {
        "id": "mrdLyXL0x56x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create Logistic Regression model\n",
        "base_model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Wrap with One-vs-One strategy\n",
        "ovo_model = OneVsOneClassifier(base_model)\n",
        "\n",
        "# Train model\n",
        "ovo_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = ovo_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print accuracy\n",
        "print(\"One-vs-One Logistic Regression Accuracy: {:.2f}\".format(accuracy))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4eHgmx2x6ZD",
        "outputId": "8bfa749c-46f0-4011-d9b2-77e357b2cc18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One-vs-One Logistic Regression Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary classification.\n"
      ],
      "metadata": {
        "id": "NsUeRIeIx65H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5,\n",
        "                           n_redundant=2, n_classes=2, random_state=42)\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create and train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize confusion matrix\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Predicted 0', 'Predicted 1'],\n",
        "            yticklabels=['Actual 0', 'Actual 1'])\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Prediction')\n",
        "plt.ylabel('Actual')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "9eabQFeMx7iJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "outputId": "a616f640-138d-4510-f63a-5886d69b7c34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. Write a Python program to train a Logistic Regression model and evaluate its performance using Precision, Recall, and Fl-Score.\n"
      ],
      "metadata": {
        "id": "tk_z-C_sO8hi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Step 1: Generate synthetic binary classification data\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5,\n",
        "                           n_redundant=2, n_classes=2, random_state=42)\n",
        "\n",
        "# Step 2: Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 5: Evaluate performance\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Step 6: Print the evaluation metrics\n",
        "print(\"Precision:\", round(precision, 4))\n",
        "print(\"Recall:\", round(recall, 4))\n",
        "print(\"F1-Score:\", round(f1, 4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6SLU7cdO89Q",
        "outputId": "e79a75c7-f58d-434e-bfb6-c951683473ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.8296\n",
            "Recall: 0.7887\n",
            "F1-Score: 0.8087\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to improve model performance.\n"
      ],
      "metadata": {
        "id": "N-mUpPkCO9Xp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Generate imbalanced binary classification data\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5,\n",
        "                           n_redundant=2, weights=[0.9, 0.1], flip_y=0, random_state=42)\n",
        "\n",
        "# Step 2: Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Train Logistic Regression with class weights\n",
        "model = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 5: Evaluate model\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Step 6: Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='YlGnBu',\n",
        "            xticklabels=['Predicted 0', 'Predicted 1'],\n",
        "            yticklabels=['Actual 0', 'Actual 1'])\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Prediction')\n",
        "plt.ylabel('Actual')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "id": "RkPu_DLZO9vJ",
        "outputId": "05d5d73d-93c9-47f1-e643-52f34423ede3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.80      0.88       274\n",
            "           1       0.26      0.73      0.38        26\n",
            "\n",
            "    accuracy                           0.80       300\n",
            "   macro avg       0.61      0.77      0.63       300\n",
            "weighted avg       0.91      0.80      0.84       300\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and evaluate performance.\n"
      ],
      "metadata": {
        "id": "t5kTBHcGPNmQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Step 1: Load Titanic dataset\n",
        "df = sns.load_dataset('titanic')\n",
        "\n",
        "# Step 2: Preprocess data\n",
        "# Drop rows with missing 'embarked', 'age', or 'fare'\n",
        "df = df.dropna(subset=['embarked', 'age', 'fare'])\n",
        "\n",
        "# Select relevant features and encode categorical variables\n",
        "df['sex'] = df['sex'].map({'male': 0, 'female': 1})\n",
        "df['embarked'] = df['embarked'].map({'S': 0, 'C': 1, 'Q': 2})\n",
        "\n",
        "# Fill any remaining missing values\n",
        "df = df[['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']].dropna()\n",
        "\n",
        "# Step 3: Define features and target\n",
        "X = df.drop('survived', axis=1)\n",
        "y = df['survived']\n",
        "\n",
        "# Step 4: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 5: Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Step 7: Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='coolwarm',\n",
        "            xticklabels=['Predicted 0', 'Predicted 1'],\n",
        "            yticklabels=['Actual 0', 'Actual 1'])\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Prediction')\n",
        "plt.ylabel('Actual')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "id": "9bfxvUtvPOAD",
        "outputId": "e1ea947d-b744-4bf9-c90b-c5f82c6488bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.86      0.82       122\n",
            "           1       0.79      0.70      0.74        92\n",
            "\n",
            "    accuracy                           0.79       214\n",
            "   macro avg       0.79      0.78      0.78       214\n",
            "weighted avg       0.79      0.79      0.79       214\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression model. Evaluate its accuracy and compare results with and without scaling.\n"
      ],
      "metadata": {
        "id": "lq0BcaFLPOYT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Titanic dataset\n",
        "df = sns.load_dataset('titanic')\n",
        "\n",
        "# Preprocess data\n",
        "df = df.dropna(subset=['age', 'fare', 'embarked'])\n",
        "\n",
        "# Encode categorical features\n",
        "df['sex'] = df['sex'].map({'male': 0, 'female': 1})\n",
        "df['embarked'] = df['embarked'].map({'S': 0, 'C': 1, 'Q': 2})\n",
        "\n",
        "# Use selected features\n",
        "features = ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']\n",
        "X = df[features]\n",
        "y = df['survived']\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# ----------- Without Scaling -----------\n",
        "model_plain = LogisticRegression(max_iter=1000)\n",
        "model_plain.fit(X_train, y_train)\n",
        "y_pred_plain = model_plain.predict(X_test)\n",
        "acc_plain = accuracy_score(y_test, y_pred_plain)\n",
        "\n",
        "# ----------- With Standardization -----------\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_scaled = LogisticRegression(max_iter=1000)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "acc_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# ----------- Compare Results -----------\n",
        "print(\"Accuracy WITHOUT Scaling:\", round(acc_plain, 4))\n",
        "print(\"Accuracy WITH Scaling:   \", round(acc_scaled, 4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNwYgSs6POvE",
        "outputId": "6127edeb-6d7b-4504-8d36-0b61d7eb8b3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy WITHOUT Scaling: 0.7897\n",
            "Accuracy WITH Scaling:    0.7897\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XremREPHPlxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Load and prepare Titanic dataset\n",
        "df = sns.load_dataset('titanic')\n",
        "df = df.dropna(subset=['age', 'fare', 'embarked'])\n",
        "\n",
        "# Encode categorical features\n",
        "df['sex'] = df['sex'].map({'male': 0, 'female': 1})\n",
        "df['embarked'] = df['embarked'].map({'S': 0, 'C': 1, 'Q': 2})\n",
        "\n",
        "# Select features and target\n",
        "features = ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']\n",
        "X = df[features]\n",
        "y = df['survived']\n",
        "\n",
        "# Step 2: Train/Test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict probabilities and calculate ROC-AUC\n",
        "y_probs = model.predict_proba(X_test)[:, 1]\n",
        "roc_auc = roc_auc_score(y_test, y_probs)\n",
        "\n",
        "print(\"ROC-AUC Score:\", round(roc_auc, 4))\n",
        "\n",
        "# Step 5: Plot ROC Curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_probs)\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(fpr, tpr, label=f'Logistic Regression (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "mAimCX8_PmK2",
        "outputId": "83d963a9-917e-4e7e-92f3-79b59826609e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.8111\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "17. Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate accuracy.\n"
      ],
      "metadata": {
        "id": "oMA0jKnuPmjq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load and clean Titanic dataset\n",
        "df = sns.load_dataset('titanic')\n",
        "df = df.dropna(subset=['age', 'fare', 'embarked'])\n",
        "\n",
        "# Encode categorical variables\n",
        "df['sex'] = df['sex'].map({'male': 0, 'female': 1})\n",
        "df['embarked'] = df['embarked'].map({'S': 0, 'C': 1, 'Q': 2})\n",
        "\n",
        "# Step 2: Feature selection\n",
        "features = ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']\n",
        "X = df[features]\n",
        "y = df['survived']\n",
        "\n",
        "# Step 3: Train/Test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 4: Train Logistic Regression with C=0.5\n",
        "model = LogisticRegression(C=0.5, max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy with C=0.5:\", round(accuracy, 4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzR5q1sVPnB4",
        "outputId": "7a798e4e-9d48-4195-fe33-b8cf7993742c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with C=0.5: 0.7944\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. Write a Python program to train Logistic Regression and identify important features based on model coefficients.\n"
      ],
      "metadata": {
        "id": "PhZSFLr9PnX3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Step 1: Load and preprocess Titanic dataset\n",
        "df = sns.load_dataset('titanic')\n",
        "df = df.dropna(subset=['age', 'fare', 'embarked'])\n",
        "\n",
        "# Encode categorical features\n",
        "df['sex'] = df['sex'].map({'male': 0, 'female': 1})\n",
        "df['embarked'] = df['embarked'].map({'S': 0, 'C': 1, 'Q': 2})\n",
        "\n",
        "# Step 2: Select features and target\n",
        "features = ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']\n",
        "X = df[features]\n",
        "y = df['survived']\n",
        "\n",
        "# Step 3: Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 4: Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Extract and display feature importances\n",
        "coefficients = model.coef_[0]\n",
        "feature_importance = pd.Series(coefficients, index=features).sort_values()\n",
        "\n",
        "print(\"Feature Importances (from coefficients):\")\n",
        "print(feature_importance)\n",
        "\n",
        "# Step 6: Visualize\n",
        "plt.figure(figsize=(8, 5))\n",
        "feature_importance.plot(kind='barh', color='skyblue')\n",
        "plt.title('Feature Importance (Logistic Regression Coefficients)')\n",
        "plt.xlabel('Coefficient Value')\n",
        "plt.ylabel('Feature')\n",
        "plt.axvline(0, color='gray', linestyle='--')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "id": "3GPtzDcFPnw3",
        "outputId": "3895eb3d-5e38-401f-e208-727685f8be23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances (from coefficients):\n",
            "pclass     -1.451062\n",
            "sibsp      -0.280636\n",
            "age        -0.039263\n",
            "fare        0.000036\n",
            "parch       0.032745\n",
            "embarked    0.156335\n",
            "sex         2.618901\n",
            "dtype: float64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. Write a Python program to train Logistic Regression and evaluate its performance using Cohen's Kappa Score.\n"
      ],
      "metadata": {
        "id": "FunX1Wh1QKxM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "# Step 1: Load and preprocess Titanic dataset\n",
        "df = sns.load_dataset('titanic')\n",
        "df = df.dropna(subset=['age', 'fare', 'embarked'])\n",
        "\n",
        "# Encode categorical variables\n",
        "df['sex'] = df['sex'].map({'male': 0, 'female': 1})\n",
        "df['embarked'] = df['embarked'].map({'S': 0, 'C': 1, 'Q': 2})\n",
        "\n",
        "# Step 2: Select features and target\n",
        "features = ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']\n",
        "X = df[features]\n",
        "y = df['survived']\n",
        "\n",
        "# Step 3: Train/Test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 4: Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Predict and evaluate using Cohen’s Kappa Score\n",
        "y_pred = model.predict(X_test)\n",
        "kappa_score = cohen_kappa_score(y_test, y_pred)\n",
        "\n",
        "print(\"Cohen's Kappa Score:\", round(kappa_score, 4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v226Sk01QLQe",
        "outputId": "ba0a61ae-de74-45fc-923d-a92c805e4ba9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cohen's Kappa Score: 0.5646\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary classification\n"
      ],
      "metadata": {
        "id": "UeS1dEEBQLuD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Load and preprocess Titanic dataset\n",
        "df = sns.load_dataset('titanic')\n",
        "df = df.dropna(subset=['age', 'fare', 'embarked'])\n",
        "\n",
        "# Encode categorical variables\n",
        "df['sex'] = df['sex'].map({'male': 0, 'female': 1})\n",
        "df['embarked'] = df['embarked'].map({'S': 0, 'C': 1, 'Q': 2})\n",
        "\n",
        "# Step 2: Feature selection\n",
        "features = ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']\n",
        "X = df[features]\n",
        "y = df['survived']\n",
        "\n",
        "# Step 3: Train/Test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 4: Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Predict probabilities\n",
        "y_probs = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Step 6: Calculate Precision-Recall values\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_probs)\n",
        "avg_precision = average_precision_score(y_test, y_probs)\n",
        "\n",
        "# Step 7: Plot Precision-Recall Curve\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.plot(recall, precision, label=f'Avg Precision = {avg_precision:.2f}', color='blue')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "GCh7D6FeQMQR",
        "outputId": "1519718c-60fd-4e71-b93c-073a0da05dd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 700x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. Write a Python program to train Logistic Regression with different solvers (liblinear, saga, Ibfgs) and compare their accuracy.\n"
      ],
      "metadata": {
        "id": "v2PFgtcKQMrB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load and preprocess Titanic dataset\n",
        "df = sns.load_dataset('titanic')\n",
        "df = df.dropna(subset=['age', 'fare', 'embarked'])\n",
        "\n",
        "# Encode categorical variables\n",
        "df['sex'] = df['sex'].map({'male': 0, 'female': 1})\n",
        "df['embarked'] = df['embarked'].map({'S': 0, 'C': 1, 'Q': 2})\n",
        "\n",
        "# Step 2: Feature selection\n",
        "features = ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']\n",
        "X = df[features]\n",
        "y = df['survived']\n",
        "\n",
        "# Step 3: Train/Test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 4: Define solvers\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "accuracies = {}\n",
        "\n",
        "# Step 5: Train Logistic Regression with each solver and evaluate accuracy\n",
        "for solver in solvers:\n",
        "    model = LogisticRegression(solver=solver, max_iter=1000)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracies[solver] = accuracy\n",
        "\n",
        "# Step 6: Display the accuracy comparison\n",
        "for solver, accuracy in accuracies.items():\n",
        "    print(f\"Accuracy with {solver} solver: {round(accuracy, 4)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2g2D18PQPHt",
        "outputId": "833482d2-3bc5-4c60-aa05-c4edf56e0ef1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with liblinear solver: 0.785\n",
            "Accuracy with saga solver: 0.6636\n",
            "Accuracy with lbfgs solver: 0.7897\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Write a Python program to train Logistic Regression and evaluate its performance using Matthews Correlation Coefficient (MCC).\n"
      ],
      "metadata": {
        "id": "L_itQXH3QNHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "# Step 1: Load and preprocess Titanic dataset\n",
        "df = sns.load_dataset('titanic')\n",
        "df = df.dropna(subset=['age', 'fare', 'embarked'])\n",
        "\n",
        "# Encode categorical variables\n",
        "df['sex'] = df['sex'].map({'male': 0, 'female': 1})\n",
        "df['embarked'] = df['embarked'].map({'S': 0, 'C': 1, 'Q': 2})\n",
        "\n",
        "# Step 2: Feature selection\n",
        "features = ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']\n",
        "X = df[features]\n",
        "y = df['survived']\n",
        "\n",
        "# Step 3: Train/Test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 4: Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Predict and evaluate using Matthews Correlation Coefficient (MCC)\n",
        "y_pred = model.predict(X_test)\n",
        "mcc_score = matthews_corrcoef(y_test, y_pred)\n",
        "\n",
        "print(\"Matthews Correlation Coefficient (MCC):\", round(mcc_score, 4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kt2dC9OQQQB7",
        "outputId": "e87c428c-6c72-459b-febb-0c4d223ada18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matthews Correlation Coefficient (MCC): 0.5678\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. Write a Python program to train Logistic Regression on both raw and standardized data. Compare their accuracy to see the impact of feature scaling.\n"
      ],
      "metadata": {
        "id": "kxsobH2QQNzo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load and preprocess Titanic dataset\n",
        "df = sns.load_dataset('titanic')\n",
        "df = df.dropna(subset=['age', 'fare', 'embarked'])\n",
        "\n",
        "# Encode categorical variables\n",
        "df['sex'] = df['sex'].map({'male': 0, 'female': 1})\n",
        "df['embarked'] = df['embarked'].map({'S': 0, 'C': 1, 'Q': 2})\n",
        "\n",
        "# Step 2: Select features and target\n",
        "features = ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']\n",
        "X = df[features]\n",
        "y = df['survived']\n",
        "\n",
        "# Step 3: Train/Test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 4: Train Logistic Regression on raw data\n",
        "model_raw = LogisticRegression(max_iter=1000)\n",
        "model_raw.fit(X_train, y_train)\n",
        "y_pred_raw = model_raw.predict(X_test)\n",
        "accuracy_raw = accuracy_score(y_test, y_pred_raw)\n",
        "\n",
        "# Step 5: Standardize the data (mean=0, std=1)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 6: Train Logistic Regression on standardized data\n",
        "model_scaled = LogisticRegression(max_iter=1000)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Step 7: Compare accuracies\n",
        "print(\"Accuracy with Raw Data:\", round(accuracy_raw, 4))\n",
        "print(\"Accuracy with Standardized Data:\", round(accuracy_scaled, 4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STeFY1w-RDce",
        "outputId": "b8892438-fb04-4f56-cc15-14d501629de4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with Raw Data: 0.7897\n",
            "Accuracy with Standardized Data: 0.7897\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using cross-validation.\n"
      ],
      "metadata": {
        "id": "kg4DL5YtQOJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load and preprocess Titanic dataset\n",
        "df = sns.load_dataset('titanic')\n",
        "df = df.dropna(subset=['age', 'fare', 'embarked'])\n",
        "\n",
        "# Encode categorical variables\n",
        "df['sex'] = df['sex'].map({'male': 0, 'female': 1})\n",
        "df['embarked'] = df['embarked'].map({'S': 0, 'C': 1, 'Q': 2})\n",
        "\n",
        "# Step 2: Feature selection\n",
        "features = ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']\n",
        "X = df[features]\n",
        "y = df['survived']\n",
        "\n",
        "# Step 3: Train/Test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 4: Define Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Step 5: Define the parameter grid for 'C' values\n",
        "param_grid = {'C': [0.01, 0.1, 1, 10, 100]}\n",
        "\n",
        "# Step 6: Use GridSearchCV for cross-validation to find the best 'C'\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Step 7: Display the best 'C' and the corresponding accuracy\n",
        "print(f\"Best value of C: {grid_search.best_params_['C']}\")\n",
        "print(f\"Best cross-validation accuracy: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Step 8: Evaluate the model with the best 'C' on the test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Test set accuracy with optimal C: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "isQ1IPAyRREj",
        "outputId": "676b39d3-45b4-467e-cc5a-a0dea42dcc15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best value of C: 0.1\n",
            "Best cross-validation accuracy: 0.8032\n",
            "Test set accuracy with optimal C: 0.7850\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to make predictions"
      ],
      "metadata": {
        "id": "sYRjNafWRX7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib\n",
        "\n",
        "# Step 1: Load and preprocess Titanic dataset\n",
        "df = sns.load_dataset('titanic')\n",
        "df = df.dropna(subset=['age', 'fare', 'embarked'])\n",
        "\n",
        "# Encode categorical variables\n",
        "df['sex'] = df['sex'].map({'male': 0, 'female': 1})\n",
        "df['embarked'] = df['embarked'].map({'S': 0, 'C': 1, 'Q': 2})\n",
        "\n",
        "# Step 2: Feature selection\n",
        "features = ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']\n",
        "X = df[features]\n",
        "y = df['survived']\n",
        "\n",
        "# Step 3: Train/Test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 4: Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Save the trained model using joblib\n",
        "joblib.dump(model, 'logistic_regression_model.joblib')\n",
        "print(\"Model saved successfully!\")\n",
        "\n",
        "# Step 6: Load the saved model\n",
        "loaded_model = joblib.load('logistic_regression_model.joblib')\n",
        "\n",
        "# Step 7: Make predictions using the loaded model\n",
        "y_pred = loaded_model.predict(X_test)\n",
        "\n",
        "# Step 8: Evaluate the model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test set accuracy with loaded model: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyKIhSCWRYYO",
        "outputId": "915d4052-b0cd-4200-8296-4a642c7c6cd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved successfully!\n",
            "Test set accuracy with loaded model: 0.7897\n"
          ]
        }
      ]
    }
  ]
}